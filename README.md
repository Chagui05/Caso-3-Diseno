# Caso-3-Diseno

## 1. Planeamiento

En esta secci√≥n se detallan los aspectos relacionados con la comprensi√≥n del problema, la forma en que se dividir√° el trabajo dentro del equipo, los hitos en los que se estructura el desarrollo del proyecto y los mecanismos que permitir√°n evaluar si se est√° avanzando conforme a lo planificado.

### 1.1 Estructura del Equipo, Stakeholders, Key Players

#### Estructura Interna

El equipo de trabajo consiste de 5 integrantes:

- Santiago Chaves Garbanzo
- Anthony Fuentes
- Luis David Blanco
- Gabriel Guti√©rrez
- Jefferson Salas Cordero

Todos tenemos asignaciones distintas dentro del proyecto, pero como vamos a trabajar con metodolog√≠a Kanban, cualquiera puede hacerse cargo de cualquier tarea. Basta con elegirla en ClickUp y ponerse a trabjar.

Adem√°s, una pieza muy importante en el proyecto ser√° Rodrigo N√∫√±ez, el cu√°l actuar√° como Product Owner de Data Pura Vida, y consultor de dise√±o de software en caso de que tengamos alg√∫n tipo de duda.

#### Stakeholders y Key Players:

Se identificaron los principales actores que influyen o se ven afectados por el desarrollo de la plataforma. A continuaci√≥n, se presenta una matriz que los muestra:

![matrizstakeholders](img/matriz-stakeholders.png)

Como se puede observar, los principales interesados en que el proyecto avance son el Product Owner y el equipo de trabajo, ya que est√°n comprometidos con lograr que el producto final cumpla con los objetivos planteados desde el inicio.

La poblaci√≥n general, aunque no tiene un alto nivel de poder dentro del proyecto, demuestra un gran inter√©s. Una plataforma de este tipo promover√≠a la transparencia en el acceso a datos y aportar√≠a a un Costa Rica con mayor disponibilidad de informaci√≥n para todos (incluso aunque parte del contenido sea privado).

Por otro lado, el actor con mayor poder es el gobierno, al ser quien financia el proyecto. Su inter√©s se considera moderado, posiblemente por cierto escepticismo respecto al impacto que la plataforma podr√≠a tener sobre su reputaci√≥n.

Finalmente, aquellos actores que incurren en pr√°cticas como el lavado de dinero o evasi√≥n fiscal son los menos interesados, ya que el sistema ofrecer√≠a mayor visibilidad sobre los datos, lo cual podr√≠a exponer dichas irregularidades.

Ahora bien, ya que se conocen los stakeholders principales se puede evidenciar que los key players son tanto el Product Owner, como el Equipo de trabajo, ya que ser√°n los encargados de que el proyecto tenga exito.

#### Sistemas y Ecosistemas de Software Existentes:

En Costa Rica no existe ning√∫n sistema ni ecosistema previo que funcione como antecedente directo de Data Pura Vida, por lo que no se anticipan problemas de integraci√≥n con plataformas existentes. Si bien existen herramientas puntuales, como el API del TSE para consultas por c√©dula, estas no representan un obst√°culo, ya que los requerimientos del proyecto contemplan la capacidad del sistema para aceptar datos provenientes de APIs externas.

### 1.2 Gesti√≥n de la Comunicaci√≥n y Documentaci√≥n del proyecto

Para la comunicaci√≥n interna del equipo se utilizar√° Slack. A trav√©s de esta herramienta se coordinar√° la asignaci√≥n de tareas en ClickUp, y tambi√©n se notificar√° cuando una tarea haya sido finalizada. Antes de marcarla como completada, la tarea deber√° pasar al estado de "Esperando Aprobaci√≥n" en ClickUp, para que Rodrigo pueda revisarla y aprobarla.

Adem√°s, se utilizar√° Discord para realizar al menos una reuni√≥n semanal, en la cual se discutir√°n avances generales del proyecto. En caso de surgir dudas m√°s complejas, se invitar√° al profesor para que pueda brindar orientaci√≥n.

La documentaci√≥n principal del proyecto se mantendr√° en el README del repositorio de GitHub Chagui05/Caso-3-Diseno. En ese archivo se incluir√°n todos los detalles relevantes. Si existieran anexos, como la hoja de requerimientos o la entrevista con el profesor, se agregar√°n al mismo repositorio en archivos separados con nombres descriptivos, y se har√° referencia a ellos desde el README. Toda la documentaci√≥n ser√° escrita en formato Markdown.

### 1.3 Entendimiento del problema

Para entender adecuadamente el problema, el equipo realiz√≥ una entrevista al Product Owner, basada en una serie de preguntas que surgieron tras revisar la especificaci√≥n del proyecto. Las respuestas obtenidas fueron registradas en el archivo RespuestaEntrevista, y permitieron extraer informaci√≥n clave, como por ejemplo:

- La plataforma debe permitir el registro de personas f√≠sicas primero, y luego dar la posibilidad de vincularlas como administradoras de organizaciones.
- La validaci√≥n de cuentas puede involucrar m√∫ltiples m√©todos: revisi√≥n manual, validaciones automatizadas y uso de servicios externos como SumSub.
- La inclusi√≥n de IBAN y datos de tarjetas en el registro busca filtrar usuarios no comprometidos y evitar registros superficiales.
- Se recomienda restringir el acceso por IP solo a la secci√≥n de registro, o permitir el registro de IPs confiables para quienes est√©n en el extranjero.
- Los datos cargados (sin importar el origen) deben convertirse a un formato unificado tras aplicar el proceso ETDL (relacional, documental, etc.).
- El sistema debe validar estructura, formato y contenido de los datasets, considerando reglas como formatos de fechas, booleanos y tipos de datos.
- El motor de visualizaci√≥n de dashboards debe permitir a los usuarios crear sus propios paneles, compartirlos y gestionarlos.
- La arquitectura de backend puede ser definida libremente por el equipo (monol√≠tica o microservicios), seg√∫n convenga al dise√±o general.
- Se permite la integraci√≥n con herramientas de IA, siempre que se respeten los criterios de autorizaci√≥n definidos por la administraci√≥n de la plataforma.
- El portal web de backoffice ser√° administrado por una organizaci√≥n gubernamental registrada dentro del sistema, actuando como custodio de la informaci√≥n.

A partir de esta informaci√≥n, se desarrollaron los siguientes diagramas de flujo que ilustran las tareas clave identificadas dentro del sistema:

- Diagrama de registro:

El siguiente diagrama presenta una visi√≥n general del proceso de registro en nuestra plataforma. No incluye detalles t√©cnicos ni especificaciones sobre los campos din√°micos que var√≠an seg√∫n el tipo de entidad registrada; su objetivo es ilustrar de forma abstracta y comprensible c√≥mo se estructura el flujo de registro dentro del sistema.

![matrizstakeholders](img/entendimientoRegistro.png)

- Diagrama de Ingesta y configuraci√≥n de un dataset

Subir y configurar un dataset en la plataforma no es nada trivial, por eso se armaron estos dos diagramas que separan el proceso en dos partes.

El primer diagrama muestra c√≥mo se le pide al usuario que suba el dataset: qu√© datos tiene que dar, qu√© informaci√≥n se necesita para la IA, y c√≥mo se valida el archivo que subi√≥ (formato, estructura, nombres de columnas, etc.).

![matrizstakeholders](img/subidaDataset1.png)

El segundo diagrama arranca una vez que el dataset ya fue validado. Ah√≠ se definen cosas como si el conjunto de datos va a ser p√∫blico, privado o de pago, y qu√© m√©todos de acceso y cobro se van a aplicar.

![matrizstakeholders](img/subidaDataset2.png)

Es importante aclarar que en el diagrama II no se detalla paso a paso lo que hace el motor ETDL, pero s√≠ se deja claro que va a encargarse de tareas como: detectar duplicados, relacionar datos con otros ya cargados, ajustar el modelo seg√∫n las conexiones que encuentre, y aplicar autom√°ticamente un flujo con extracci√≥n, transformaci√≥n, limpieza, detecci√≥n de contexto, modelado y carga con ayuda de AI.

#### Componentes del Sistema

Con el fin de lograr una arquitectura modular, segura y mantenible, el sistema se divide en macrocomponentes. Cada uno aborda un conjunto espec√≠fico de requerimientos funcionales y no funcionales. En esta secci√≥n se listan los componentes y sus principales responsabilidades. La implementaci√≥n t√©cnica y subdivisi√≥n de estos se detalla m√°s adelante en el documento.

##### Bioregistro

Este m√≥dulo gestiona el proceso de incorporaci√≥n de personas f√≠sicas y jur√≠dicas a la plataforma. Abarca desde el llenado de formularios hasta la validaci√≥n de identidad y la emisi√≥n de credenciales digitales. Debe cumplir con regulaciones AML y est√°ndares avanzados de identidad digital.

Requerimientos:

- El componente debe permitir el registro de personas f√≠sicas, jur√≠dicas, instituciones, c√°maras, grupos y empresas.
- El formulario de registro debe adaptarse din√°micamente seg√∫n el tipo de entidad seleccionada.
- El registro de usuarios debe estar asegurado con MFA y biometr√≠a, cumpliendo est√°ndares de identidad digital avanzada.
- El componente debe solicitar y capturar informaci√≥n personal, societaria, legal y tributaria seg√∫n el tipo de entidad.
- El componente debe revisar que cuando se van a asignar personas f√≠sicas a la organizaci√≥n al crearla, efectivamente formen parte de dicho conjunto.
- El registro debe pasar por una etapa de validaci√≥n interna manual para el registro de empresas
- El componente debe implementar validaci√≥n autom√°tica por inteligencia artificial de los documentos subidos.
- El componente debe exigir a los representantes legales el registro como individuos con: identidad digital, biometr√≠a, prueba de vida y autenticaci√≥n multifactor (MFA).
- Cada organizaci√≥n debe recibir llaves de seguridad que le permitan delegar o revocar accesos a sus usuarios.
- Un usuario debe poder administrar m√∫ltiples organizaciones desde una √∫nica cuenta.
- El componente debe capturar datos preliminares de cuentas IBAN y/o tarjetas de cr√©dito como parte del registro.
- El componente debe enviar una notificaci√≥n por correo electr√≥nico cuando un registro sea aprobado.
- El componente debe exigir documentos espec√≠ficos seg√∫n el tipo de entidad: c√©dulas f√≠sicas o jur√≠dicas, actas, RTN, direcci√≥n, etc.
- El componente debe permitir registrar direcciones IP institucionales (listas blancas) para permitir acceso autorizado.
- El componente debe permitir √∫nicamente IPs costarricenses en el registro
- El sistema debe proteger las claves generadas mediante un esquema de llave tripartita, distribuidas entre Data Pura Vida y dos custodios.

##### La B√≥veda

La B√≥veda es el almac√©n central de datos del sistema, dise√±ado para ser seguro, escalable y auditable. Unifica todos los datos cargados, sin importar su formato de origen, y permite relaciones entre datasets. Cifra la informaci√≥n en tr√°nsito y reposo, controla el acceso por roles y entidades, y mantiene trazabilidad completa del uso y movimientos de los datos. Est√° pensada para soportar millones de registros con alto rendimiento y cumplir est√°ndares de gobierno de datos.

Requerimientos:

- La B√≥veda tiene que almacenar los datos en un solo formato, por m√°s de que las fuentes externas sean de distintos tipos (relacionales, documentales, csv, excel)
- La B√≥veda debe permitir especificar columnas que relacionan un dataset con otros datasets del ecosistema.
- La B√≥veda debe de estar monitoreada en todo momento para detectar movimientos sospechosos, para dar contenido de uso de un dataset, y para asegurar trazabilidad y diagn√≥stico r√°pido de fallas.
- Debe ser resiliente, auditable y alineado con est√°ndares de gobierno de datos.
- Debe permitir crecimiento din√°mico sin perder eficiencia.
- Debe escalar a millones de registros y miles de usuarios concurrentes.
- Mantener trazabilidad de datos usados, no usados y descartados.
- Todos los datos cargados deben estar protegidos mediante cifrado, incluso frente al personal t√©cnico ("ingenieros de la plataforma").
- Cifrar toda la data en tr√°nsito y en reposo, dejando trazabilidad auditable.
- Permitir almacenamiento masivo de datos estructurados y semiestructurados.
- Controlar accesos l√≥gicos por entidad, usuario o tipo de dato.
- Implementar control de acceso a nivel de rol (RBAC) y a nivel de fila (RLS) o equivalentes.

##### Centro de Carga

Este m√≥dulo permite a los usuarios cargar sus datasets a la plataforma. Desde ac√° pueden definir qu√© datos desean cifrar, especificar el formato de origen y configurar otros par√°metros clave para asegurar que la carga se procese correctamente.

Requerimientos:

- Permitir a los usuarios decidir qu√© datos compartir dentro del ecosistema.
- Requerir que cada dataset tenga un nombre √∫nico.
- Soportar m√∫ltiples m√©todos de carga de datos: archivos Excel, CSV, JSON, APIs y conexiones directas a bases de datos SQL y NoSQL.
- Requerir nombre, descripci√≥n y metadata √∫til para IA sobre las columnas del dataset.
- Permitir configurar los par√°metros de conexi√≥n de forma cifrada para cada medio de carga.
- Los par√°metros de conexi√≥n de bases de datos y APIs deben almacenarse de forma cifrada.
- Permitir configurar si el dataset es p√∫blico o privado, gratuito o pagado, permanente o con disponibilidad temporal.
- El sistema de permisos debe prevenir accesos no autorizados a datasets privados o pagos.
- Asignar permisos de acceso a los datasets privados.
- Permitir definir montos de acceso para datasets con modelo de cobro.
- Restringir acceso a datos por tiempo, volumen o frecuencia de consulta.
- Indicar si la carga es √∫nica o recurrente, completa o por deltas.
- Configurar par√°metros para carga por deltas: campos diferenciales, frecuencia (timed pull) o mediante callbacks.
- Habilitar control granular de acceso por instituci√≥n, persona o grupo.

##### Motor de Transformaci√≥n

Este m√≥dulo es clave para garantizar que los datasets se almacenen correctamente en la B√≥veda. Se encarga de recibir datos desde distintas fuentes, validar que el formato coincida con el indicado en el formulario de ingesta y, en caso contrario, rechazar la carga. Una vez superada esta validaci√≥n, aplica todo el proceso de ETDL y mapea los datos al formato interno de la B√≥veda.

Requerimientos:

- Validar el formato, estructura y contenido de cada dataset cargado sea correcto, o bien adaptarlo al interno de la B√≥veda (formatos de fecha, booleans, etc.).
- Validar el formato, estructura y contenido de cada dataset cargado coincida con lo especificado en el proceso de carga.
- Automatizar el proceso de carga mediante un motor de IA que aplique un flujo ETDL (extracci√≥n, transformaci√≥n, limpieza, detecci√≥n de contexto, modelado y carga).
- Aplicar IA para normalizar, redise√±ar modelos de datos y vincularlos autom√°ticamente.
- Detectar duplicidades, optimizar relaciones y ajustar el modelo de datos autom√°ticamente seg√∫n las interrelaciones detectadas.
- Monitorear el proceso completo con m√©tricas de transferencia, carga, limpieza, eliminaci√≥n, modelado, volumen, datos omitidos, datos consultados y tasa de √©xito.
- El sistema debe ser capaz de procesar cargas recurrentes y automatizadas sin intervenci√≥n manual.
- Soportar cargas delta con identificaci√≥n de cambios.
- Realizar merges eficientes sin p√©rdida de integridad.

##### Centro de Visualizaci√≥n y Consumo

Este m√≥dulo est√° compuesto por 3 subcomponentes clave:

1. **Generador de dashboards**: permite a los usuarios dise√±ar y crear gr√°ficos de forma r√°pida y amigable para visualizar cualquier dataset.

Requerimientos:

- El sistema debe permitir la construcci√≥n de dashboards personalizados de forma manual.
- El sistema debe permitir construir dashboards manualmente o mediante prompts inteligentes que generen visualizaciones autom√°ticas.
- El sistema debe permitir representar visualmente los datos en tablas, gr√°ficos, conteos, tendencias y predicciones.
- El sistema debe permitir a los usuarios guardar sus dashboards personalizados.
- El sistema debe permitir compartir dashboards con otros usuarios o hacerlos p√∫blicos dentro de la plataforma.
- La interfaz de construcci√≥n de dashboards debe ser segura, intuitiva y con capacidad de respuesta en tiempo real.

2. **Visualizaci√≥n y Consumo**: ofrece una interfaz para revisar esas visualizaciones y realizar an√°lisis de datos directamente sobre los dashboards.

Requerimientos:

- El sistema debe permitir visualizar todos los datasets accesibles como una fuente consolidada.
- El sistema debe bloquear toda exportaci√≥n directa de datos y gr√°ficos desde el portal.
- El sistema debe mostrar datos de forma preliminar en modo de construcci√≥n de dashboard y luego con datos reales al ejecutar consultas
- El sistema debe deshabilitar temporalmente el acceso a datasets cuando se superen los l√≠mites de consumo.
- El sistema debe registrar todas las transacciones y consumos de datos en un historial accesible para cada usuario.
- El sistema debe mostrar m√©tricas de uso: volumen de datos consultados, n√∫mero de consultas realizadas, tiempo restante o l√≠mites alcanzados.
- El sistema no debe permitir en ning√∫n momento la descarga directa de datasets o gr√°ficos generados.
- La visualizaci√≥n de datos debe realizarse exclusivamente dentro del portal, sin opciones de exportaci√≥n, captura o embedding externo.
- Los l√≠mites de consumo deben aplicarse en tiempo real, sin permitir bypasses o reintentos abusivos.

3. **Consumo para IA**: Este subcomponente es el regulador de consumo de IA, define l√≠mites y los m√©todos de ingesta disponibles desde el sistema para los usuarios.

Requerimientos:

- El sistema debe permitir el acceso sistema a sistema √∫nicamente para alimentar modelos de IA aprobados.
- La entrega de datos para modelos de IA debe ser monitoreada, registrada y limitada a contextos aprobados expl√≠citamente por Data Pura Vida.
- El sistema debe ofrecer plataformas limitadas y controladas para esta alimentaci√≥n de IA. Solo permitir√° 2 por usuario.
- El sistema debe minimizar al m√°ximo el riesgo de descargas indirectas mediante presunci√≥n de uso en IA.
- Los datos deben ser env√≠ados en un formato que no permita poder ser desencriptado para otro uso que no sea alimentar IA (por ejemplo uso de embeddings).

##### Marketplace

Este m√≥dulo est√° enfocado en ofrecer una interfaz amigable que permita a los usuarios encontrar datasets de forma eficiente, con descripciones claras y navegaci√≥n fluida. Adem√°s, incluye una secci√≥n adicional para buscar dashboards creados por otros usuarios, facilitando el descubrimiento y reutilizaci√≥n de visualizaciones dentro de la plataforma.

Requerimientos:

- La experiencia de compra de datasets debe ser fluida, transparente y accesible desde los dashboards personales.
- Incluir un m√≥dulo de compra donde se visualicen datasets disponibles bajo acceso pagado.
- Permitir seleccionar un dataset, visualizar precio, t√©rminos de uso, duraci√≥n del acceso y condiciones de cobro.
- Soportar m√∫ltiples m√©todos de pago: tarjeta de cr√©dito, d√©bito y otros mecanismos nacionales compatibles.
- Mostrar confirmaciones de transacci√≥n y activar el acceso seg√∫n condiciones (tiempo, volumen, frecuencia).
- El sistema debe mostrar opciones para renovar o ampliar los paquetes de acceso en caso de superar el l√≠mite.

##### Backoffice Administrativo

Este m√≥dulo concentra las herramientas de backoffice necesarias para la gesti√≥n integral de la plataforma. Su enfoque est√° en el control, la seguridad, la gobernanza de datos y la trazabilidad completa de las operaciones.

Requerimientos:

- Administrar usuarios: validaci√≥n de identidad, membres√≠a y roles.
- Gestionar reglas de carga de datos (formatos, estructuras, validaciones).
- Configurar conexiones externas (APIs, BDs, callbacks).
- Activar, desactivar y supervisar objetos de datos, pipelines y flujos.
- Revocar o regenerar llaves de seguridad (sim√©tricas, asim√©tricas, tri-partitas).
- Administrar custodios de llaves y flujos de confirmaci√≥n mancomunada.
- Auditar operaciones por usuario, fecha, acci√≥n y resultado.
- Generar reportes de uso, calidad, integraci√≥n y anomal√≠as.
- Monitorear el estado operativo de servicios y tareas.
- Extraer evidencias para procesos legales bajo autorizaci√≥n.
- Gestionar permisos y accesos mediante RBAC.
- Debe ofrecer una interfaz robusta y segura solo para personal autorizado.
- Debe permitir gesti√≥n flexible pero estricta de accesos y configuraciones.

#### Prototipado

Se desarroll√≥ un prototipo funcional de la p√°gina del Bioregistro Verde con el objetivo principal de probar el comportamiento de los formularios din√°micos. Este prototipo no incluye procesos de prueba de vida ni captura de datos biom√©tricos, y tampoco recolecta la informaci√≥n final que se almacenar√° en el sistema definitivo. Su prop√≥sito es demostrar, a alto nivel, c√≥mo el formulario se adapta din√°micamente seg√∫n las selecciones del usuario.

A continuaci√≥n las im√°genes del flujo de recolecci√≥n de data de una persona f√≠sica:

![alt text](img/userReg0.png)

![alt text](img/userReg1.png)

![alt text](img/userReg2.png)

![alt text](img/userReg3.png)

Adem√°s, se adjunta el proceso de registar compa√±√≠a p√∫blica:

![alt text](img/orgPub0.png)
![alt text](img/orgPub1.png)
![alt text](img/orgPub2.png)
![alt text](img/orgPub3.png)
![alt text](img/orgPub4.png)
![alt text](img/orgPub5.png)

Si dese√° probar el prototipo visite el siguiente [link](https://gentle-signup-wizard.lovable.app/).

### 1.4 Customer Journeys

Este Service Blueprint representa el recorrido completo de un ciudadano dentro del ecosistema Data Pura Vida, desde el descubrimiento de la plataforma hasta la creaci√≥n, publicaci√≥n y monitoreo de un dashboard personalizado con datos p√∫blicos.

El diagrama detalla no solo las acciones del usuario, sino tambi√©n los touchpoints del sistema, los procesos internos (backstage) y las herramientas de soporte involucradas en cada etapa. Adem√°s, se integran las emociones del usuario para identificar oportunidades de mejora en la experiencia.

Las l√≠neas de interacci√≥n, visibilidad e interacci√≥n interna permiten visualizar claramente los l√≠mites entre lo que el ciudadano ve, lo que ocurre detr√°s del sistema y las herramientas tecnol√≥gicas que lo sustentan.

Este blueprint se organiza en **seis** fases principales:

1. Conciencia: El ciudadano conoce la existencia de la plataforma.

2. Ingreso: Accede y se autentica de forma segura.

3. Exploraci√≥n: Navega los datasets disponibles.

4. Creaci√≥n: Construye un dashboard visual con datos abiertos.

5. Publicaci√≥n: Comparte su dashboard con otros usuarios.

6. Monitoreo: Consulta m√©tricas de visualizaci√≥n y uso.b

![alt text](img/journey1.png)

Este Service Blueprint representa el recorrido completo de una empresa dentro del ecosistema Data Pura Vida, desde el descubrimiento de la plataforma hasta la publicaci√≥n y monitoreo de un dataset privado con fines de monetizaci√≥n.

Este blueprint se organiza en siete fases principales:

1. Conciencia: La empresa conoce la plataforma y sus posibilidades de monetizaci√≥n.

2. Registro: Se registra como entidad jur√≠dica mediante un proceso validado.

3. Carga de datos: Sube su dataset en formatos compatibles como archivos o APIs.

4. Configuraci√≥n: Define permisos, privacidad, cifrado y condiciones de acceso.

5. Validaci√≥n de valor: Eval√∫a la calidad y el potencial del dataset con ayuda de IA.

6. Publicaci√≥n: Hace p√∫blico el dataset con opciones de pago o acceso controlado.

7. Monitoreo: Supervisa ingresos, visualizaciones y transacciones en tiempo real.

![alt text](img/JourneyIP.png)

**Fases principales:**

1. Registro: Un representante oficial crea una cuenta institucional y sube los documentos requeridos.
2. Validaci√≥n: La plataforma aplica validaciones autom√°ticas y manuales con ayuda de IA.
3. Configuraci√≥n: La instituci√≥n decide qu√© datos compartir y con qu√© restricciones.
4. Carga: Se conecta una base de datos externa para carga automatizada.
5. Publicaci√≥n: El dataset queda disponible para actores autorizados mediante pago.
6. Seguimiento: La instituci√≥n consulta m√©tricas de acceso y consumo.

![alt text](img/journey2.png)

### 1.5 Plan de ejecuci√≥n del proyecto

#### Plan de Dise√±o

El proyecto se estructura en cinco hitos principales que marcar√°n su progreso:

- Planeamiento del Proyecto
- Supuestos del Proyecto
- Stack Tecnol√≥gico
- Dise√±o de los Componentes
- Validaci√≥n de los requerimientos

Cada uno de estos hitos cuenta con un plazo definido para su ejecuci√≥n, lo cual se puede observar en el siguiente diagrama de Gantt:

![gantt](img/gantt.png)

Dentro de cada hito se contemplan varias tareas. Cada integrante del equipo debe seleccionar una tarea seg√∫n su disponibilidad. Un hito se considera finalizado √∫nicamente cuando todas sus tareas han sido completadas y se encuentran en el tablero de completado en ClickUp.

Adem√°s, como se indic√≥ anteriormente, se realizar√°n reuniones semanales para verificar que el proyecto avance conforme al plan establecido.

Adem√°s, como se dijo previamente, se har√°n reuniones semanales para verificar que el proyecto se est√© realizando seg√∫n lo dice el plan.

#### Plan de Ejecuci√≥n para Desarrolladores

Este plan indica c√≥mo avanzar progresivamente en la construcci√≥n del sistema, desde preparar el entorno hasta desplegar y probar los m√≥dulos principales. No detalla c√≥mo funciona cada m√≥dulo, sino c√≥mo se implementan y conectan entre s√≠, con sus respectivos entregables por etapa.

##### 1. Preparaci√≥n del Entorno de Desarrollo

**Objetivo:** Sentar las bases para que todo el equipo trabaje de forma coordinada, segura y replicable.

**Actividades:**

- Establecer repositorio con control de versiones.
- Configurar ambientes separados para desarrollo, pruebas y producci√≥n.
- Montar infraestructura local (contenedores, redes internas, secretos).
- Habilitar flujos b√°sicos de CI/CD y documentaci√≥n t√©cnica inicial.

**Entregables:**

- Repositorio con estructura base.
- Manual de instalaci√≥n local y buenas pr√°cticas.
- Plantilla de CI/CD con al menos una validaci√≥n b√°sica.
- Ambiente de desarrollo replicable con un comando (ej. Docker Compose).

##### 2. Implementaci√≥n del Bioregistro

**Objetivo:** Habilitar la incorporaci√≥n de personas f√≠sicas y jur√≠dicas a la plataforma.

**Actividades:**

- Crear formulario de registro adaptativo por tipo de entidad.
- Implementar simulaciones de validaci√≥n autom√°tica y revisi√≥n manual.
- Gestionar jerarqu√≠as usuario‚Äìorganizaci√≥n y generaci√≥n de llaves.
- Activar sistema de notificaciones y control geogr√°fico b√°sico.

**Entregables:**

- Flujo funcional de registro completo.
- Formulario con l√≥gica adaptativa por entidad.
- Simulaci√≥n de validaciones autom√°ticas y manuales.
- Sistema de emisi√≥n de llaves y notificaci√≥n por correo.

##### 3. Habilitar el Centro de Carga de Datos

**Objetivo:** Permitir a los usuarios cargar datasets desde distintas fuentes.

**Actividades:**

- Desarrollar interfaz de carga de archivos (CSV, Excel, JSON).
- Capturar metadatos b√°sicos (nombre, descripci√≥n, privacidad, etc.).
- Simular conexi√≥n con bases de datos externas.
- Almacenar cargas en espacio temporal con trazabilidad.

**Entregables:**

- M√≥dulo de carga funcional con validaciones m√≠nimas.
- Interfaz para configuraci√≥n de metadatos y privacidad.
- Log de cargas realizadas para trazabilidad.
- Cargas almacenadas provisionalmente.

##### 4. Desarrollar el Motor de Transformaci√≥n

**Objetivo:** Procesar los datos cargados, limpiarlos y convertirlos a un formato interno.

**Actividades:**

- Validar estructura y contenido de cada carga.
- Aplicar l√≥gica b√°sica de transformaci√≥n (normalizaci√≥n, fechas, duplicados).
- Generar versiones limpias de los datos.
- Conectar a almacenamiento de datos validado (La B√≥veda).

**Entregables:**

- Flujo de transformaci√≥n activo con trazabilidad.
- Reportes de validaci√≥n y errores por dataset.
- Datos transformados almacenados de forma estructurada.
- M√©tricas b√°sicas del proceso (tiempo, √©xito, errores).

##### 5. Configurar La B√≥veda

**Objetivo:** Consolidar y proteger los datos procesados para su consumo posterior.

**Actividades:**

- Crear repositorio √∫nico para los datasets internos.
- Implementar segmentaci√≥n de acceso por rol, entidad y tipo de dato.
- Establecer cifrado b√°sico en tr√°nsito y reposo.
- Documentar relaciones entre datasets si aplica.

**Entregables:**

- Sistema de almacenamiento central con control de accesos.
- Datasets organizados y protegidos.
- Trazabilidad de acceso a cada dataset.
- Esquema inicial de relaciones entre datasets (si corresponde).

##### 6. Activar la Visualizaci√≥n y los Dashboards

**Objetivo:** Permitir a los usuarios explorar datos mediante gr√°ficos sin exportarlos.

**Actividades:**

- Crear constructor b√°sico de dashboards con tablas y gr√°ficos.
- Activar vistas previas con datos ficticios y reales.
- Controlar consumo (frecuencia, volumen, consultas).
- Habilitar compartir dashboards con otros usuarios.

**Entregables:**

- M√≥dulo visual con constructor de dashboards funcional.
- Dashboards guardables y compartibles.
- L√≥gica de l√≠mites de uso aplicada.
- Registro de interacciones y consultas realizadas.

##### 7. Simular el Consumo para Modelos de IA

**Objetivo:** Simular acceso regulado a datasets por sistemas externos autorizados.

**Actividades:**

- Definir reglas de uso y l√≠mites por usuario y contexto.
- Habilitar endpoints simulados para "entrenamiento" de IA.
- Registrar y auditar cada consulta o consumo.
- Aplicar restricciones estrictas para evitar abuso.

**Entregables:**

- API simulada para consumo por IA.
- Sistema de seguimiento y l√≠mites aplicado.
- Log de accesos con usuario, contexto y volumen consultado.
- Validaci√≥n de cumplimiento de reglas definidas.

##### 8. Prototipar el Marketplace de Datos

**Objetivo:** Permitir explorar, adquirir y acceder a datasets bajo condiciones.

**Actividades:**

- Crear buscador y navegador de datasets p√∫blicos y pagos.
- Mostrar precios, t√©rminos y opciones de compra.
- Simular proceso de adquisici√≥n y activaci√≥n de accesos.
- Gestionar historial de compras y permisos vigentes.

**Entregables:**

- Interfaz funcional del marketplace.
- Flujo de compra simulado con activaci√≥n de acceso.
- Historial de transacciones por usuario.
- Permisos temporales aplicados tras cada compra.

##### 9. Activar el Backoffice Administrativo

**Objetivo:** Brindar herramientas de gesti√≥n y supervisi√≥n al equipo administrador.

**Actividades:**

- Crear panel seguro para personal autorizado.
- Visualizar registros, cargas y actividad por componente.
- Permitir aprobaci√≥n/rechazo de registros y cargas.
- Generar reportes y estad√≠sticas de uso.

**Entregables:**

- Panel de administraci√≥n con control de usuarios y datos.
- Visualizaci√≥n de actividad y estado del sistema.
- Herramientas para revisi√≥n y auditor√≠a b√°sica.
- Generaci√≥n de reportes autom√°ticos.

##### 10. Pruebas Integradas y Simulaci√≥n de Casos Reales

**Objetivo:** Asegurar que todo funcione de forma integrada.

**Actividades:**

- Simular flujos completos: registro ‚Üí carga ‚Üí transformaci√≥n ‚Üí consumo.
- Usar casos reales o sint√©ticos para testear extremos del sistema.
- Validar reglas de permisos, l√≠mites y visualizaci√≥n.
- Documentar fallos, mejoras y tiempos de respuesta.

**Entregables:**

- Casos de prueba documentados.
- Resultados de pruebas con observaciones.
- Validaci√≥n de flujos completos con usuarios simulados.
- Lista de bugs o ajustes para corregir.

##### 11. Despliegue Controlado y Evaluaci√≥n

**Objetivo:** Publicar la plataforma en un entorno accesible para validaci√≥n final.

**Actividades:**

- Preparar un entorno de pruebas compartido (local o nube).
- Desplegar todos los m√≥dulos de forma conectada.
- Habilitar usuarios de prueba para feedback externo.
- Preparar una demo p√∫blica o privada para presentaci√≥n.

**Entregables:**

- Versi√≥n desplegada en entorno de pruebas.
- Acceso limitado para usuarios externos (testers/docentes).
- Feedback recolectado para iteraci√≥n.
- Material de presentaci√≥n o demo funcional lista.

### 1.6 WBS del sistema

Como parte del an√°lisis inicial del sistema **Data Pura Vida**, se realiz√≥ una descomposici√≥n de alto nivel para identificar los l√≠mites del sistema y los actores involucrados. A continuaci√≥n, se presenta el diagrama de contexto basado en las t√©cnicas descritas para la identificaci√≥n del sistema y sus l√≠mites:

![Work Breakdown Structure](img/WorkBreakdownStructure.jpg)

Esta representaci√≥n facilita el entendimiento general del sistema y servir√° como base para la posterior descomposici√≥n en subsistemas, componentes funcionales y dise√±o arquitect√≥nico detallado.

#### Prop√≥sito del diagrama

- **Identificaci√≥n de l√≠mites del sistema:** El diagrama establece una frontera clara entre lo que est√° dentro y fuera del alcance del desarrollo, lo cual es crucial para evitar ambig√ºedades durante el dise√±o detallado.

- **Visualizaci√≥n de los actores externos:** Permite entender qui√©nes interact√∫an con el sistema y con qu√© prop√≥sito.

- **Detecci√≥n de puntos de integraci√≥n:** Ayuda a anticipar necesidades de interoperabilidad, seguridad, formatos de intercambio de datos y protocolos de comunicaci√≥n.

#### Consideraciones adicionales

Este diagrama ser√° utilizado como punto de partida para:

- La descomposici√≥n en subsistemas o m√≥dulos funcionales, agrupando responsabilidades afines.

- La definici√≥n de casos de uso y escenarios de interacci√≥n.

- La elaboraci√≥n de la arquitectura t√©cnica, donde se identificar√°n servicios, componentes y flujos de datos internos.

En resumen, este modelo de contexto es una herramienta clave para asegurar un entendimiento compartido del dominio del problema y sentar las bases de una soluci√≥n t√©cnica coherente, escalable y alineada con los objetivos del proyecto.

### 1.7 Evaluaci√≥n de Riesgos

#### Metodolog√≠a ISO 31000

La evaluaci√≥n de riesgos sigue los principios de **ISO 31000** para la gesti√≥n de riesgos del proyecto Data Pura Vida.

#### Marco de Evaluaci√≥n:

La evaluaci√≥n de riesgos utiliza una matriz de probabilidad versus impacto basada en criterios espec√≠ficos del proyecto Data Pura Vida y su contexto de dise√±o de sistemas complejos.

#### Escala de probabilidad:

- Muy Alta (100%) : Es pr√°cticamente seguro que el riesgo ocurrir√° durante el proyecto (9 de cada 10 proyectos similares)
- Alta (80%) : Es muy probable que el riesgo se materialice (7-8 de cada 10 casos)
- Medios (60%) : Hay una posibilidad moderada de que ocurra (5-6 de cada 10 casos)
- Baja (40%) : Es poco probable pero posible que suceda (3-4 de cada 10 casos)
- Muy Baja (20%) : Es muy poco probable que se materialice (1-2 de cada 10 casos)

#### Escala de Impacto:

- Muy Alto (100%) : Falla completa del proyecto, redise√±o total necesario, o retraso superior a 4 semanas
- Alto (80%) : Compromete objetivos principales del proyecto, retraso de 2-4 semanas, o afecta m√∫ltiples componentes cr√≠ticos
- Medio (60%) : Afecta la calidad del dise√±o o genera retraso de 1-2 semanas, requiere trabajo adicional significativo
- Bajo (40%) : Impacto menor en cronograma (3-7 d√≠as) o calidad, se puede resolver con ajustes menores
- Muy Bajo (20%) : Impacto m√≠nimo (menos de 3 d√≠as), no afecta objetivos principales del proyecto

#### Riesgos para el Dise√±o de Data Pura Vida

| ID      | Categor√≠a         | Riesgo                                               | Descripci√≥n Detallada                                                                                                                                                                           | Probabilidad        | Impacto             | Clasificaci√≥n   | Estrategia     | Plan de Respuesta                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ------- | ----------------- | ---------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- | ------------------- | --------------- | -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **R01** | **Dise√±o**        | **Complejidad arquitect√≥nica del ecosistema**        | Dise√±ar una arquitectura que integre efectivamente portal web, API backend, datalake, backoffice y m√∫ltiples sistemas de seguridad requiere experiencia en arquitecturas distribuidas complejas | **Muy Alta (100%)** | **Muy Alto (100%)** | **üî¥ EXTREMO**  | **MITIGAR**    | **Prevenci√≥n:** Definir m√°ximo 5 patrones arquitect√≥nicos (Semana 1), crear ADRs (Architecture Decision Records) para cada decisi√≥n, revisi√≥n arquitect√≥nica obligatoria cada viernes 1h con validaci√≥n t√©cnica<br>**Contingencia:** Crear spike de 16h para dise√±ar arquitectura simplificada (3 capas: Frontend-API-Data), eliminar microservicios y usar monolito modular, reducir integraciones complejas a APIs REST est√°ndar |
| **R02** | **Alcance**       | **Subestimaci√≥n del alcance del dise√±o**             | El tiempo asignado puede ser insuficiente para dise√±ar completamente todos los componentes t√©cnicos con el nivel de detalle requerido para un sistema de esta magnitud                          | **Muy Alta (100%)** | **Medio (60%)**     | **üü† ALTO**     | **MITIGAR**    | **Prevenci√≥n:** Planning Poker diario 30min, re-estimaci√≥n mi√©rcoles, time tracking obligatorio en ClickUp, descomponer tareas en m√°ximo 8h cada una<br>**Contingencia:** Si desv√≠o > 150% en 3 tareas: reducir nivel de detalle en diagramas de secuencia (de completos a conceptuales), simplificar especificaciones APIs (menos endpoints), priorizar componentes cr√≠ticos primero, redistribuir trabajo en 1 d√≠a               |
| **R03** | **Documentaci√≥n** | **Inconsistencias en la documentaci√≥n t√©cnica**      | Generar documentaci√≥n t√©cnica coherente entre arquitectura de alto nivel, especificaciones de APIs, modelos de datos, diagramas de seguridad y patrones de integraci√≥n                          | **Alta (80%)**      | **Alto (80%)**      | **üü† ALTO**     | **MITIGAR**    | **Prevenci√≥n:** Templates est√°ndar GitHub, peer review obligatorio, checklist calidad por componente<br>**Contingencia:** Auditor√≠a documental semanal viernes 2h, refactoring inmediato de documentos inconsistentes, responsable: Santiago Chaves                                                                                                                                                                                |
| **R04** | **Tiempo**        | **Cronograma optimista para la complejidad**         | El tiempo asignado puede ser insuficiente para dise√±ar completamente todos los componentes t√©cnicos con el nivel de detalle requerido para un sistema de esta magnitud                          | **Muy Alta (100%)** | **Medio (60%)**     | **üü† ALTO**     | **MITIGAR**    | **Prevenci√≥n:** Re-estimaci√≥n semanal con burndown charts, escalaci√≥n autom√°tica si > 20% desv√≠o, buffer de 2 d√≠as por semana<br>**Contingencia:** Redistribuir tareas inmediatamente, asignar 2 personas a componentes cr√≠ticos (Bio Registro y La B√≥veda), reducir documentaci√≥n detallada a documentaci√≥n funcional, completar dise√±o b√°sico de todos los componentes                                                           |
| **R05** | **T√©cnico**       | **Complejidad del motor de transformaci√≥n**                | Especificar t√©cnicamente un motor que procese autom√°ticamente m√∫ltiples formatos, detecte duplicados, relacione datos y aplique transformaciones inteligentes es altamente complejo             | **Media (60%)**     | **Muy Alto (100%)** | **üü† ALTO**     | **MITIGAR**    | **Prevenci√≥n:** Spike 16h Apache Spark + PySpark (Luis David), prototipo 3 casos (CSV‚ÜíPostgreSQL, JSON‚ÜíS3, API‚ÜíDynamoDB), validar 10MB en <30min<br>**Contingencia:** Motor simplificado con AWS Glue + transformaciones predefinidas, o integraci√≥n Talend Open Studio (setup 1 semana)                                                                                                                                           |
| **R06** | **Seguridad**     | **Dise√±o de sistema de cifrado tripartito**          | Especificar correctamente un sistema de llaves criptogr√°ficas divididas entre tres custodios, incluyendo protocolos de recuperaci√≥n y validaci√≥n mancomunada                                    | **Baja (40%)**      | **Muy Alto (100%)** | **üü† ALTO**     | **TRANSFERIR** | **Prevenci√≥n:** Consulta expertos criptograf√≠a (8h consultor√≠a), documentar est√°ndares FIPS 140-2, validaci√≥n externa con especialista<br>**Contingencia:** Implementar cifrado HSM tradicional AWS KMS, esquema dual en lugar de tripartito, mantiene seguridad pero reduce complejidad                                                                                                                                           |
| **R07** | **Integraci√≥n**   | **Interfaces entre componentes mal definidas**       | Riesgo de que las especificaciones de APIs, contratos de datos y protocolos de comunicaci√≥n entre portal, backend y datalake no sean completamente compatibles                                  | **Media (60%)**     | **Alto (80%)**      | **üü† ALTO**     | **MITIGAR**    | **Prevenci√≥n:** Contratos OpenAPI 3.0 obligatorios, reuniones sync bi-semanales martes/viernes, diagramas de secuencia por flujo<br>**Contingencia:** Workshop alineaci√≥n 4h si incompatibilidades detectadas, redise√±o contratos en 2 d√≠as, validaci√≥n cruzada inmediata                                                                                                                                                          |
| **R08** | **Escalabilidad** | **Arquitectura no preparada para la carga esperada** | El dise√±o puede no contemplar adecuadamente el manejo de millones de registros, miles de usuarios concurrentes y procesamiento de grandes vol√∫menes de datos                                    | **Baja (40%)**      | **Medio (60%)**     | **üü° MODERADO** | **MITIGAR**    | **Prevenci√≥n:** Definir l√≠mites t√©cnicos concretos por componente (Bio Registro: 100 req/min, La B√≥veda: 10GB/d√≠a), especificar patrones de escalabilidad (load balancers, auto-scaling), calcular capacidad m√≠nima requerida<br>**Contingencia:** Redise√±ar arquitectura con clustering activo/pasivo, implementar sharding en dise√±o de BD, especificar CDN y caching layers, definir estrategia de particionamiento horizontal  |
| **R09** | **Recursos**      | **Disponibilidad limitada del Product Owner**        | El Product Owner puede no estar disponible para validar decisiones arquitect√≥nicas cr√≠ticas o para resolver ambig√ºedades en los requerimientos t√©cnicos                                         | **Media (60%)**     | **Bajo (40%)**      | **üü° MODERADO** | **ACEPTAR**    | **Prevenci√≥n:** Agenda fija martes/viernes, decisiones escritas en Slack, timeboxing 24h para respuestas<br>**Contingencia:** Escalaci√≥n a stakeholders si > 48h sin respuesta, decisiones t√©cnicas por equipo con validaci√≥n posterior, documentar en GitHub para trazabilidad                                                                                                                                                    |
| **R10** | **Coordinaci√≥n**  | **Dise√±os de componentes desconectados**             | Los diferentes integrantes del equipo pueden dise√±ar sus componentes sin suficiente coordinaci√≥n, resultando en interfaces incompatibles o duplicaci√≥n de funcionalidades                       | **Media (60%)**     | **Medio (60%)**     | **üü° MODERADO** | **MITIGAR**    | **Prevenci√≥n:** Sincronizaci√≥n semanal viernes 1h, documentaci√≥n centralizada GitHub, daily stand-ups 15min<br>**Contingencia:** Workshop alineaci√≥n medio d√≠a si interfaces incompatibles, redise√±o coordinado 3 d√≠as m√°ximo, matriz de dependencias actualizada                                                                                                                                                                  |

### 1.8 Definici√≥n de KPIs

#### KPIs por Hito del Proyecto

##### Hito 1: Planeamiento del Proyecto

**Per√≠odo**: 18-22 Mayo 2025 (Semana W20)

| KPI                              | M√©trica                            | Objetivo | M√©todo de Recolecci√≥n                              |
| -------------------------------- | ---------------------------------- | -------- | -------------------------------------------------- |
| **Cumplimiento de cronograma**   | % de tareas completadas a tiempo   | 100%     | ClickUp - estado de tareas vs. fechas planificadas |
| **Completitud de documentaci√≥n** | % de entregables documentados      | 100%     | Revisi√≥n de README y archivos en GitHub            |
| **Participaci√≥n del equipo**     | % de integrantes activos en tareas | 100%     | ClickUp - asignaci√≥n y progreso de tareas          |
| **Validaci√≥n del Product Owner** | % de entregables aprobados         | 100%     | Estado "Aprobado" en ClickUp                       |

##### Hito 2: Supuestos del Proyecto

**Per√≠odo**: 25-31 Mayo 2025 (Semana W21)

| KPI                            | M√©trica                              | Objetivo | M√©todo de Recolecci√≥n                            |
| ------------------------------ | ------------------------------------ | -------- | ------------------------------------------------ |
| **Cumplimiento de cronograma** | % de tareas completadas a tiempo     | 100%     | ClickUp - comparaci√≥n fecha planificada vs. real |
| **Calidad de supuestos**       | N√∫mero de supuestos validados con PO | 100%     | Documentaci√≥n de validaciones en Slack/GitHub    |
| **Identificaci√≥n de riesgos**  | N√∫mero de riesgos documentados       | ‚â•10      | Matriz de riesgos actualizada                    |

##### Hito 3: Stack Tecnol√≥gico

**Per√≠odo**: 1-7 Junio 2025 (Semana W22)

| KPI                               | M√©trica                                       | Objetivo | M√©todo de Recolecci√≥n                        |
| --------------------------------- | --------------------------------------------- | -------- | -------------------------------------------- |
| **Cumplimiento de cronograma**    | % de tareas completadas a tiempo              | 100%     | ClickUp - estado vs. cronograma              |
| **Decisiones tecnol√≥gicas**       | % de tecnolog√≠as seleccionadas y justificadas | 100%     | Documentaci√≥n t√©cnica en GitHub              |
| **Factibilidad t√©cnica**          | Prototipos de concepto funcionando            | ‚â•2       | Repositorio con ejemplos funcionales         |
| **Compatibilidad con requisitos** | % de requisitos cubiertos por stack           | 100%     | Matriz de trazabilidad requisitos-tecnolog√≠a |

##### Hito 4: Dise√±o de los Componentes

**Per√≠odo**: 8-14 Junio 2025 (Semana W23)

| KPI                               | M√©trica                                   | Objetivo | M√©todo de Recolecci√≥n                 |
| --------------------------------- | ----------------------------------------- | -------- | ------------------------------------- |
| **Cumplimiento de cronograma**    | % de tareas completadas a tiempo          | 100%     | ClickUp - seguimiento diario          |
| **Cobertura de componentes**      | % de componentes dise√±ados vs. requeridos | 100%     | Documentaci√≥n de arquitectura         |
| **Calidad del dise√±o**            | Revisiones aprobadas por PO               | 100%     | Estados de aprobaci√≥n en ClickUp      |
| **Integraci√≥n entre componentes** | % de interfaces definidas                 | 100%     | Diagramas de integraci√≥n documentados |

##### Hito 5: Validaci√≥n de los Requerimientos

**Per√≠odo**: 15-21 Junio 2025 (Semana W24)

| KPI                            | M√©trica                           | Objetivo | M√©todo de Recolecci√≥n             |
| ------------------------------ | --------------------------------- | -------- | --------------------------------- |
| **Cumplimiento de cronograma** | Entrega a tiempo                  | 100%     | Fecha de entrega final            |
| **Cobertura de requisitos**    | % de requisitos validados         | 100%     | Matriz de trazabilidad completa   |
| **Calidad de documentaci√≥n**   | Checklist de atributos completado | 100%     | Revisi√≥n contra checklist oficial |
| **Aprobaci√≥n final**           | Validaci√≥n del Product Owner      | 100%     | Confirmaci√≥n formal de aceptaci√≥n |

#### KPIs Transversales del Proyecto

##### Gesti√≥n y Comunicaci√≥n

| KPI                         | M√©trica                        | Objetivo | Frecuencia de Medici√≥n |
| --------------------------- | ------------------------------ | -------- | ---------------------- |
| **Comunicaci√≥n efectiva**   | Respuestas en Slack < 24h      | 90%      | Semanal                |
| **Reuniones semanales**     | Asistencia a reuniones         | 100%     | Semanal                |
| **Actualizaci√≥n de tareas** | Tareas actualizadas en ClickUp | Diario   | Diario                 |
| **Resoluci√≥n de bloqueos**  | Tiempo promedio de resoluci√≥n  | <48h     | Semanal                |

##### Calidad y Riesgos

| KPI                        | M√©trica                             | Objetivo | Frecuencia de Medici√≥n |
| -------------------------- | ----------------------------------- | -------- | ---------------------- |
| **Gesti√≥n de riesgos**     | % de riesgos con plan de mitigaci√≥n | 100%     | Semanal                |
| **Incidencias cr√≠ticas**   | N√∫mero de riesgos materializados    | 0        | Semanal                |
| **Calidad de entregables** | % de entregables sin retrabajos     | 90%      | Por hito               |

#### Mecanismos de Recolecci√≥n y C√°lculo

##### Herramientas de Monitoreo

1. **ClickUp**: Seguimiento autom√°tico de tareas, tiempos y estados
2. **Slack**: M√©tricas de comunicaci√≥n y tiempo de respuesta
3. **GitHub**: Commits, documentaci√≥n y versiones
4. **Reuniones semanales**: Revisi√≥n manual de KPIs y ajustes


## 2. Supuestos del proyecto

### 2.1 Est√°ndares y Regulaciones

Para el proyecto "Data Pura Vida", la revisi√≥n de est√°ndares y regulaciones nacionales e internacionales es crucial para garantizar la legalidad, seguridad, privacidad y gobernanza de los datos. A continuaci√≥n, se detalla la relevancia de cada una de las normativas mencionadas y c√≥mo se aplican a los requerimientos de la plataforma:

#### 1. Ley 8968 (Costa Rica) - Ley de Protecci√≥n de la Persona frente al Tratamiento de sus Datos Personales

Esta es la normativa nacional fundamental que rige la protecci√≥n de datos personales en Costa Rica. "Data Pura Vida" debe cumplir √≠ntegramente con sus disposiciones, dado que el sistema manejar√° una gran cantidad de datos personales de personas f√≠sicas y jur√≠dicas.

##### Aplicaci√≥n a los Requerimientos de la Plataforma:

##### Bioregistro:

##### ART√çCULO 5.- Principio de consentimiento informado:\*\*REST, GraphQL,

El principio del consentimiento de informaci√≥n se regie por dos puntos importantes, a continuaci√≥n, se mencionan los dos puntos y su aplicaci√≥n dentro de la plataforma:

**Punto 1 - Obligaci√≥n de informar**
Durante el proceso de registro en **Bio Registro Verde**, el sistema debe presentar de forma destacada y f√°cil de entender la siguiente informaci√≥n:

- La existencia de la base de datos **Data Pura Vida**.

- La finalidad clara de la recolecci√≥n de datos (ej. validar identidad para operar en el ecosistema, permitir compartir/consumir datos, etc.).

- Los destinatarios de los datos (ej. otros usuarios del ecosistema con los que el usuario decida compartir, la administraci√≥n de "Data Pura Vida").

- La obligatoriedad de ciertos datos (ej. c√©dula, informaci√≥n tributaria) y las consecuencias de no proporcionarlos (ej. imposibilidad de completar el registro o acceder a ciertas funcionalidades).

- Los derechos ARCO (Acceso, Rectificaci√≥n, Cancelaci√≥n y Oposici√≥n) y c√≥mo ejercerlos dentro del portal. Esta informaci√≥n debe estar disponible antes de que el usuario env√≠e sus datos.

  **Punto 2 - Otorgamiento del consentimiento**
  El consentimiento para el tratamiento de datos debe ser expreso. "Bio Registro Verde" debe implementar un mecanismo de aceptaci√≥n clara y expl√≠cita, como:

- Un checkbox de "Acepto los T√©rminos y Condiciones y la Pol√≠tica de Privacidad" que el usuario debe marcar activamente.

- La documentaci√≥n de este consentimiento debe almacenarse de forma segura, vinculada al registro del usuario.

- La autenticaci√≥n avanzada (identidad digital, biometr√≠a, prueba de vida, MFA) y la validaci√≥n documental automatizada por IA refuerzan la seguridad del proceso de consentimiento, asegurando que la persona que da el consentimiento es quien dice ser.

##### ART√çCULOS 6 y 7 - Principio de calidad de la informaci√≥n; y Derechos que le asisten a la persona( Derechos ARCO ):

Estos principios garantizan que los datos sean apropiados y que los usuarios mantengan el control sobre su informaci√≥n.

**Aplicaci√≥n a la Plataforma:**

Los datos solicitados (informaci√≥n personal, societaria, legal y tributaria) deben ser estrictamente necesarios y pertinentes para la creaci√≥n y operaci√≥n de una cuenta dentro del ecosistema **Data Pura Vida**.

La implementaci√≥n de IA para verificar la completitud y validez de los documentos subidos (c√©dulas, actas, registros tributarios) es clave para asegurar la veracidad y exactitud de los datos, cumpliendo con el Art√≠culo 6 (Principio de Calidad de la Informaci√≥n). Esto tambi√©n ayuda a evitar la recolecci√≥n de datos fraudulentos.

El portal debe ofrecer mecanismos claros y accesibles para que los usuarios puedan acceder, rectificar o solicitar la eliminaci√≥n de sus datos personales, directamente desde su perfil o mediante un proceso de solicitud documentado, en cumplimiento con el Art√≠culo 7 (Derechos ARCO). Esto incluye la posibilidad de actualizar informaci√≥n o cerrar cuentas.

##### ART√çCULO 9 - Categor√≠as particulares de los datos:

Aunque los requerimientos actuales del "Bio Registro Verde" no mencionan expl√≠citamente la recolecci√≥n de "datos sensibles" (como salud, origen racial, etc.), si el alcance de la plataforma evolucionara para incluirlos, "Data Pura Vida" deber√° implementar garant√≠as adicionales y obtener un consentimiento a√∫n m√°s expl√≠cito y espec√≠fico para el tratamiento de estas categor√≠as, seg√∫n lo exige el Art√≠culo 9 (Datos Sensibles).

##### ART√çCULO 10 - Seguridad de los Datos:

Este art√≠culo impone la obligaci√≥n de proteger los datos de car√°cter personal y evitar su alteraci√≥n, destrucci√≥n accidental o il√≠cita, p√©rdida, tratamiento o acceso no autorizado, as√≠ como cualquier otra acci√≥n contraria a esta ley al responsable de la base de datos.

Los requerimientos de seguridad del **Bioregistro** son una respuesta directa al Art√≠culo 10 ( Seguridad de los datos):

- El uso de autenticaci√≥n avanzada (identidad digital, biometr√≠a, prueba de vida, MFA) son medidas de seguridad l√≥gicas para controlar el acceso.

- La asignaci√≥n y protecci√≥n de llaves de seguridad criptogr√°ficas (sim√©tricas y asim√©tricas), incluyendo el sistema de llave tripartita, son medidas de seguridad l√≥gicas esenciales para proteger la integridad y confidencialidad de la identidad y datos asociados.

- La restricci√≥n de acceso al portal solo desde direcciones IP ubicadas en Costa Rica, o mediante listas blancas de IPs institucionales, es una medida de seguridad l√≥gica que limita el acceso geogr√°fico y fortalece la protecci√≥n contra accesos no autorizados.

- El cifrado de datos en reposo y en tr√°nsito y el control de acceso estricto para ingenieros (para evitar acceso a datos en claro) son vitales para cumplir con el deber de confidencialidad y proteger la informaci√≥n sensible.

##### Feliz Compartiendo Datos:

##### ART√çCULO 4.- Autodeterminaci√≥n informativa:

La capacidad de los usuarios para gestionar sus datasets es central para este principio.

La secci√≥n **Feliz Compartiendo Datos** encarna el Art√≠culo 4 (Autodeterminaci√≥n Informativa) al permitir a los usuarios:

- Decidir qu√© datos compartir dentro del ecosistema
- Configurar la visibilidad del dataset (p√∫blico o privado).
- Definir el modelo de acceso (gratuito o pagado).
- Establecer control granular sobre el acceso por instituci√≥n, persona o grupo de actores.

Estas funcionalidades garantizan que el titular mantenga el control sobre el uso y la difusi√≥n de su informaci√≥n.

##### ART√çCULO 6 - Principio de calidad de la informaci√≥n:

La Ley 8968 exige que la recolecci√≥n y uso de datos sea proporcional a la finalidad.

Los requerimientos de **Feliz Compartiendo Datos** se alinean con el Art√≠culo 6 (Principio de Calidad de la Informaci√≥n) al promover la minimizaci√≥n y el prop√≥sito limitado:

- La opci√≥n de seleccionar campos espec√≠ficos a cifrar dentro del dataset permite a los usuarios proteger solo la informaci√≥n sensible, sin necesidad de cifrar todo, lo que se alinea con la minimizaci√≥n del tratamiento de datos sensibles.

- La capacidad de restringir acceso a datos por l√≠mites de tiempo, volumen o frecuencia de consulta asegura que el acceso y uso de los datos se realice √∫nicamente para la finalidad acordada y bajo las condiciones definidas por el titular.

##### ART√çCULO 14 - Transferencia de datos personales, regla general:

Si bien la "comercializaci√≥n" dentro del ecosistema se enfoca en el acceso y consumo interno, el Art√≠culo 20 es una consideraci√≥n preventiva. Si la plataforma habilitara en el futuro transferencias a entidades o servicios fuera de Costa Rica (por ejemplo, para alimentar modelos de IA en la nube en otros pa√≠ses), se deber√≠an cumplir las condiciones establecidas por este art√≠culo, que incluyen el consentimiento del titular y garant√≠as de seguridad adecuadas para los datos transferidos.

##### Descubriendo Costa Rica:

##### ART√çCULOS 4 y 10 - Autodeterminaci√≥n informativa; Seguridad de los datos:

La protecci√≥n de la autodeterminaci√≥n informativa y la seguridad son cruciales en la visualizaci√≥n.

La secci√≥n **Descubriendo Costa Rica** refuerza el Art√≠culo 4 (Autodeterminaci√≥n Informativa) y el Art√≠culo 11 (Seguridad de los datos) al:

- Impedir la descarga directa de datos en cualquier momento y bloquear exportaciones de gr√°ficos y contenidos. Esta medida es fundamental para mantener el control del titular sobre la informaci√≥n y prevenir usos no autorizados fuera del entorno seguro de la plataforma.

- Al obligar a la visualizaci√≥n exclusivamente dentro del portal, "Data Pura Vida" implementa una medida de seguridad l√≥gica que reduce el riesgo de fugas de datos y asegura que el uso de la informaci√≥n est√© bajo la gobernanza y protecci√≥n de la Ley 8968. Esto tambi√©n apoya el principio de limitaci√≥n de la finalidad.

##### Backend API y Datalake: Aplicaci√≥n de la Ley 8968 (Costa Rica)

##### Art√≠culo 10 y 30 - Seguridad de los datos; Faltas graves:

Estos son el muy importantes para la infraestructura de seguridad.

Los requerimientos del Backend API y el Datalake son directamente aplicables al Art√≠culo 10 (Seguridad de los datos) y al Art√≠culo 30 (Faltas Graves):

La exigencia de que "Data Pura Vida" implemente medidas t√©cnicas y organizativas para asegurar la protecci√≥n de los datos se refleja en:

- La protecci√≥n de la API mediante whitelist de IPs, validaci√≥n de tokens y MFA.

- Los m√≥dulos separados para gesti√≥n de credenciales, firmas y cifrado de datos.

- La trazabilidad, cumplimiento legal y control de cada transacci√≥n, lo que demuestra un enfoque proactivo en la gesti√≥n de la seguridad.

- La implementaci√≥n de RBAC (Role-Based Access Control) y RLS (Row-Level Security) en el Datalake, junto con el cifrado de datos sensibles en reposo y en tr√°nsito, son medidas de seguridad robustas para proteger la confidencialidad e integridad de la informaci√≥n, previniendo accesos indebidos.

- La prohibici√≥n expl√≠cita de que ingenieros o personal t√©cnico accedan a los datos en claro o sin autorizaci√≥n refuerza el principio de "conocimiento necesario" y el deber de confidencialidad del Art√≠culo 10.

La auditor√≠a detallada de todas las operaciones realizadas en el sistema (por usuario, acci√≥n, fecha y efecto) y el historial de consumo de datos son esenciales para:

- Demostrar el cumplimiento con la Ley 8968 ante la PRODHAB o en caso de una auditor√≠a.

- Permitir la trazabilidad necesaria para la rendici√≥n de cuentas y la identificaci√≥n de cualquier actividad irregular, ayudando a prevenir faltas graves como el tratamiento no autorizado de datos.

- Facilitar la extracci√≥n de evidencias para procesos legales o regulatorios.

#### 2. GDPR (General Data Protection Regulation)

Aunque es una regulaci√≥n de la Uni√≥n Europea, el GDPR tiene un alcance extraterritorial. Si "Data Pura Vida" procesa datos de ciudadanos o residentes de la UE, o si ofrece bienes y servicios a ellos, entonces el GDPR es aplicable, independientemente de d√≥nde se encuentre el servidor o la empresa. Dado que Costa Rica es un destino tur√≠stico y centro de negocios internacional, es muy probable que haya interacci√≥n con datos de la UE. Adem√°s, el GDPR ha influenciado muchas leyes de privacidad a nivel mundial, por lo que su cumplimiento a menudo supera los requisitos de otras normativas locales.

##### Aplicaci√≥n a los Requerimientos de la Plataforma:

##### Bio Registro Verde:

**Bases Legales para el Tratamiento:** El registro debe establecer claramente la base legal para el procesamiento de cada tipo de dato (consentimiento, obligaci√≥n legal, inter√©s leg√≠timo, etc.) seg√∫n lo establece el GDPR (art√≠culos 6 y 7). El consentimiento debe ser libre, espec√≠fico, informado e inequ√≠voco, y f√°cilmente revocable.

**Privacy by Design and by Default:** El dise√±o del sistema debe integrar la privacidad desde el inicio (por ejemplo, el cifrado de datos, el control granular de acceso, la minimizaci√≥n de datos por defecto), como exige el art√≠culo 25 del GDPR.

**Derechos de los Interesados:** El GDPR otorga derechos robustos a los interesados (data subjects):

- **Derecho a la Informaci√≥n:** Transparencia sobre el procesamiento de datos (art√≠culos 13 y 14 del GDPR).

- **Derecho de Acceso:** Los usuarios deben poder acceder a sus datos personales (Art√≠culo 15).

- **Derecho de Rectificaci√≥n:** Correcci√≥n de datos inexactos (Art√≠culo 16).

- **Derecho de Supresi√≥n ("Derecho al Olvido"):** Eliminaci√≥n de datos bajo ciertas condiciones (Art√≠culo 17).

- **Derecho a la Limitaci√≥n del Tratamiento:** Restringir el procesamiento de datos (Art√≠culo 18).

- **Derecho a la portabilidad de los Datos:** Permite a los usuarios r recibir y transmitir sus datos en un formato estructurado (art√≠culo 20).

- **Derecho de Oposici√≥n:** Oponerse al tratamiento de sus datos (Art√≠culo 21).

- **Derechos en relaci√≥n con Decisiones Automatizadas y Perfilado:** El uso de IA debe considerar el derecho a no ser objeto de decisiones basadas √∫nicamente en procesamiento automatizado (art√≠culo 22).

##### Feliz Compartiendo Datos:

**Transferencias Internacionales de Datos:** Si los datos pudieran ser accedidos o transferidos fuera del Espacio Econ√≥mico Europeo, deben cumplirse los requisitos del Cap√≠tulo V del GDPR (art√≠culos 44 al 50), incluyendo garant√≠as como cl√°usulas tipo o reglas corporativas vinculantes.

**Evaluaci√≥n de Impacto de Protecci√≥n de Datos (DPIA):** Para el procesamiento de datos de alto riesgo (como la combinaci√≥n de grandes vol√∫menes de datos sensibles, uso de IA para perfilado), una DPIA ser√≠a obligatoria bajo el GDPR (Art√≠culo 35).

##### Backend API y Datalake:

**Oficial de Protecci√≥n de Datos (DPO):** Si se cumple con los criterios (ej. procesamiento a gran escala de categor√≠as especiales de datos o monitoreo sistem√°tico de interesados), Data Pura Vida deber√≠a designar un Oficial de Protecci√≥n de Datos (DPO) (Art√≠culo 37).

**Notificaci√≥n de Violaciones de Seguridad:** En caso de una brecha de seguridad que afecte datos personales, el GDPR exige la notificaci√≥n a la autoridad de control en un plazo de 72 horas y, en ciertos casos, tambi√©n a los interesados (Art√≠culos 33 y 34). Esto implica un robusto sistema de monitoreo y respuesta a incidentes.

#### 3. ISO/IEC 27001 - Sistemas de Gesti√≥n de la Seguridad de la Informaci√≥n (SGSI)

Relevancia para **Data Pura Vida**: Aunque no es una ley obligatoria, la ISO/IEC 27001 es un est√°ndar internacional que proporciona un marco para establecer, implementar, mantener y mejorar continuamente un Sistema de Gesti√≥n de la Seguridad de la Informaci√≥n (SGSI). Obtener la certificaci√≥n ISO 27001 demostrar√≠a un compromiso serio con la seguridad de la informaci√≥n y la protecci√≥n de activos, generando confianza en un ecosistema de datos.

##### Aplicaci√≥n a los Requerimientos de la Plataforma:

La ISO 27001 se basa en la identificaci√≥n y gesti√≥n de riesgos de seguridad de la informaci√≥n. Esto es fundamental para "Data Pura Vida", dada la sensibilidad y el volumen de los datos. Se debe realizar una evaluaci√≥n de riesgos exhaustiva para determinar las medidas de seguridad necesarias.

Estan los controles de seguidad ubicadas en el anexo A de ISO 27001/ISO 27002 en la cual se ubican el est√°ndar para un conjunto de controles que son relevantes para todos los requerimientos de la plataforma:

- **A.5 Pol√≠ticas de seguridad de la informaci√≥n:** Definir pol√≠ticas claras para el uso y acceso de datos.
- **A.6 Organizaci√≥n de la seguridad de la informaci√≥n:** Definir roles y responsabilidades (ej. PM, roles del Backoffice).
- **A.6.2.3 Tratamiento de la seguridad en contratos con terceras personas:** Gesti√≥n de la seguridad con terceros si se utilizan servicios externos.
- **A.7 Gesti√≥n de activos:** Clasificaci√≥n de la informaci√≥n (p√∫blico/privado, gratuito/pagado), y protecci√≥n de activos.
- **A.8 Seguridad de los recursos humanos:** Seguridad antes, durante y despu√©s del empleo (fundamental para el personal de Data Pura Vida, incluyendo ingenieros y personal de backoffice).
- **A.10.6 Gesti√≥n de seguridad de redes:** Protecci√≥n de la informaci√≥n en redes.
- **A.10.10 Monitoreo:** Gesti√≥n de logs (auditor√≠a), monitoreo de sistemas, gesti√≥n de vulnerabilidades.
- **A.11 Control de acceso:** Implementaci√≥n de RBAC, RLS, MFA, validaci√≥n de tokens, whitelists de IPs, y la asignaci√≥n/revocaci√≥n de llaves de seguridad. La "llave tripartita" es un control de seguridad avanzado.
- **A.12.3 Controles criptogr√°ficos:** El uso de cifrado para datos en reposo y en tr√°nsito, y el cifrado de campos espec√≠ficos, son controles clave.
- **A.13 Gesti√≥n de incidentes de seguridad de la informaci√≥n:** Proceso para detectar, reportar, responder y aprender de los incidentes (crucial para notificar brechas como exige GDPR).
- **A.14.1 Aspectos de seguridad de la informaci√≥n de la gesti√≥n de la continuidad del negocio:** Planes de respaldo y recuperaci√≥n.
- **A.15 Cumplimiento:** Identificaci√≥n y cumplimiento de requisitos legales y contractuales.

**Mejora Continua (Ciclo PDCA):** ISO 27001 promueve un ciclo de Planificar-Hacer-Verificar-Actuar (PDCA), lo que se alinea con la necesidad de monitoreo continuo de m√©tricas, auditor√≠as y revisi√≥n de la direcci√≥n para garantizar la mejora continua de la seguridad y el cumplimiento normativo.

#### 4. OECD Data Governance

La OCDE establece principios fundamentales de gobernanza de datos que sirven como referencia para proyectos como Data Pura Vida, orientados a maximizar el uso y la compartici√≥n responsable de datos, mientras se protege la privacidad y se fortalece la confianza.

##### Principios Fundamentales de la OCDE

##### Enfoque Integral (Whole-of-Government)

- Promueve la participaci√≥n de todos los actores (p√∫blicos y privados) y la coherencia entre sectores y niveles de gobierno.

##### Equilibrio de Beneficios y Riesgos

- Reconoce la necesidad de equilibrar los beneficios del acceso y uso de datos con los riesgos asociados (privacidad, seguridad, propiedad intelectual).

##### Diversidad de Datos y Respeto a Derechos

- Reconoce distintos niveles de sensibilidad y riesgo de los datos, y garantiza derechos como el acceso, la rectificaci√≥n y la autodeterminaci√≥n informativa.

##### Fortalecimiento de Capacidades y Confianza

- Fomenta la cultura de datos, el desarrollo de infraestructura y el establecimiento de relaciones de confianza entre actores.

##### Recomendaciones de la OCDE Aplicables

La OCDE ha emitido siete recomendaciones que sirven como marco para la gobernanza de datos y que deben integrarse a Data Pura Vida:

- Acceso a datos de investigaci√≥n financiados p√∫blicamente (2006).
- Acceso y uso de informaci√≥n del sector p√∫blico (2008).
- Protecci√≥n de privacidad y flujos transfronterizos de datos (2013).
- Gobernanza de datos de salud (2016).
- Estrategias de gobierno digital (2014).

##### Aplicaci√≥n a los Requerimientos de la Plataforma

##### Bio Registro Verde

Aplica el enfoque integral al registrar a todos los actores relevantes con autenticaci√≥n avanzada (MFA, biometr√≠a, prueba de vida).

Implementa controles criptogr√°ficos (llaves sim√©tricas y asim√©tricas) y segmentaci√≥n de acceso por roles, siguiendo los est√°ndares de confianza y seguridad.

##### Feliz Compartiendo Datos

Permite la clasificaci√≥n de datasets (p√∫blicos, privados, gratuitos o pagados) y la definici√≥n de controles de acceso granular, balanceando beneficios de compartici√≥n con protecci√≥n de derechos.

Soporta m√∫ltiples formatos de carga y mecanismos de conexi√≥n, fomentando la interoperabilidad.

##### Descubriendo Costa Rica

Limita la descarga directa y exportaci√≥n de datos, asegurando que el acceso a datos se haga solo en entornos seguros y controlados.

Construcci√≥n de dashboards personalizables con visibilidad granular, respetando la autonom√≠a de los usuarios y el principio de minimizaci√≥n.

##### Backend API y Datalake

Implementaci√≥n de MFA, whitelists de IPs y control de acceso estricto para proteger la confidencialidad y cumplir con las recomendaciones de seguridad de la OCDE.

Uso de IA para normalizaci√≥n, relaci√≥n de datos y detecci√≥n de duplicidades, reforzando la calidad y eficiencia de la gobernanza de datos.

La implementaci√≥n de **Data Pura Vida** no solo debe enfocarse en la funcionalidad, sino que debe tener la privacidad y seguridad integradas desde el dise√±o. El cumplimiento de la Ley 8968 es mandatorio para operar en Costa Rica. La incorporaci√≥n de principios del GDPR y ISO 27001 garantizar√° un nivel de protecci√≥n de datos de clase mundial y facilitar√° la confianza, mientras que las directrices de la OCDE proporcionar√°n la base para una gobernanza de datos efectiva y una promoci√≥n responsable del intercambio y uso de la informaci√≥n.

#### 5. Checklist para el Equipo de Desarrollo de "Data Pura Vida"

Este checklist tiene como objetivo presentar los requisitos legales y de seguridad de **Data Pura Vida** en acciones concretas para el equipo de desarrollo, asegurando el cumplimiento con la Ley 8968, GDPR, ISO/IEC 27001 y los principios de la OCDE.

##### Datalake

###### Cifrado de Datos:

- [ ] Implementar cifrado en reposo para todos los datos sensibles en el Datalake.
- [ ] Implementar cifrado en tr√°nsito para todas las comunicaciones hacia y desde el Datalake.
- [ ] Asegurar que los campos espec√≠ficos marcados como sensibles puedan ser cifrados a nivel de campo.

###### Control de Acceso:

- [ ] Configurar RBAC (Role-Based Access Control) para todos los usuarios y servicios que interact√∫an con el Datalake, otorgando el m√≠nimo privilegio necesario.
- [ ] Implementar RLS (Row-Level Security) para asegurar que los usuarios solo puedan ver las filas de datos a las que tienen autorizaci√≥n expl√≠cita.
- [ ] Asegurar que ning√∫n ingeniero o personal t√©cnico pueda acceder a los datos en claro sin autorizaci√≥n.

###### Calidad y Gobernanza de Datos:

- [ ] Implementar mecanismos de validaci√≥n de datos en el punto de entrada para asegurar la calidad y exactitud.
- [ ] Desarrollar y aplicar algoritmos de IA para normalizaci√≥n, relaci√≥n de datos y detecci√≥n de duplicidades.

###### Auditor√≠a y Trazabilidad:

- [ ] Implementar auditor√≠a detallada de todas las operaciones de CRUD (Crear, Leer, Actualizar, Borrar) en el Datalake, registrando usuario, acci√≥n, fecha, hora y efecto.
- [ ] Mantener un historial de consumo de datos por parte de los usuarios y servicios.

##### Backend API

###### Seguridad de la API:

- [ ] Proteger la API con whitelist de IPs (si aplica, para IPs institucionales o de Costa Rica).
- [ ] Implementar un robusto sistema de validaci√≥n de tokens (ej. JWT) para todas las solicitudes.
- [ ] Exigir Multi-Factor Authentication (MFA) para el acceso a la API para usuarios administrativos o con privilegios elevados.

###### Gesti√≥n de Credenciales y Criptograf√≠a:

- [ ] Desarrollar m√≥dulos separados para la gesti√≥n de credenciales, firmas y cifrado de datos.
- [ ] Implementar el sistema de llave tripartita para la protecci√≥n de identidades y datos asociados.

###### Registro y Monitoreo:

- [ ] Asegurar la trazabilidad y registro de cada transacci√≥n que pase por la API.
- [ ] Implementar monitoreo continuo de la API para detectar actividades an√≥malas o intentos de acceso no autorizado.

###### Transferencia de Datos:

- [ ] Si hay transferencia de datos fuera de Costa Rica, asegurar que se cumplen las garant√≠as de seguridad.

##### Interfaz de Usuario (UI) - Bio Registro Verde

###### Consentimiento Informado (Ley 8968 Art√≠culo. 5, GDPR Art√≠culos. 6 y 7):

- [ ] Dise√±ar una secci√≥n clara y destacada en el registro para informar sobre:

  - La existencia de "Data Pura Vida" y su finalidad.
  - Los destinatarios de los datos.
  - La obligatoriedad de ciertos datos y sus consecuencias.
  - Los derechos ARCO y c√≥mo ejercerlos.

- [ ] Implementar un checkbox expl√≠cito de "Acepto los T√©rminos y Condiciones y la Pol√≠tica de Privacidad" que el usuario debe marcar activamente.
- [ ] Almacenar de forma segura la documentaci√≥n del consentimiento vinculada al registro del usuario.

###### Autenticaci√≥n y Validaci√≥n:

- [ ] Integrar identidad digital, biometr√≠a o prueba de vida en el proceso de autenticaci√≥n inicial.
- [ ] Implementar MFA para el acceso de los usuarios a sus cuentas.
- [ ] Integrar validaci√≥n documental automatizada por IA para verificar la completitud y validez de documentos (ej. c√©dulas, etc.).

###### Derechos ARCO (Acceso, Rectificaci√≥n, Cancelaci√≥n y Oposici√≥n) (Ley 8968 Art√≠culo. 7, GDPR Art√≠culos. 15-21):

- [ ] Proporcionar un mecanismo claro y accesible en el perfil del usuario para:

  - Acceder a sus datos personales.
  - Rectificar datos inexactos.
  - Solicitar la eliminaci√≥n de datos (Derecho al Olvido), con la l√≥gica de negocio asociada.

- [ ] Si aplica, ofrecer opciones para limitar el tratamiento y ejercer la portabilidad de datos.
- [ ] Considerar el derecho a oponerse a decisiones basadas √∫nicamente en procesamiento automatizado si la IA afecta decisiones legales significativas sobre el usuario.

###### Privacidad de Datos (Ley 8968 Art√≠culo. 6, GDPR Art√≠culo. 25):

- [ ] Asegurar que sistema integre la privacidad desde el inicio (ej. el cifrado de datos, el control granular de acceso, la minimizaci√≥n de datos por defecto).

##### Interfaz de Usuario (UI) - Feliz Compartiendo Datos

###### Autodeterminaci√≥n Informativa (Ley 8968 Art√≠culo. 4):

- [ ] Desarrollar funcionalidades para que el usuario pueda:
      Decidir qu√© datasets compartir.

  - Configurar la visibilidad del dataset (p√∫blico/privado).
  - Definir el modelo de acceso (gratuito/pagado).
  - Establecer control granular sobre el acceso por instituci√≥n, persona o grupo de actores.

- [ ] Permitir la selecci√≥n de campos espec√≠ficos a cifrar dentro del dataset compartido.

- [ ] Habilitar la capacidad de restringir el acceso a datos por l√≠mites de tiempo, volumen o frecuencia de consulta.

###### Interoperabilidad (Principios OCDE):

- [ ] Soportar m√∫ltiples formatos de carga y mecanismos de conexi√≥n para datasets.

##### Interfaz de Usuario (UI) - Descubriendo Costa Rica

###### Seguridad en Visualizaci√≥n (Ley 8968 Art√≠culo. 10):

- [ ] Bloquear la descarga directa de datos desde los dashboards o visualizaciones.
- [ ] Impedir la exportaci√≥n de gr√°ficos y contenidos a formatos externos.
- [ ] Asegurar que la visualizaci√≥n de datos solo sea posible dentro del entorno seguro del portal.

###### Control Granular y Personalizaci√≥n:

- [ ] Permitir la construcci√≥n de dashboards personalizables por los usuarios.
- [ ] Asegurar que la visibilidad granular aplicada en "Feliz Compartiendo Datos" se refleje correctamente en las visualizaciones.

##### Seguridad General y Operaciones

###### Pol√≠ticas y Procedimientos (ISO 27001 A.5, A.6, A.8):

- [ ] Colaborar con el equipo de PM/Seguridad para la implementaci√≥n de las pol√≠ticas de seguridad de la informaci√≥n.
- [ ] Asegurar que el personal de desarrollo (ingenieros, backoffice) cumpla con los controles de seguridad antes, durante y despu√©s del empleo.

###### Controles de Acceso L√≥gico (ISO 27001 A.11):

- [ ] Restringir el acceso al portal solo desde direcciones IP ubicadas en Costa Rica o a trav√©s de listas blancas de IPs institucionales.

###### Monitoreo y Gesti√≥n de Incidentes (ISO 27001 A.10.10, A.13, GDPR Art√≠culos. 33 y 34):

- [ ] Implementar monitoreo de sistemas y gesti√≥n de logs para todas las plataformas.
- [ ] Desarrollar un proceso claro y automatizado para la detecci√≥n, reporte y respuesta a incidentes de seguridad.
- [ ] Preparar la capacidad t√©cnica para notificar brechas de seguridad a la autoridad de control (PRODHAB, DPA de la UE) y a los interesados dentro de los plazos establecidos (ej. 72 horas para GDPR).

##### Cifrado General (ISO 27001 A.12.3):

- [ ] Asegurar el uso de cifrado para todos los datos en reposo y en tr√°nsito a trav√©s de la plataforma.

###### Pruebas de Seguridad:

- [ ] Realizar pruebas de penetraci√≥n y escaneos de vulnerabilidades de forma regular.
- [ ] Incluir pruebas de seguridad en el ciclo de vida de desarrollo de software.

###### Continuidad del Negocio (ISO 27001 A.14.1):

- [ ] Implementar planes de respaldo y recuperaci√≥n para todos los componentes cr√≠ticos del sistema.

##### Gobernanza de Datos y Cumplimiento

###### Auditor√≠a Interna y Externa:

- [ ] Estar preparado para auditor√≠as internas y externas para demostrar el cumplimiento con la Ley 8968, GDPR e ISO 27001.
- [ ] Asegurar la disponibilidad de evidencias (logs, configuraciones, pol√≠ticas) para procesos legales o regulatorios.

###### Documentaci√≥n:

- [ ] Mantener una documentaci√≥n actualizada de la arquitectura de seguridad, controles implementados y flujos de datos.

### 2.2 Pr√°cticas de Manejo de C√≥digo

Para garantizar que el c√≥digo fuente de Data Pura Vida sea seguro, mantenible y escalable, se adoptan tres marcos principales de buenas pr√°cticas:

#### 1. OWASP Secure Coding Practices

Basados en los est√°ndares de OWASP Top 10 y OWASP ASVS:

- Validaci√≥n de entradas:

  - Se realiza con el objetivo de que solo los datos parametrizados correctamente entren al workflow de Data Pura Vida.
  - Se evita que malformaciones de datos entren en las bases de datos y realicen un efecto en cadena de malfuncionamientos.

- Control de acceso y privilegios m√≠nimos:

  - El objetivo es asegurar que cada usuario solo tenga acceso a los recursos estrictamente necesarios para cumplir su funci√≥n.
  - Se evitar√° la acumulaci√≥n de privilegios innecesarios a lo largo del tiempo mediante revisiones peri√≥dicas.
  - Todas las solicitudes ser√°n validadas en el backend, independientemente de su origen.
  - En lugar de depender exclusivamente de roles (RBAC) tambi√©n se hara el uso de ABAC (Attribute-Based Access Control).

- Almacenamiento y transmisi√≥n segura:
  - Todos los datos sensibles ser√°n cifrados en tr√°nsito mediante TLS 1.3 y en reposo mediante AES-256.
  - **TLS 1.3 (Transport Layer Security):** protocolo criptogr√°fico utilizado para proteger los datos en tr√°nsito entre el cliente y el servidor. Asegura la confidencialidad e integridad de la informaci√≥n, evitando ataques de MITM(Man in the middle). - **AES-256 (Advanced Encryption Standard):** algoritmo de cifrado sim√©trico que se usa para proteger los datos almacenados en el sistema. Utiliza una clave de 256 bits, lo que lo hace extremadamente resistente a ataques de fuerza bruta. Es uno de los est√°ndares m√°s seguros y reconocidos a nivel mundial.
    - El sistema implementar√° gesti√≥n segura de llaves y almacenamiento segmentado para evitar accesos no autorizados, incluso desde el personal t√©cnico.
- Manejo seguro de errores:

  - Los mensajes de error visibles para el usuario ser√°n gen√©ricos, evitando revelar informaci√≥n del sistema.
  - Los errores ser√°n registrados en logs internos enmascarados, permitiendo an√°lisis posterior sin comprometer datos sensibles.

- Protecci√≥n ante dependencias vulnerables:

  - Se integrar√° un escaneo automatizado de dependencias y librer√≠as de terceros en el pipeline de CI/CD.
  - Las versiones ser√°n actualizadas de forma peri√≥dica y se restringir√° el uso de paquetes sin mantenimiento o con vulnerabilidades conocidas.

- Autenticaci√≥n robusta:
  - El sistema implementar√° OAuth2 para autorizaci√≥n y JWT como mecanismo de sesi√≥n segura.
    - **OAuth2 (Open Authorization 2.0):** es un protocolo de autorizaci√≥n que permite a una aplicaci√≥n acceder a recursos en nombre del usuario sin necesidad de compartir sus credenciales.
    - **JWT (JSON Web Token):** es un formato compacto y seguro para transmitir informaci√≥n entre partes como un objeto JSON firmado. Se usa para manejar sesiones de forma segura, ya que contiene los datos del usuario y sus permisos codificados y firmados digitalmente, lo que permite validar su autenticidad sin necesidad de consultar una base de datos en cada petici√≥n.
  - Adem√°s, toda cuenta que acceda a √°reas cr√≠ticas deber√° utilizar autenticaci√≥n multifactor (MFA), y se exigir√° prueba de vida y biometr√≠a en el registro de representantes legales.

#### 2. Clean code

El proyecto aplicar√° los principios fundamentales de Clean Code propuestos por Robert C. Martin y enriquecidos con buenas pr√°cticas reconocidas de la industria, con el objetivo de asegurar un c√≥digo legible, mantenible y confiable.

**Principios clave aplicados:**

- **Funciones peque√±as y espec√≠ficas:** Cada funci√≥n debe realizar una sola tarea de forma clara.

- **Nombres claros y sem√°nticos:** Variables, funciones y clases deben ser autodescriptivas.

- **Evitar c√≥digo innecesario:** Se eliminar√° c√≥digo muerto o comentarios obvios.

- **Regla del Boy Scout:** Dejar el c√≥digo m√°s limpio de como se encontr√≥.

- **Manejo expl√≠cito de errores:** Nunca se deben silenciar excepciones.

- **Evitar condicionales complejos:** Preferir polimorfismo sobre if o switch.

- **Separaci√≥n vertical:** Agrupar c√≥digo relacionado en bloques visuales cercanos.

- **Ley de Demeter:** Cada clase solo debe conocer sus dependencias directas.

**Implementaci√≥n concreta en el proyecto:**

- **Uso de linters y formatters:** Se utilizaran para validar estilo de c√≥digo en cada lenguaje usado (ESLint, SonarQube, entre otros).

- **Refactorizaci√≥n continua:** Esta pr√°ctica ser√° integrada como una actividad habitual dentro del flujo de trabajo del equipo.

- **Validaciones reutilizables:** Se implementar√°n middlewares espec√≠ficos para validar tokens, estructura de datos y permisos, evitando duplicaci√≥n de l√≥gica (principio DRY: Don‚Äôt Repeat Yourself).

- **Inyecci√≥n de dependencias:** Los servicios cr√≠ticos como cifrado, autenticaci√≥n y conexi√≥n a base de datos se gestionar√°n por inyecci√≥n de dependencias, lo que facilita el testing y desacopla las capas del sistema.

- **Controladores REST claros:** Los endpoints de la API seguir√°n convenciones sem√°nticas (`GET /datasets`, `POST /register`, etc.) y estar√°n separados en archivos por entidad, facilitando su lectura y mantenimiento.

- **Nombres de funciones como verbos y de clases como sustantivos:**
  Por ejemplo: `validateInput()`, `encryptDataset()`, `DatasetUploader`, `IbanVerifier`.

- **Uso de constantes centralizadas:** Valores como `MAX_UPLOAD_SIZE_MB`, `SUPPORTED_FORMATS`, `DEFAULT_LANGUAGE` estar√°n definidos en archivos de configuraci√≥n o constantes globales.

- **Errores bien definidos y gestionados:** Todos los errores tendr√°n estructuras estandarizadas `({code, message, details})` y ser√°n manejados con `try/catch` en backend, registrando trazas seguras en logs sin exponer detalles internos al usuario.

Estas pr√°cticas no solo mejoran la calidad del software, sino que tambi√©n reducen los costos de mantenimiento, facilitan las auditor√≠as de seguridad y mejoran la experiencia del equipo de desarrollo durante todo el ciclo de vida del sistema.

#### 3. The Twelve-Factor App

El desarrollo de Data Pura Vida tambi√©n se alinea con los principios del manifiesto Twelve-Factor App, con el objetivo de garantizar una arquitectura moderna, portable, escalable y apta para despliegues en la nube o entornos h√≠bridos. A continuaci√≥n se detallan los factores aplicados:

1. **C√≥digo base:** El c√≥digo est√° centralizado en un √∫nico repositorio de GitHub con control de versiones. Cada microservicio o m√≥dulo mantiene su propio namespace l√≥gico.

2. **Dependencias expl√≠citas:** Se utiliza un package manager en cada stack (como npm, pip, o cargo) y un archivo de lock (package-lock.json, Pipfile.lock, etc.) para gestionar y auditar todas las dependencias.

3. **Configuraci√≥n separada:** Variables sensibles como claves de API, cadenas de conexi√≥n y llaves criptogr√°ficas se mantienen fuera del c√≥digo, gestionadas mediante variables de entorno y servicios seguros de secretos (como AWS Secrets Manager o .env en desarrollo).

4. **Servicios externos como recursos:** Todas las bases de datos, colas, y almacenamiento externo se consideran recursos gestionados a trav√©s de URLs o credenciales inyectadas din√°micamente, lo que permite su intercambio sin afectar el c√≥digo.

5. **Construcci√≥n, ejecuci√≥n y publicaci√≥n desacopladas:** Se definen pipelines de CI/CD que separan claramente la fase de build (npm run build), la fase de ejecuci√≥n (node app.js) y el entorno de despliegue (Docker/OCI).

6. **Procesos sin estado:** El backend es stateless. Toda la sesi√≥n del usuario se gestiona mediante JWT y los archivos temporales se almacenan en servicios externos como S3, no en disco local.

7. **Vinculaci√≥n de puertos:** Cada componente expone sus servicios por medio de puertos est√°ndar (PORT=3000) facilitando integraci√≥n y despliegue en contenedores.

8. **Concurrencia:** Se utilizan workers y multiprocesamiento para escalar horizontalmente los m√≥dulos de procesamiento intensivo, como el motor de validaci√≥n ETDL y el generador de dashboards.

9. **Descarte r√°pido:** Las apps manejan se√±ales del sistema operativo (SIGTERM, SIGINT) y liberan recursos como conexiones o archivos abiertos. Esto permite ciclos de despliegue seguros y r√°pidos.

10. **Entornos de desarrollo y producci√≥n iguales:** Se utilizan contenedores (Docker) para asegurar que el c√≥digo se ejecute de forma id√©ntica en desarrollo, staging y producci√≥n.

11. **Logs como streams:** Los logs no se almacenan en archivos, sino que se emiten a stdout/stderr y se redirigen a herramientas de agregaci√≥n como Elastic Stack o Grafana Loki.

12. **Procesos administrativos como tareas one-off:** Scripts de migraci√≥n, pruebas y carga inicial de datos se ejecutan como procesos independientes (npm run seed, python manage.py migrate), no embebidos en el ciclo de vida principal de la app.

#### Buenas Pr√°cticas Complementarias de Codificaci√≥n Segura

| Objetivo                      | Pr√°ctica                                       | Aplicaci√≥n                                                                       |
| ----------------------------- | ---------------------------------------------- | -------------------------------------------------------------------------------- |
| **Visibilidad y detecci√≥n**   | Logs + monitoreo en tiempo real                | Uso de Prometheus y Alertmanager para monitoreo                                  |
| **Seguridad en dependencias** | Escaneo continuo y alertas autom√°ticas         | GitHub Dependabot activado                                                       |
| **Gesti√≥n de secretos**       | Manejo seguro de claves, tokens y credenciales | Uso de servicios como AWS Secrets Manager o archivos .env con acceso restringido |
| **Protecci√≥n de endpoints**   | CORS y rate-limiting                           | Configuraci√≥n estricta de origen cruzado (CORS) y l√≠mites de solicitudes por IP  |

#### Validaci√≥n automatizada de c√≥digo

Con el fin de reforzar las pr√°cticas de codificaci√≥n segura, mantenible y coherente, el proyecto Data Pura Vida adoptar√° una estrategia de validaci√≥n automatizada. Esta estrategia abarcar√° tanto el estilo y la calidad del c√≥digo como la seguridad y el cumplimiento de reglas internas de desarrollo.

**1. Linter personalizado y estilizaci√≥n de c√≥digo:**

- Se usar√° `ESLint` como base para c√≥digo en Node.js, incorporando lo siguiente:

  - Principios de Clean Code.

  - Recomendaciones OWASP para codificaci√≥n segura.

  - Reglas adicionales generadas con ayuda de una IA especializada en refactorizaci√≥n.

- Adem√°s, se incluir√° `Prettier` para formateo del c√≥digo. Las reglas podr√°n ser vistas en archivos con los siguientes formatos:

  - `/.config/lint/eslintrc.js`

  - `/docs/estandares-codigo.md`

- A continuaci√≥n, un ejemplo de un archivo `.eslintrc.js` que funciona para un proyecto Node.js:

```js
module.exports = {
  root: true,
  env: {
    node: true,
    es2021: true,
  },
  extends: [
    "eslint:recommended",
    "plugin:security/recommended", // OWASP rules
    "airbnb-base", // Airbnb style guide
    "plugin:prettier/recommended",
  ],
  plugins: ["security", "prettier"],
  parserOptions: {
    ecmaVersion: 12,
    sourceType: "module",
  },
  rules: {
    // --- Clean Code ---
    "no-unused-vars": ["warn", { argsIgnorePattern: "^_" }],
    "no-console": "warn",
    "prefer-const": "error",
    "no-magic-numbers": [
      "warn",
      { ignore: [0, 1, -1], enforceConst: true, detectObjects: false },
    ],
    "consistent-return": "error",
    "no-else-return": "error",

    // --- Seguridad (OWASP & buenas pr√°cticas) ---
    "security/detect-object-injection": "warn",
    "security/detect-non-literal-fs-filename": "warn",
    "security/detect-eval-with-expression": "error",
    "security/detect-buffer-noassert": "error",

    // --- Legibilidad y mantenibilidad ---
    "max-lines": [
      "warn",
      { max: 300, skipBlankLines: true, skipComments: true },
    ],
    "max-depth": ["warn", 4],
    complexity: ["warn", 10],
    camelcase: "error",
    "newline-before-return": "warn",

    // --- Estilo general (Airbnb + Prettier) ---
    "prettier/prettier": [
      "error",
      {
        printWidth: 100,
        semi: true,
        singleQuote: true,
        trailingComma: "es5",
        arrowParens: "always",
      },
    ],

    // --- Reglas adaptadas al proyecto Data Pura Vida ---
    "no-param-reassign": ["error", { props: true }],
    "import/order": [
      "warn",
      {
        groups: [
          ["builtin", "external"],
          "internal",
          "parent",
          "sibling",
          "index",
        ],
        "newlines-between": "always",
      },
    ],
  },
  overrides: [
    {
      files: ["*.test.js", "*.spec.js"],
      env: {
        jest: true,
      },
    },
  ],
};
```

**2. An√°lisis de calidad y seguridad del c√≥digo:**

- Se integrar√° `SonarQube` para an√°lisis est√°tico continuo del c√≥digo.
- Su uso ser√° para detectar usos inseguros de variables de entorno, errores de autenticaci√≥n, entre otros.

**3. Validaci√≥n de dependencias:**

- Se har√° uso de `Dependabot` para revisar librer√≠as vulnerables.
- En estas validaciones se revisan versiones desactualizadas o inseguras de paquetes, licencias incompatibles con el proyecto, paquetes abandonados, entre otros.

### 2.3 Sistema de Versionamiento

Para el versionamiento de los distintos componentes de Data Pura Vida manejaremos un solo repositorio en GitHub, utilizando un enfoque inspirado en Git Flow, adaptado para flujos modernos con automatizaci√≥n CI/CD, de la siguiente forma:

- main: rama estable lista para production.

- dev: rama de integraci√≥n en ella se corren los tests

- feature/\*: son ramas ef√≠meras en las que se desarrolla una caracter√≠stica en espec√≠fico

- fix/\*: son ramas ef√≠meras en las que se solventan hotfixes o bugs.

Todo cambio realizado en las ramas de feature y hotfix, una vez est√©n listos, se deben fusionar a la rama dev, donde se ejecutar√°n las pruebas correspondientes. Luego, cuando todo est√© aprobado, se har√°n merge a la rama main para que se realice el despliegue a producci√≥n.

#### Versionado

Se seguir√° un esquema de versionado sem√°ntico usando la notaci√≥n MAJOR.MINOR.PATCH, por ejemplo: 2.3.1. Esto permitir√° comunicar de forma clara el tipo de cambios introducidos:

MAJOR: Se incrementa cuando hay cambios incompatibles con versiones anteriores.

MINOR: Se incrementa al agregar funcionalidades nuevas que mantienen compatibilidad.

PATCH: Se incrementa al aplicar correcciones de errores menores o mejoras no disruptivas.

Ejemplos:

Cambiar la estructura del modelo de datos ‚Üí 2.0.0

Agregar una nueva funcionalidad al generador de dashboards ‚Üí 2.1.0

Corregir un bug en la visualizaci√≥n de gr√°ficos ‚Üí 2.1.1

#### Estructura del repositorio

A continuaci√≥n, esta ser√° la estructura del repositorio:

```bash
/data-pura-vida/
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ bioregistro-verde/
‚îÇ   ‚îú‚îÄ‚îÄ la-boveda/
‚îÇ   ‚îú‚îÄ‚îÄ ingestor/
‚îÇ   ‚îú‚îÄ‚îÄ motor-de-transformacion/
‚îÇ   ‚îú‚îÄ‚îÄ centro-de-visualizacion-y-consumo/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generador-de-dashboards/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ consumo-para-ia/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualizaci√≥n-consumo/
‚îÇ   ‚îú‚îÄ‚îÄ marketplace/
‚îÇ   ‚îî‚îÄ‚îÄ backoffice/
‚îú‚îÄ‚îÄ shared/
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ auth/
‚îú‚îÄ‚îÄ infra/
‚îÇ   ‚îî‚îÄ‚îÄ terraform/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îî‚îÄ‚îÄ docker-compose.yml
```

- En la carpeta de components estar√°n albergados todos los componentes del sistema, junto con sus subcomponentes.

- El el directorio de shared se encontrar√°n librerias y herramientas comunes a todos los componentes.

- En terraform/ estar√° la estructura para el despliegue en AWS, todo cambio al app para poder verse reflejado en el cloud provider debe pasar por ac√°.

ejemplos de archivos en terraform son:

```hcl

# provider.tf, para poner la metada del cloud provider
provider "aws" {
  region = "us-east-1"
}

# s3.tf, para crear un bucket de S3
resource "aws_s3_bucket" "react_app_bucket" {
  bucket = "mi-bucket-react-app-unique-1234"
  acl    = "public-read"
}

# eks.tf, para crear un cluster de EKS
module "eks" {
  source          = "terraform-aws-modules/eks/aws"
  cluster_name    = "mi-cluster-eks"
  cluster_version = "1.27"

  subnets         = ["subnet-12345", "subnet-67890"] # las subnets donde va el cluster
  vpc_id          = "vpc-abcde123"

  node_groups = {
    default = {
      desired_capacity = 2
      max_capacity     = 3
      min_capacity     = 1

      instance_type = "t3.medium"
    }
  }
}

```

- En .github/ estar√°n ubicados los pipelines de GitHub Actions. Definir√° las reglas de despliegue del app, por ejmplo, cuando se haga un push a main de cierto componente, se encargar√° de prepararlo y hacer su deploy al cloud provider. A continuaci√≥n un ejemplo de un pipeline que monta un microservicio en EKS:

```yaml
name: Deploy-Microservice

on:
  push:
    branches:
      - main

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up AWS CLI
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Terraform Init and Apply
        working-directory: ./terraform/bioregistro-verde
        run: |
          terraform init
          terraform apply -auto-approve

      - name: Build Docker image
        run: |
          docker build -t ${{ secrets.ECR_REPO_URI }}/bioregistro-verde:latest .
          aws ecr get-login-password | docker login --username AWS --password-stdin ${{ secrets.ECR_REPO_URI }}
          docker push ${{ secrets.ECR_REPO_URI }}/bioregistro-verde:latest

      - name: Deploy to EKS
        run: |
          helm upgrade --install bioregistro-verde ./charts/bioregistro-verde \
          --set image.repository=${{ secrets.ECR_REPO_URI }}/bioregistro-verde \
          --set image.tag=latest
```

### 2.4 Sistemas de Terceros

Durante el desarrollo e integraci√≥n de la plataforma Data Pura Vida, se contempla el uso de m√∫ltiples sistemas de terceros que habilitan funciones clave como autenticaci√≥n, verificaci√≥n de identidad, procesamiento inteligente y orquestaci√≥n de datos. A continuaci√≥n, se describen los principales:

#### Protocolos de Autenticaci√≥n

- OAuth2: Protocolo est√°ndar utilizado para autorizaci√≥n segura entre frontend, backend y terceros que acceden a APIs protegidas.
- JWT (JSON Web Token): Para transmisi√≥n segura de credenciales y validaci√≥n de sesiones, especialmente en dashboards y servicios personalizados.
- MFA: Autenticaci√≥n multifactor implementada mediante servicios externos como Google Authenticator o Auth0, fortaleciendo el inicio de sesi√≥n y la gesti√≥n de cuentas.

#### Verificaci√≥n de Identidad y Seguridad

- SumSub: Plataforma externa para verificaci√≥n de identidad (KYC), validaci√≥n documental autom√°tica y prueba de vida para personas f√≠sicas o representantes institucionales.

#### Proveedor de Nube

- AWS: Plataforma seleccionada para el despliegue de componentes, incluyendo servicios de hosting, bases de datos, colas de eventos, control de accesos, API Gateway, y otros servicios espec√≠ficos como S3, Lambda, DynamoDB, etc.

#### Inteligencia Artificial y Recomendaciones

- Hugging Face / GPT Recommender: Integraciones exploradas para generar recomendaciones de datasets mediante modelos preentrenados de lenguaje natural.

#### Orquestaci√≥n y Flujos de Datos

- Google Cloud Workflows y BigQuery: Se valorar√° el uso de servicios de Google para tareas espec√≠ficas de integraci√≥n de datos y an√°lisis en el datalake, incluyendo procesamiento por lotes, consultas distribuidas y automatizaci√≥n de flujos de datos complejos.

### 2.5 Aspectos de Calidad/SLA

Para garantizar que **Data Pura Vida** funcione exitosamente como ecosistema nacional de datos de Costa Rica, se establecen cinco aspectos de calidad fundamentales con implementaciones t√©cnicas espec√≠ficas que guiar√°n el dise√±o y operaci√≥n del sistema.


#### **2.5.1 Escalabilidad**

La escalabilidad es la capacidad del sistema para manejar un crecimiento progresivo de usuarios, datos y transacciones sin que se degrade el rendimiento o la calidad del servicio.

Data Pura Vida debe comenzar con las instituciones p√∫blicas principales y crecer gradualmente hasta servir a miles de usuarios simult√°neos, incluyendo ciudadanos, empresas, organizaciones sociales y entidades gubernamentales. El sistema debe soportar desde datasets iniciales de unas pocas instituciones hasta vol√∫menes masivos de informaci√≥n nacional.

**Capacidades de crecimiento requeridas:**

- Soporte para miles de usuarios trabajando al mismo tiempo sin ralentizaci√≥n
- Almacenamiento que puede expandirse desde gigabytes hasta terabytes de informaci√≥n
- Procesamiento capaz de manejar cientos de datasets nuevos diariamente durante per√≠odos de alta actividad
- Cobertura nacional con tiempos de respuesta r√°pidos desde cualquier provincia

**Configuraciones t√©cnicas espec√≠ficas para escalabilidad:**

**Balanceador de Carga y Gateway:**
El sistema utilizar√° un balanceador de carga configurado en la infraestructura cloud de AWS con par√°metros espec√≠ficos para garantizar distribuci√≥n eficiente del tr√°fico:

- Configuraci√≥n en 2 zonas de disponibilidad con verificaciones de salud cada 60 segundos
- Capacidad de 500 solicitudes por segundo por instancia con escalado autom√°tico hasta 6 instancias
- Tiempo l√≠mite de 30 segundos para solicitudes normales, 600 segundos para cargas de datasets grandes
- Terminaci√≥n SSL con TLS 1.3 y certificados renovados cada 6 meses

**Gesti√≥n de Conexiones y Concurrencia:**
Para manejar m√∫ltiples usuarios simult√°neos, se implementar√° un sistema de agrupaci√≥n de conexiones a base de datos:

- Agrupaci√≥n con m√≠nimo 3 conexiones activas, m√°ximo 12 conexiones por servicio
- Tiempo l√≠mite de conexi√≥n de 45 segundos, tiempo l√≠mite de adquisici√≥n de 90 segundos
- Tiempo l√≠mite inactivo de 60 segundos para liberar conexiones no utilizadas
- Tiempo l√≠mite de conexi√≥n de 10 segundos para establecimiento de nuevas conexiones

**Estrategia de Cach√© Multicapa:**
El almacenamiento temporal de datos frecuentemente accedidos mejorar√° el rendimiento mediante:

- **Nivel 1 - Cach√© de Aplicaci√≥n**: Cach√© en memoria con tiempo de vida de 15 minutos
- **Nivel 2 - Cach√© Distribuido**: Tecnolog√≠a de cach√© distribuido a definir en Stack Tecnol√≥gico
- **Nivel 3 - Cach√© de Base de Datos**: Optimizaci√≥n a nivel de base de datos
- Tiempo de vida espec√≠fico por tipo de dato: usuarios (2 horas), metadatos de datasets (8 horas), validaciones (48 horas)

**Procesamiento de Datos con Inteligencia Artificial:**
Para el Motor ETDL (Extracci√≥n, Transformaci√≥n y Carga) que utiliza inteligencia artificial, se configurar√°n colas de procesamiento:

- Cola tipo FIFO (Primero en Entrar, Primero en Salir) para garantizar orden en procesamiento
- Tiempo l√≠mite de visibilidad de 30 minutos (tiempo m√°ximo de procesamiento por dataset)
- Retenci√≥n de mensajes de 7 d√≠as para reintento de fallos
- Tama√±o de lote de 5 mensajes por procesamiento para eficiencia
- M√°ximo 3 datasets proces√°ndose simult√°neamente para evitar sobrecarga

**Control de Concurrencia para Inteligencia Artificial:**
Dado que modificar el dise√±o de modelos concurrentemente es peligroso, se implementar√°n sem√°foros y procesamiento as√≠ncrono:

- Sem√°foro √∫nico para modificaciones concurrentes del modelo de datos
- Solo 1 modificaci√≥n de esquema simult√°nea permitida
- Tiempo l√≠mite de 60 minutos para operaciones de modificaci√≥n de modelo
- Cola de espera para modificaciones pendientes con prioridad FIFO

**Mecanismos de escalabilidad:**

El sistema utilizar√° escalado autom√°tico, que significa que cuando detecta mayor actividad, autom√°ticamente asigna m√°s recursos computacionales (servidores adicionales) para mantener el rendimiento. Cada componente puede crecer independientemente seg√∫n su demanda espec√≠fica, y el sistema se optimiza continuamente bas√°ndose en los patrones de uso reales de los costarricenses.



#### **2.5.2 Mantenibilidad**

La mantenibilidad se refiere a la facilidad con que el sistema puede ser actualizado, corregido y mejorado a lo largo del tiempo sin interrumpir el servicio a los usuarios.

Un sistema nacional debe poder evolucionar constantemente. Las regulaciones cambian, las necesidades del pa√≠s se transforman, y la tecnolog√≠a avanza. Data Pura Vida debe adaptarse a estos cambios sin afectar su operaci√≥n diaria.

**Compromisos de mantenibilidad:**

- Resoluci√≥n de problemas cr√≠ticos en m√°ximo cuatro horas
- Implementaci√≥n de mejoras sin interrumpir el servicio a usuarios
- Capacidad de revertir cambios problem√°ticos en menos de quince minutos
- Monitoreo proactivo que detecta problemas antes de que afecten a los usuarios

**Configuraciones t√©cnicas espec√≠ficas para mantenibilidad:**

**Sistema de Monitoreo Integrado:**
La observabilidad es la capacidad de entender el estado interno del sistema bas√°ndose en los datos que produce:

- **M√©tricas**: Retenci√≥n de 15 d√≠as con resoluci√≥n de 30 segundos para m√©tricas cr√≠ticas
- **Registros**: Retenci√≥n de 30 d√≠as con compresi√≥n autom√°tica despu√©s de 3 d√≠as
- **Trazas**: Seguimiento de solicitudes entre componentes para depuraci√≥n
- **Alertas**: 25 reglas configuradas para diferentes umbrales cr√≠ticos del sistema
- **Herramientas**: Stack de monitoreo a definir en Stack Tecnol√≥gico

**Integraci√≥n Continua y Despliegue Continuo:**
La pr√°ctica de integrar c√≥digo frecuentemente y desplegarlo autom√°ticamente incluye:

- **GitHub Actions**: Pipelines automatizados seg√∫n la estructura definida en .github/workflows/
- **Terraform**: Infraestructura como c√≥digo para despliegue en AWS seg√∫n /infra/terraform/
- **Estrategia de Despliegue**: Estrategia de despliegue seguro a definir en Stack Tecnol√≥gico
- Pruebas autom√°ticas con cobertura m√≠nima del 70% antes de despliegue
- Construcci√≥n y empaquetado de aplicaciones con etiquetado por commit SHA
- Verificaci√≥n de salud autom√°tica con tiempo l√≠mite de 5 minutos antes de activaci√≥n

**Migraciones de Base de Datos:**
Los scripts que permiten evolucionar la estructura de la base de datos de forma controlada:

- Scripts de migraci√≥n versionados con nomenclatura est√°ndar
- Reversi√≥n autom√°tica si la migraci√≥n falla
- Validaci√≥n con verificaci√≥n de suma de comprobaci√≥n en cada despliegue
- Respaldo autom√°tico antes de cada migraci√≥n cr√≠tica
- **Herramienta**: Sistema de migraci√≥n a definir seg√∫n tecnolog√≠a de base de datos seleccionada

**Estrategia de Reversi√≥n:**

- Reversi√≥n autom√°tica a revisi√≥n anterior en caso de fallo
- Tiempo l√≠mite configurado a 10 minutos m√°ximo para reversi√≥n completa
- Preservaci√≥n de √∫ltimas 5 revisiones para reversi√≥n selectiva
- Scripts de reversi√≥n semiautom√°ticos para cambios de base de datos

**Estrategias de mantenimiento:**

El sistema utiliza una arquitectura modular, lo que significa que cada componente puede actualizarse independientemente sin afectar los dem√°s. Los despliegues son automatizados con validaci√≥n previa, y existe documentaci√≥n que se actualiza autom√°ticamente. El sistema incluye observabilidad completa, que es la capacidad de monitorear en tiempo real el rendimiento, errores y patrones de uso.

#### **2.5.3 Reutilizaci√≥n**

La reutilizaci√≥n maximiza el aprovechamiento de cada funcionalidad desarrollada, permitiendo que sea utilizada en m√∫ltiples componentes del sistema para optimizar recursos y garantizar consistencia.

Con recursos p√∫blicos limitados, cada desarrollo debe aprovecharse al m√°ximo. Cuando se crea una funcionalidad para validar documentos costarricenses, esta debe servir para todo el sistema, no solo para una parte espec√≠fica.

**Componentes reutilizables principales:**

- Sistema de autenticaci√≥n unificado que permite un solo acceso para toda la plataforma
- Validadores espec√≠ficos para documentos costarricenses (c√©dulas, IBAN, registros tributarios)
- Biblioteca de componentes visuales que garantiza interfaces consistentes
- Herramientas de cifrado estandarizadas para protecci√≥n de datos
- APIs (interfaces de programaci√≥n) comunes para integraciones con sistemas externos

**Configuraciones t√©cnicas espec√≠ficas para reutilizaci√≥n:**

**Librer√≠as Compartidas seg√∫n estructura del repositorio:**
Bas√°ndose en el directorio /shared/ definido en el README:

- **shared/utils/**: Utilidades comunes para todos los componentes
- **shared/auth/**: Sistema de autenticaci√≥n unificado usando OAuth2 y JWT

**OAuth2 (Autorizaci√≥n Abierta 2.0) y JWT (Token Web JSON):**
OAuth2 es un protocolo de autorizaci√≥n que permite a aplicaciones obtener acceso limitado a cuentas de usuario:

- Configuraci√≥n con proveedores Google y Microsoft seg√∫n requerimientos
- Alcances: 'openid', 'profile', 'email' para informaci√≥n b√°sica del usuario
- URI de redirecci√≥n configurado seg√∫n ambiente de despliegue

JWT es un est√°ndar para transmitir informaci√≥n de forma segura entre partes como un objeto JSON firmado:

- Expiraci√≥n de 8 horas con renovaci√≥n autom√°tica si el usuario est√° activo
- Emisor espec√≠fico 'data-pura-vida' con audiencia configurada para 'web', 'mobile', 'api'
- Algoritmo de firma RS256 para validaci√≥n de integridad con rotaci√≥n de claves mensual

**Autenticaci√≥n Multifactor (MFA):**
M√©todo de autenticaci√≥n que requiere m√∫ltiples formas de verificaci√≥n de identidad:

- **TOTP (Contrase√±a de Un Solo Uso Basada en Tiempo)**: Ventana de 3 per√≠odos (90 segundos) para flexibilidad
- **Integraci√≥n con SumSub**: Seg√∫n sistemas de terceros definidos para KYC (Conoce a Tu Cliente)
- **Biometr√≠a**: Integraci√≥n para prueba de vida en representantes legales
- 8 c√≥digos de respaldo generados autom√°ticamente con posibilidad de regeneraci√≥n
- C√≥digos QR de 256x256 p√≠xeles para mejor compatibilidad con dispositivos m√≥viles

**Plantillas de Infraestructura con Terraform:**
Seg√∫n directorio /infra/terraform/:

- M√≥dulos reutilizables para cada componente en /components/
- Variables configurables para nombre, imagen, CPU y memoria
- 2 r√©plicas por defecto con escalado autom√°tico configurado hasta 6 r√©plicas
- Solicitudes de recursos de 250m CPU y 256Mi memoria por defecto
- L√≠mites de recursos de 1 CPU y 1Gi memoria m√°ximo
- Verificaciones de salud autom√°ticas en endpoint /health con tiempo l√≠mite de 10 segundos

**Beneficios de la reutilizaci√≥n:**

- Desarrollo significativamente m√°s r√°pido al aprovechar funcionalidades ya construidas y probadas
- Experiencia de usuario consistente en todos los componentes
- Mantenimiento simplificado donde un cambio se propaga autom√°ticamente
- Mejora continua de la calidad a medida que los componentes se refinan con el uso

#### **2.5.4 Eficiencia**

La eficiencia busca optimizar el uso de recursos computacionales y financieros para ofrecer el mejor rendimiento posible con el menor costo operativo, utilizando responsablemente los recursos p√∫blicos.

El sistema debe proporcionar respuestas r√°pidas y una experiencia fluida mientras utiliza los recursos de manera inteligente, evitando desperdicios y optimizando costos.

**Objetivos de eficiencia:**

- Tiempos de respuesta que los usuarios perciban como instant√°neos para operaciones comunes
- Uso √≥ptimo de la capacidad de los servidores, manteniendo un balance eficiente
- Almacenamiento inteligente con compresi√≥n autom√°tica para reducir costos
- Escalado din√°mico que ajusta recursos seg√∫n la demanda real

**Configuraciones t√©cnicas espec√≠ficas para eficiencia:**

**Optimizaci√≥n de Consultas de Base de Datos:**
Optimizaci√≥n espec√≠fica para La B√≥veda, que debe almacenar datos en formato unificado independientemente del origen (relacional, documental, CSV, Excel):

- **Particionado autom√°tico**: Organizaci√≥n por fecha para datasets hist√≥ricos
- **√çndices optimizados**: Para b√∫squedas de texto completo en espa√±ol (Costa Rica)
- **Relaciones entre datasets**: √çndices para columnas que relacionan datasets del ecosistema
- **Trazabilidad completa**: √çndices para auditor√≠a de datos usados, no usados y descartados

**Estrategia de Almacenamiento AWS:**
Seg√∫n proveedor cloud definido (AWS), se utilizar√°n servicios espec√≠ficos de almacenamiento:

- **Almacenamiento activo**: Para datos activos frecuentemente accedidos
- **Almacenamiento tibio**: Para datos de acceso ocasional
- **Almacenamiento fr√≠o**: Para archivo a largo plazo
- **Almacenamiento de archivo**: Para archivo permanente
- **Pol√≠ticas de ciclo de vida**: Migraci√≥n autom√°tica seg√∫n antig√ºedad y patrones de acceso

**Estrategia de Compresi√≥n espec√≠fica para datasets:**

- **LZ4**: Compresi√≥n r√°pida para datasets recientes (< 7 d√≠as) con proporci√≥n 2:1
- **ZSTD**: Compresi√≥n alta para datasets archivados (> 30 d√≠as) con proporci√≥n 5:1
- **BZIP2**: M√°xima compresi√≥n para almacenamiento fr√≠o (> 1 a√±o) con proporci√≥n 8:1
- Migraci√≥n autom√°tica seg√∫n pol√≠ticas de ciclo de vida de AWS

**Gesti√≥n de Recursos:**
Orquestaci√≥n de contenedores seg√∫n arquitectura seleccionada:

- **N√∫cleos de CPU**: 12 en solicitudes totales, 24 n√∫cleos de CPU en l√≠mites
- **Memoria**: 24Gi en solicitudes totales, 48Gi memoria en l√≠mites
- **Escalado autom√°tico**: Configurado por componente seg√∫n tecnolog√≠a seleccionada
- **Disparadores de escalado**: CPU mayor a 75% o Memoria mayor a 85%
- **Comportamiento de escalado**: M√°ximo 1 instancia por escalado hacia arriba cada 2 minutos, m√°ximo 1 por escalado hacia abajo cada 5 minutos

**Estrategias de optimizaci√≥n:**

El sistema implementa cach√© multicapa, que mantiene los datos m√°s consultados en memoria de acceso r√°pido para respuestas inmediatas. Utiliza consultas optimizadas dise√±adas para ser eficientes incluso con millones de registros, y compresi√≥n autom√°tica que reduce el espacio de almacenamiento sin p√©rdida de informaci√≥n. Incluye balanceo de carga inteligente que distribuye las consultas entre m√∫ltiples servidores para evitar sobrecargas.

#### **2.5.5 Claridad y Gesti√≥n de Complejidad**

La claridad asegura que un sistema t√©cnicamente sofisticado sea comprensible y f√°cil de usar tanto para usuarios finales como para desarrolladores que lo mantienen.

Data Pura Vida debe ocultar su complejidad t√©cnica detr√°s de interfaces simples e intuitivas. Los usuarios no deben necesitar conocimiento t√©cnico para aprovechar sus capacidades.

**Principios de claridad:**

- Interfaces consistentes con navegaci√≥n predecible y uniforme en toda la plataforma
- Mensajes comprensibles que explican claramente qu√© est√° ocurriendo, especialmente en casos de error
- Documentaci√≥n autom√°tica que se mantiene actualizada sin intervenci√≥n manual
- Configuraci√≥n organizada de forma l√≥gica y comprensible

**Configuraciones t√©cnicas espec√≠ficas para claridad:**

**Dise√±o de APIs (Interfaces de Programaci√≥n):**
API RESTful (Transferencia de Estado Representacional) es un estilo arquitect√≥nico para dise√±ar interfaces web que permite la comunicaci√≥n entre sistemas de forma sencilla:

- **Endpoints sem√°nticos**: /api/v1/datasets, /api/v1/users, /api/v1/dashboards
- **M√©todos HTTP**: GET (consultar), POST (crear), PUT (actualizar completo), PATCH (actualizar parcial), DELETE (eliminar)
- **C√≥digos de Estado**: 200 (OK), 201 (Creado), 400 (Solicitud Incorrecta), 401 (No Autorizado), 404 (No Encontrado)
- **Tipo de Contenido**: application/json para intercambio de datos
- **Versionado**: /api/v1/ como prefijo para mantener compatibilidad hacia atr√°s

**Manejo de Errores Estructurado:**

- **C√≥digo sem√°ntico**: DATASET_NOT_FOUND, INVALID_CEDULA_FORMAT, UNAUTHORIZED_ACCESS
- **Mensaje descriptivo**: En espa√±ol para usuarios finales costarricenses
- **Detalles t√©cnicos**: Campo espec√≠fico con error y valor esperado
- **ID de Correlaci√≥n**: Para seguimiento y depuraci√≥n entre componentes
- **Marca de Tiempo**: Formato ISO 8601 para consistencia internacional

**Categor√≠as de Error espec√≠ficas para Costa Rica:**

- **Errores de validaci√≥n**: Formato de c√©dula costarricense, IBAN nacional, RTN
- **Errores de autenticaci√≥n**: MFA, biometr√≠a, prueba de vida
- **Errores de autorizaci√≥n**: Permisos RBAC (Control de Acceso Basado en Roles), acceso por IP costarricense
- **Errores de l√≥gica de negocio**: Reglas espec√≠ficas del ecosistema nacional de datos

**Organizaci√≥n del C√≥digo:**
Seg√∫n estructura del repositorio definida:

```
/data-pura-vida/
‚îú‚îÄ‚îÄ components/               # Cada componente independiente
‚îÇ   ‚îú‚îÄ‚îÄ bioregistro-verde/   # M√°ximo 200 l√≠neas por archivo
‚îÇ   ‚îú‚îÄ‚îÄ la-boveda/           # Separaci√≥n clara de responsabilidades
‚îÇ   ‚îú‚îÄ‚îÄ motor-de-transformacion/ # Funciones espec√≠ficas para ETDL
‚îÇ   ‚îî‚îÄ‚îÄ centro-de-visualizacion-y-consumo/
‚îú‚îÄ‚îÄ shared/                  # C√≥digo reutilizable
‚îÇ   ‚îú‚îÄ‚îÄ utils/              # Utilidades comunes
‚îÇ   ‚îî‚îÄ‚îÄ auth/               # Autenticaci√≥n centralizada
```

**Est√°ndares de C√≥digo Limpio:**

- **Funciones**: M√°ximo 50 l√≠neas, una responsabilidad espec√≠fica
- **Clases**: Principio de responsabilidad √∫nica, m√°ximo 300 l√≠neas
- **Variables**: Nombres autodescriptivos en espa√±ol/ingl√©s seg√∫n contexto
- **Complejidad ciclom√°tica**: M√°ximo 15 por funci√≥n (moderadamente complejo)
- **Documentaci√≥n**: JSDoc para funciones p√∫blicas, comentarios en espa√±ol para l√≥gica compleja

**Gesti√≥n de Configuraci√≥n:**
Configuraci√≥n espec√≠fica por ambiente:

- **Desarrollo**: Configuraci√≥n local con bases de datos de prueba
- **Pruebas**: Ambiente de pruebas en AWS con datos simulados
- **Producci√≥n**: Ambiente productivo con datos reales costarricenses
- **Gesti√≥n de secretos**: Servicios AWS para credenciales sensibles
- **Variables de ambiente**: Configuraci√≥n espec√≠fica por ambiente sin valores fijos en c√≥digo

**Gesti√≥n de complejidad:**

El sistema utiliza separaci√≥n de responsabilidades, donde cada componente tiene una funci√≥n espec√≠fica y bien definida. Implementa abstracciones √∫tiles que ocultan la complejidad t√©cnica detr√°s de interfaces simples, y aplica patrones reconocibles con soluciones consistentes para problemas similares. Incluye escalamiento gradual que presenta funcionalidades b√°sicas primero y avanzadas despu√©s.

**Implementaci√≥n pr√°ctica:**

Las APIs (interfaces de programaci√≥n) utilizan nomenclatura sem√°nticamente clara que explica exactamente qu√© hacen. El manejo de errores es estructurado, proporcionando mensajes que incluyen qu√© ocurri√≥ y c√≥mo solucionarlo. La arquitectura se organiza en capas claras que separan presentaci√≥n, l√≥gica de negocio y datos.

#### **2.5.6 M√©tricas y SLAs Espec√≠ficos**

**M√©tricas de Rendimiento alineadas con requerimientos**

**SLAs de Tiempo de Respuesta:**

- **Endpoints de API**: 95% de operaciones completadas en menos de 800ms
- **Generaci√≥n de Tableros**: Menos de 5 segundos para gr√°ficos simples
- **Carga de Datasets**: M√°ximo 30 minutos para procesamiento ETDL
- **Operaciones de B√∫squeda**: Menos de 3 segundos para b√∫squedas en espa√±ol

**Tiempos de Procesamiento ETDL espec√≠ficos:**

- **Datasets peque√±os** (menor a 1MB): m√°ximo 10 minutos de procesamiento
- **Datasets medianos** (1-10MB): m√°ximo 30 minutos
- **Datasets grandes** (10-100MB): m√°ximo 2 horas
- **Datasets extra-grandes** (mayor a 100MB): m√°ximo 8 horas con procesamiento por lotes

**SLAs de Disponibilidad para ecosistema nacional**

**Disponibilidad del Sistema:**

- **Sistema general**: 99.5% tiempo activo anual (m√°ximo 43.8 horas inactivo/a√±o)
- **Bio Registro Verde**: 99.7% tiempo activo (componente cr√≠tico para acceso)
- **La B√≥veda**: 99.6% tiempo activo (almacenamiento central de datos)
- **Centro Visualizaci√≥n**: 99.0% tiempo activo (menor criticidad, m√°s tolerante a fallas)
- **Ventana de mantenimiento**: Domingos 02:00-06:00 GMT-6 (horario costarricense)

**Planificaci√≥n de Capacidad espec√≠fico para Costa Rica**

**Proyecciones de Crecimiento basadas en adopci√≥n nacional:**

**A√±o 1 (Instituciones p√∫blicas principales):**

- 25,000 usuarios registrados (funcionarios p√∫blicos y ciudadanos early adopters)
- 5,000 datasets (datos de ministerios principales, TSE, BCCR)
- 2TB almacenamiento (documentos b√°sicos, bases de datos institucionales core)
- 5M llamadas de API mensuales

**A√±o 3 (Sector privado integrado):**

- 100,000 usuarios (empresas medianas, algunas c√°maras, organizaciones)
- 25,000 datasets (expansi√≥n gradual con datos comerciales selectos)
- 12TB almacenamiento (crecimiento org√°nico con sector privado)
- 25M llamadas de API mensuales

**A√±o 5 (Ecosistema en maduraci√≥n):**

- 250,000 usuarios (adopci√≥n amplia pero no total)
- 75,000 datasets (ecosistema robusto pero en crecimiento)
- 50TB almacenamiento (volumen significativo de datos hist√≥ricos)
- 100M llamadas de API mensuales

#### **Aseguramiento de Calidad espec√≠fica para Costa Rica**

**Puertas de Calidad de C√≥digo:**

- **Cobertura m√≠nima**: 70% para c√≥digo nuevo, 75% objetivo general
- **Validaciones nacionales**: 85% cobertura para validadores de c√©dula, IBAN, RTN
- **Cumplimiento de seguridad**: Cumplimiento b√°sico Ley 8968, consideraci√≥n GDPR e ISO 27001
- **Internacionalizaci√≥n**: Soporte principal para espa√±ol costarricense

**Pruebas de Rendimiento espec√≠fico:**

- **Pruebas de carga**: 500 usuarios concurrentes (estimaci√≥n conservadora)
- **Pruebas de restricci√≥n geogr√°fica**: Validaci√≥n de acceso preferencial desde IPs costarricenses
- **Recuperaci√≥n ante desastres**: Tiempo de recuperaci√≥n m√°ximo 8 horas
- **Integridad de datos**: 98% integridad en transferencias entre componentes (tolerancia a errores menores)

## 3. Stack Tecnol√≥gico

En cada una documentar versiones de frameworks, SDKs, lenguajes y herramientas utilizadas, as√≠ como sus restricciones y licencias

### Frontend

- **React.js**: Un framework de javascript especializado en web apps
- **Vite**: Empaquetador de react.
- **Tailwind CSS**: Librer√≠a para acelerar la creaci√≥n de estilos mediante utilidades predefinidas.
- **Axios**: Libreria de javascript que permite hacer llamadas a rest APIs.
- **Formik + Yup**: Dos librer√≠as de Javascript que har√°n la escritura de formularios m√°s simple. Formik para la estructura de formularios, Yup para validaci√≥n
- **Cognito**: Servicio de AWS que ser√° usado para el registro de personas.
- **Plotly**: Librer√≠a para gr√°ficos interactivos y avanzados con soporte para fuentes din√°micas y control total. Presione [aqu√≠](https://www.chartjs.org/docs/latest/samples/information.html) para ver los gr√°ficos que ofrecen.
- **AWS S3:** Servicio de almacenamiento escalable donde se alojan los archivos est√°ticos de la aplicaci√≥n React (HTML, CSS, JS, im√°genes, etc.).
- **AWS Cloudfront:** Red de distribuci√≥n de contenido (CDN) que entrega los archivos desde S3 con baja latencia y alta velocidad, mejorando el rendimiento y la disponibilidad global.

### Backend

- **Python**: Lenguaje de programaci√≥n versatil, con variedad de librer√≠as y frameworks especializados en ETL e IA.
- **FastAPI**: Framework as√≠ncrono en Python ideal para construir APIs r√°pidas y escalables.
- **RabbitMQ**: Broker de mensajer√≠a para comunicaci√≥n as√≠ncrona entre m√≥dulos backend.
- **EKS**: Servicio de Kubernetes gestionado por AWS para despliegue escalable y seguro del backend.
- **Apache Spark**: Framework especializado en procesamiento distribuido para ETL, validaci√≥n y transformaci√≥n de datos (usando PySpark).
- **Apache Airflow**:Orquestador de workflows para automatizar y monitorear procesos ETL con Spark, asegurando orden, trazabilidad y escalabilidad.
- **Helm**: Herramienta para gestionar despliegues Kubernetes mediante plantillas din√°micas.
- **Docker**: Ser√° usado para crear im√°genes de los distintos m√≥dulos del backend.

### Data

- **PostgreSQL:** Almacenamiento relacional de datos estructurados, ideal para usuarios y clientes.
- **DynamoDB:** Base de datos NoSQL para gestionar metadatos din√°micos y de alto rendimiento.
- **AWS S3:** Almacenamiento de objetos escalable y seguro para grandes vol√∫menes de datos no estructurados, como archivos.
- **AWS Glue:** Servicio ETL gestionado para la transformaci√≥n y preparaci√≥n de datos en flujos automatizados.\*tentativo, puede que prefiramos implementar nuestro propio cluster de spark en EKS organizado con airflow.
- **AWS SageMaker:** Plataforma integral para crear, entrenar y desplegar modelos de machine learning de forma segura y escalable.
- **AWS KMS (Key Management Service):** Servicio de administraci√≥n de claves criptogr√°ficas para cifrar y proteger datos sensibles en todos los servicios de AWS.

### AI

- **Hugging Face Transformers:** Uso de modelos preentrenados (ej. all-mpnet-base-v2) para generar embeddings sem√°nticos de texto.
- **LangChain:** Orquestaci√≥n de agentes inteligentes y manejo de flujos de lenguaje natural.
- **OpenAI (GPT-4):** Procesamiento de lenguaje natural, generaci√≥n de texto y clasificaci√≥n sem√°ntica.
- **Amazon SageMaker**: Entrenamiento, ajuste fino y despliegue de modelos personalizados de machine learning.
- **Hugging face**: para modelos ya entrenados que nos puedan servir (all-mpnet-base-v2 genera embeddings que podr√≠a servir para entrenar IA)


### Sistemas de Terceros

- **SumSub:** Sistema para poder realizar las comprobaciones KYC, AML y sdk para realizar pruebas de vida.
- **AWS:** Ser√° nuestro cloud provider, y usaremos distintos servicios como S3, Glue, Cognito, etc.
- **Stripe:** Sistema que permite manejar los pagos dentro de nuestro sitio web.
- **Hugging Face:** Fuente para usar m√≥delos de IA ya entrenados.

### Cloud

#### **Proveedor Principal**

- **Amazon Web Services (AWS)**: Plataforma de computaci√≥n en la nube para toda la infraestructura de Data Pura Vida.

#### Servicios de Computaci√≥n

- **Amazon EKS:** Kubernetes gestionado para contenedores del backend
- **AWS Lambda:** Funciones serverless para procesos espec√≠ficos

#### **Servicios de Red**

- **AWS Application Load Balancer:** Balanceador de carga
- **Amazon CloudFront:** CDN para contenido est√°tico
- **AWS VPC:** Red privada virtual para aislar recursos

#### **Servicios de Gesti√≥n**

- **AWS IAM:** Gesti√≥n de identidades y permisos
- **AWS CloudWatch:** Monitoreo y m√©tricas (ya definido en DevOps)
- **AWS CloudTrail:** Auditor√≠a de acciones

### DevOps y Testing

#### Infraestructura como C√≥digo (IaC)

- **AWS CloudFormation:** plantilla oficial de AWS para definir infraestructura como c√≥digo.

- **Terraform:** herramienta para definir y aprovisionar la infraestructura en AWS mediante archivos .tf, asegurando consistencia entre ambientes y facilitando el versionamiento y rollback de cambios.

- **Helm:** gestor de paquetes para Kubernetes que permite definir despliegues mediante chart templates reutilizables, simplificando el despliegue de servicios backend.

#### Integraci√≥n y Despliegue Continuo (CI/CD)

- **Github:** Para guardar codigo y control de versiones.

- **AWS CodePipeline:** herramienta nativa de AWS para construir pipelines de integraci√≥n y despliegue continuo.

- **GitHub Actions:** seguir√° siendo utilizado como integrador externo, especialmente para validar PRs, ejecutar linters, y disparar eventos hacia CodePipeline mediante webhooks.

#### Observabilidad y Monitoreo

- **AWS CloudWatch:** permite monitorear y supervisar toda la infraestructura desplegada en AWS, como RDS, DynamoDB y S3. Dado que todo el alojamiento en la nube se realizar√° en AWS, no es necesario utilizar otras herramientas externas como DataDog o Prometheus.

- **Grafana + CloudWatch + Prometheus:** para dashboards visuales personalizados directamente desde CloudWatch Metrics para los servicios de AWS, y Prometheus para los microservicios en EKS.

#### Pruebas Automatizadas

- **Pytest:** framework de pruebas para Python usado en pruebas unitarias para el backend.

- **Jest :** para pruebas unitarias de componentes React en el frontend.

- **Gatling:** para hacer pruebas de carga en la aplicaci√≥n antes de poder pasarla a producci√≥n.

- **Postman + Newman:** se usar√°n para pruebas manuales y autom√°ticas de la API REST. Newman permite integrar las colecciones en el CI.

#### Validaci√≥n de C√≥digo y Estilo

- **ESLint:** verificaci√≥n autom√°tica de estilo y seguridad en el frontend, con reglas personalizadas ancladas en el repositorio (.eslintrc.js).

- **Amazon CodeGuru Reviewer:** analiza c√≥digo Python, detectando problemas de rendimiento y vulnerabilidades usando machine learning.

- **SonarQube:** se usar√° para realizar an√°lisis est√°tico del c√≥digo backend y frontend, identificando autom√°ticamente bugs, vulnerabilidades y malas pr√°cticas. Estar√° integrado al pipeline de CI/CD para bloquear pull requests con problemas cr√≠ticos y generar reportes de calidad y seguridad.

#### Seguridad

- **AWS Secrets Manager:** gesti√≥n segura de claves API, credenciales y tokens con rotaci√≥n autom√°tica y control de acceso granular.

- **Dependabot:** para monitoreo de paquetes vulnerables desde GitHub. Se integra con CodePipeline para ejecutar pruebas de validaci√≥n al actualizar dependencias.

## 4. Dise√±o de los componentes

En esta secci√≥n se detallar√° el dise√±o de los componentes previamente definidos en la secci√≥n de planeamiento. A cada uno se le aplicar√° un an√°lisis de frontend, backend y datos, seg√∫n corresponda. Adem√°s, existe la posibilidad de incluir prototipos en forma de pruebas de concepto. Tambi√©n se especificar√° c√≥mo se llevar√° a cabo el proceso de pruebas e integraci√≥n, despliegue y mantenimiento.

Antes de comenzar cabe por dejar en claro algunas especificaciones generales que se ver√°n a lo largo de todo el dise√±o de los componentes:

- Todos los microservicios del backend estar√°n desplegados en un cluster de EKS.

- Se tendr√° un API general para todo el backend, para poder acceder a las funcionalidades de todos los microservicios se debde consultar a dicha API (ser√° RESTful). Adem√°s, estar√° construida en FastAPI, para favorecernos de sus caracter√≠sticas asincr√≥nicas que la hacen sumamente r√°pida y apta para manejar carga pesada. Estar√° desplegada en el cluster de EKS, como un deployment con N replicas (Antes de pasar a producci√≥n se le realizar√°n pruebas de carga con Gatling, para poder determinar exactamente cuantas replicas ocupar√°).

### 4.1. Bioregistro 

Este componente es el punto de entrada al sistema, tiene como prop√≥sito registrar distintos tipos de usuarios y adaptarse din√°micamente a sus requerimientos de autenticaci√≥n.

Los tipos de usuarios que se podr√°n registrar en la plataforma son los siguientes :

- **Usuarios con C√©dula F√≠sica**: Esto incluye a cualquier persona f√≠sica que tenga c√©dula costarricense.

- **Usuarios con C√©dula Jur√≠dica**: Esta capa incluye una amplia variedad de colectivos que pueden aportar datasets de valor.
  - **Empresas privadas**: Incluye PYMES y Sociedades An√≥nimas (S.A).
  - **Empresas p√∫blicas y entes estatales**: Abarca instituciones aut√≥nomas, empresas estatales, empresas municipales, y Ministerios.
  - **C√°maras y gremios**: Incluye c√°maras empresariales y gremios profesionales o t√©cnicos.
  - **Universidades y centros acad√©micos**: Comprende universidades p√∫blicas y privadas, as√≠ como sus escuelas, facultades y centros de investigaci√≥n.

Asimismo, se adjunta una descripci√≥n de que es cada uno de los colectivos listados junto con que aporte pueden dar a Data Pura Vida,que informaci√≥n se les va a solicitar para poder garantizar que son empresas verdaderas y solicitadas por sus representantes reales (Cabe aclarar que todo documento PDF debe venir con firma digital):

#### **Empresas privadas**

Organizaciones con fines de lucro que operan en sectores diversos. Se dividen principalmente en:

**PYMES (Peque√±as y Medianas Empresas)**:
Empresas de menor escala que operan en comercio, manufactura ligera, servicios digitales, turismo, entre otros. Dentro de ellas pueden haber subdivisiones.

- **Actividad diaria**: ventas, atenci√≥n al cliente, manejo de inventario, facturaci√≥n, pagos, operaciones locales.
- **Datos potenciales**: consumo local, comportamiento de clientes, tiempos de entrega, cadenas de distribuci√≥n.
- **Documentos necesarios para Identificarla**:
  - Certificaci√≥n de personer√≠a jur√≠dica: Contiene la informaci√≥n de la empresa como su nombre, c√©dula jur√≠dica, tipo de sociedad, nombre del representante.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante.
  - C√©dula jur√≠dica: Debe coincidir con la certificaci√≥n de personer√≠a jur√≠dica.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.
  - Acta Constitutiva y Estatutos: Es un acta que establece la existencia legal de la empresa.
  - Constancia de incripci√≥n en el PYME: Demuestra que est√° registrada en el MEIC y cumple los requisitos para ser PYME.
  - Departamento a Registrar: Se debe registrar a que departamento de la empresa pertenece el registro.



**Sociedades An√≥nimas (S.A.)**
Empresas grandes con estructura formal, juntas directivas y accionistas. Comunes en construcci√≥n, industria, finanzas o tecnolog√≠a.
- **Actividad diaria**: operaci√≥n por departamentos, contrataci√≥n de proveedores, desarrollo de productos, comercio exterior.
- **Datos potenciales**: operaciones financieras, productividad, log√≠stica, desempe√±o empresarial.
- **Documentos necesarios para Identificarla**:
  - Certificaci√≥n de personer√≠a jur√≠dica: Contiene la informaci√≥n de la empresa como su nombre, c√©dula jur√≠dica, tipo de sociedad, nombre del representante.
  - C√©dula jur√≠dica: Debe coincidir con la certificaci√≥n de personer√≠a jur√≠dica.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.
  - Estatutos sociales: Documento que establece la organizaci√≥n, funcionamiento, objeto social, entre otra informaci√≥n valiosa sobre la empresa.
  - Certificado de Registro Mercantil: Documento que certifica la inscripci√≥n de la empresa en el Registro Mercantil.
  - Certificado de Existencia: Documento legal que certifica la existencia de la empresa.
  - Departamento a Registrar: Se debe registrar a que departamento de la empresa pertenece el registro.


#### **Empresas p√∫blicas y entes estatales**

Entidades que operan con fondos p√∫blicos y ofrecen servicios esenciales.

**Instituciones aut√≥nomas**

Ejemplos: CCSS, ICE, INS, TSE.
- **Actividad diaria**: prestaci√≥n de servicios de salud, energ√≠a, seguros, agua, telecomunicaciones.
- **Datos potenciales**: cobertura geogr√°fica, consumo, atenci√≥n m√©dica, reclamos ciudadanos.
- **Documentos necesarios para Identificarla**:
  - C√©dula Jur√≠dica: adjuntar la c√©dula jur√≠dica con el formato "4-000-NNNNNN"
  - Nota oficial con membrete institucional: Nombre completo de la persona que actuar√° como representante de la instituci√≥n, con firma digital de un funcionario autorizado.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.
  - Acta de Resoluci√≥n Interna: Un acta firmada que describa la resoluci√≥n dada internamente en el ente estatal.
  - Departamento a Registrar: Se debe registrar a que departamento de la empresa pertenece el registro.

**Empresas estatales**
Ejemplos: RECOPE, RACSA.
- **Actividad diaria**: importaci√≥n, distribuci√≥n de bienes estrat√©gicos, operaci√≥n con entes reguladores.
- **Datos potenciales**: consumo nacional, log√≠stica, demanda energ√©tica.
- **Documentos necesarios para Identificarla**:
  - C√©dula Jur√≠dica: adjuntar la c√©dula jur√≠dica con el formato "3-NNN-NNNN"
  - Certificaci√≥n de personer√≠a jur√≠dica: Contiene la informaci√≥n de la empresa como su nombre, c√©dula jur√≠dica, nombre del representante.
  - Nota oficial con membrete institucional: Nombre completo de la persona que actuar√° como representante de la instituci√≥n, con firma digital de un funcionario autorizado.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante del √≥rgano.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.
  - Acta de Resoluci√≥n Interna: Un acta firmada que describa la resoluci√≥n dada internamente en el ente estatal.
  - Departamento a Registrar: Se debe registrar a que departamento de la empresa pertenece el registro.

**Empresas municipales**
Entidades creadas por municipalidades para servicios locales, un ejemplo es la ESPH (Empresa de Servicios P√∫blicos de Heredia).
- **Actividad diaria**: recolecci√≥n de residuos, parqueo, mantenimiento urbano, servicios culturales.
- **Datos potenciales**: desarrollo cantonal, planificaci√≥n urbana, gesti√≥n ambiental.
- **Documentos necesarios para Identificarla**:
  - C√©dula Jur√≠dica: adjuntar la c√©dula jur√≠dica.
  - Certificaci√≥n de personer√≠a jur√≠dica: Contiene la informaci√≥n de la empresa como su nombre, c√©dula jur√≠dica, nombre del representante.
  - Nota oficial con membrete institucional: Nombre completo de la persona que actuar√° como representante de la instituci√≥n, con firma digital de un funcionario autorizado.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.
  - Acuerdo Municipal: Un acta firmada que describa la resoluci√≥n dada internamente en el ente municipal.


#### **√ìrganos del Poder Ejecutivo**
Ejemplos: MEP, MINAE, MOPT
- **Actividad diaria**: Formulaci√≥n e implementaci√≥n de pol√≠ticas p√∫blicas, ejecuci√≥n de programas nacionales, regulaci√≥n sectorial, gesti√≥n presupuestaria y administrativa.
- **Datos potenciales**: Indicadores educativos, ambientales, de infraestructura y transporte; estad√≠sticas de cobertura, acceso y calidad de servicios; y datos geoespaciales y sectoriales seg√∫n competencia del ministerio.
- **Documentos necesarios para Identificarla**:
  - Oficio firmado por jefatura autorizada: Documento firmado por la jefatura con la autorizaci√≥n.
  - C√©dula del representante legal: C√©dula del representante del √≥rgano.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.
  - Nombre y Apellido del representante del √≥rgano.


#### **C√°maras y gremios**

Organizaciones que agrupan empresas o profesionales.

**C√°maras empresariales**
Ejemplos: C√°mara de la Construcci√≥n, C√°mara de Tecnolog√≠as de Informaci√≥n, C√°mara de Exportadores de Flores.
- **Actividad diaria**: representaci√≥n del sector, capacitaciones, generaci√≥n de estudios y estad√≠sticas.
- **Datos potenciales**: empleo, productividad, retos sectoriales, inversi√≥n.
- **Documentos necesarios para Identificarla**:
  - Certificaci√≥n de personer√≠a jur√≠dica: Contiene la informaci√≥n de la empresa como su nombre, c√©dula jur√≠dica, nombre del representante.
  - C√©dula Jur√≠dica: adjuntar la c√©dula jur√≠dica, que coincida con la de la certificaci√≥n de personer√≠a jur√≠dica.
  - Carta firmada por el comit√© o jefatura: Oficio que demuestre autorizaci√≥n de la c√°mara empresarial.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.

**Gremios profesionales o t√©cnicos**
Ejemplos: colegios de m√©dicos, ingenieros, abogados.
- **Actividad diaria**: certificaci√≥n profesional, formaci√≥n continua, vigilancia del ejercicio profesional.
- **Datos potenciales**: matr√≠cula, servicios ofrecidos, formaci√≥n, cobertura geogr√°fica.
- **Documentos necesarios para Identificarla**:
  - Certificaci√≥n de personer√≠a jur√≠dica: Contiene la informaci√≥n de la empresa como su nombre, c√©dula jur√≠dica, nombre del representante.
  - C√©dula Jur√≠dica: adjuntar la c√©dula jur√≠dica, que coincida con la de la certificaci√≥n de personer√≠a jur√≠dica.
  - Acta de asamblea constitutiva: Oficio que demuestre autorizaci√≥n de la c√°mara empresarial.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.

#### **Universidades y centros acad√©micos**

Instituciones de educaci√≥n superior, tanto p√∫blicas como privadas, dedicadas a la formaci√≥n profesional, la investigaci√≥n cient√≠fica y la extensi√≥n social. Dentro de estas operan subdivisiones como facultades, escuelas y centros de investigaci√≥n (CI).

**Universidades p√∫blicas y privadas**
- **Actividad diaria**: matr√≠cula, gesti√≥n de carreras, proyectos de investigaci√≥n y extensi√≥n.
- **Datos potenciales**: rendimiento acad√©mico, estad√≠sticas de graduaci√≥n, impacto social.
- **Documentos necesarios para Identificarla**:
  - C√©dula Jur√≠dica: adjuntar la c√©dula jur√≠dica, que coincida con la de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombramiento Interno: Carta que oficializa el encargado de hacer el registro en la plataforma.
  - Carta de la Unidad Interna: Oficio que da la autorizaci√≥n del unidad interna de la universidad:
  - Unidad Interna: Escuela, Facultad, Centro de Investigaci√≥n.
  - Nombre de la unidad: Ingresar nombre espec√≠fico.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.

#### Dise√±o del Frontend

##### Plataforma de Autenticaci√≥n

Primero se describir√° c√≥mo se realizar√° en el frontend el proceso de registro e inicio de sesi√≥n para personas f√≠sicas, ya que los conjuntos no tienen un inicio de sesi√≥n propio, sino que solo cuentan con creaci√≥n de cuenta, la cual es manejada posteriormente por personas f√≠sicas que la agregan a su cuenta personal.

Dado que la plataforma ser√° alojada en AWS, usaremos AWS Cognito para el registro y posterior inicio de sesi√≥n de las personas en la plataforma. Del servicio se utilizar√°n las siguientes caracter√≠sticas:

- Uso de JWT Tokens para autorizar el acceso a las APIs en EKS.
- Uso de Cognito User Pools que registren email, tel√©fono, c√©dula, nombre y apellidos.
- Uso de choice-based authentication para que los usuarios elijan c√≥mo iniciar sesi√≥n (contrase√±a, Email OTP o SMS OTP).
- Uso del AuthFlow de USER_PASSWORD_AUTH, que incluir√° un MFA obligatorio con Email OTP o SMS OTP.
- Uso del SDK de Cognito para agilizar este proceso. Sin embargo, el formulario de registro ser√° manejado program√°ticamente por nosotros, ya que el est√°ndar que ofrece Cognito no se adapta a nuestro caso de uso.

No hay que dejar de lado que un paso muy importante en nuestro sistema es la captura de imagen de la c√©dula y la prueba de vida, para comprobar que la persona que solicita la cuenta sea real. Por lo tanto, en el flujo de registro de una persona f√≠sica, este paso se realizar√° antes de autorizar el registro en el sistema por medio de cognito.

Para implementar esta l√≥gica se usar√° el sistema de terceros SumSub, ya que provee herramientas para:

- Verificaci√≥n de ID (c√©dula en nuestro caso).
- Prueba de vida y detecci√≥n de deepfake.
- Prueba de direcci√≥n, para verificar que la direcci√≥n f√≠sica del usuario sea real.
- Revisi√≥n de AML.
- Revisi√≥n de KYC.

Para realizar todas estas tareas en nuestro frontend se utilizar√° el WebSDK que SumSub provee para React, que cuenta con todas las herramientas necesarias para implementar las opciones mencionadas.

El proceso de registro de las empresas ser√° distinto, ya que requiere realizar tres tareas principales:

- Validaci√≥n y completitud de los datos.
- Validaci√≥n de personas asignadas.
- Asignaci√≥n de llaves tripartitas.

Por ello no se usar√° Cognito para las empresas. Sin embargo, para la validaci√≥n SumSub ofrece una soluci√≥n de KYB (Know Your Business), que permite:

- Revisar el registro nacional para seleccionar la empresa.
- Verificar al encargado del registro mediante prueba de vida y verificaci√≥n de c√©dula.
- Generar cuestionarios personalizados donde se pueden adjuntar documentos legales que SumSub aprobar√°.

Por lo tanto, la validaci√≥n de empresas tambi√©n ser√° implementada con SumSub. El almacenamiento de informaci√≥n y la delegaci√≥n de llaves tripartitas ser√°n discutidos m√°s adelante en la secci√≥n del backend.

Por otro lado, cabe aclarar que para poder llevar a cabo las validaciones con SumSub es necesario dirigirse a la p√°gina de SumSub y ah√≠ generar flows. Los desarrolladores tendr√°n que crear estos flows con base en las especificaciones dadas sobre que informaci√≥n se le debe solicitar a cada tipo de usuario (los distintos tipos de jur√≠dico y el f√≠sico)  que fue especificada previamente en este subcap√≠tulo.

##### Arquitectura de Cliente

Nuestra arquitectura de cliente consistir√° en Client Side Rendering con rendering est√°tico, con una √∫nica capa dedicada a la web. Esta decisi√≥n se toma porque los bundles de React generados en el build de cada proyecto ser√°n almacenados en un bucket de S3, el cual ser√° servido a los clientes mediante el CDN provisto por CloudFront.

Por otro lado, uno de los requerimientos de este m√≥dulo es que solo puede ser accedido con IPs Costarricenses (El registro), por lo que cuando se desee acceder a la p√°gina de registro Cloudfront ejecutar√° un Lambda@Edge Function que revisar√° la IP del usuario y en caso de no ser de Costa Rica, no servir√° dicha ruta del App.

Adem√°s, para acceder al backend se utilizar√° una √∫nica API, desarrollada en FastAPI. Se entrar√° en m√°s detalles de dicha API m√°s adelante.


##### Patrones de Dise√±o de Objetos

A Continuaci√≥n el diagrama de clases del frontend del Bioregistro:

![Patrones de Dise√±o de Objetos](img/FrontBioregistro.png)

- **Caja Verde**: La caja verde representa el patr√≥n de Chain of Responsability. Est√° asociado a los distintos tipos de forms que existen en el sistema, y gracias a la naturaleza del CoR, permite declararlos din√°micamente. Inclusive, permite que si en un futuro se desea agregar otra capa, sea sumamente sencillo.
- **Cajas Celeste**: Las cajas celestes representan el strategy pattern, ya que por medio de herencia se aisla los distintos tipos de forms para colectivos, y de colectivos.
- **Caja Roja**: Esta caja roja cumple dos funciones, de Singleton y de Facade. De singleton porque de esta manera solo existe una instancia que se conecta al API en todo momento. Adem√°s funciona como Facade ya que aisla toda la l√≥gica de conexi√≥n con el API del backend en una sola clase.


##### Componentes Visuales

**Patrones y Principios**

- **Responsive Design**: Aunque el enfoque principal de nuestro sistema est√° en el uso desde web desktop, es importante implementar un dise√±o responsivo para que los usuarios puedan realizar el registro, prueba de vida y verificaci√≥n de c√©dula de forma c√≥moda desde la c√°mara de sus celulares. Este dise√±o responsivo se lograr√° aprovechando las opciones que ofrece Tailwind CSS para distintos tama√±os de pantalla, utilizando prefijos como sm:, md:, lg:, y xl:, que permiten adaptar los estilos seg√∫n el dispositivo.

- **SOLID**:
  - Single Responsibility: Cada componente en el bioregistro solo tendr√° una responsabilidad. Por ejemplo, el formulario que detecta si es persona f√≠sica o un conjunto solo emplea esa tarea, o los componentes de verificaci√≥n de SumSub son distintos y cada uno hace su propia tarea: uno para la prueba de vida, otro para la verificaci√≥n de id, y as√≠ para todo componente.
  - Open Closed Principle: Los componentes de registro son din√°micos y est√°n separados, gracias a esto, si en un futuro se desea agregar otro tipo de organizaci√≥n, tan solo se debe desarrollar dicho componente y de ah√≠ la conexi√≥n con el resto del flujo ser√° directa.
  - Liskov Substitution Principle: La herencia debe ser utilizada solo cuando es necesaria. Por ejemplo, para los formularios de documentos para empresas si es valioso usar una superclase, pero no tiene sentido agruparlos en una clase madre con el formulario de prueba de vida.
  - Interface Segregation Principle: No usaremos una interf√°z enorme para agrupar a todo tipo de formulario, solo si es necesario se definir√°n, y cuando se haga se ha≈ïan lo m√°s espec√≠ficas posibles. Por ejemplo, una interf√°z madre para los procesos de SumSub que ocupen uso de camara.
  - Dependency Inversion Principle: Las clases nunca deben depender de implementaciones, deben usar las intefaces. Por ejemplo, la clase que agrupe los formularios para registro de organizaciones debe poder permitir cualquier tipo de cuestionario de documentos, independientemente de con que colectivo se este trabajando.

- **Dry principle**: En la medida de lo posible se usar√° la menor cantidad de c√≥digo repetido. Dos ejemplos de esto son: gracias a que usaremos atomic design, componentes como botones o labels ser√°n reutilizado no solo en el bioregistro, pero en todo el sistema; y otro ejemplo es que tanto en el registro de colectivos como de personas se pide prueba de vida, entonces se utilizar√° la misma clase para manejar ambas tareas.

- **Separation of Concern**: Se cumple este principio ya que las distintas capas del frontend estar√°n bien definidas. En la capa de datos solo se gestionar√°n los objetos como personas y colectivos. Luego por medio de CustomHooks se gestionar√° la l√≥gica del ViewModel, por ejemplo, las funciones como GetLivenessCheck o AttachPDF. Y Finalmente la capa de Vista se dedicar√° a tan solo eso, hacer el render de los componentes.

- **Atomic Design**: Este es un patr√≥n muy com√∫n en React, y se ver√° reflejado porque los componentes ser√°n creados empezando por √°tomos, como b√≥tones e inputs; Luego con mol√©culas, que por ejemplo podr√≠a ser un item de formulario que tenga un label, bot√≥n e input; Para despu√©s crear Organismos como los Formularios Completo; Para que despu√©s se junten todos en una p√°gina que ser√° la que finalmente renderize todo.

- **MVVM**: Estamos usando React, as√≠ que MVC no era una opci√≥n, y el flujo que tenemos planeado de comunicaci√≥n entre hooks y componentes se adapta a un MVVM. Se aplicar√° de la siguiente forma:
  - Model: Lo implementaremos en la conexi√≥n con la API (ser√°n funciones), y tambi√©n en los objetos que luego se insertar√°n en la base de datos (no vamos a poner la l√≥gica de negocio ac√°, como recomiendan empresas como Microsoft, para que nuestra app no viole el principio de responsabilidad √∫nica ni la separaci√≥n de responsabilidades) como los distintos tipos de organizaci√≥n, o las personas f√≠sicos.
  - View: Ser√° toda la parte visual de los componentes, que van a seguir atomic design.
  - ViewModel: Se implementar√° en los custom hooks reutilizables que gestionan la l√≥gica de negocio.


**Toolkits y Standards**:

- **Vite**: Se usar√° como servidor local para el desarrollo, y tambi√©n para hacer el bundle de la aplicaci√≥n.
- **React Router**: Herramienta que permite manejar un app de react por medio de rutas.
- **ESlint**: Se usar√° para mantener un est√°ndar de c√≥digo y evitar errores comunes.


##### Estructura de Carpetas

``` bash
frontend/
‚îú‚îÄ‚îÄ public/                   #Assets como imagenes
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/                  #Ac√° estar√°n las funciones del API
‚îÇ   ‚îú‚îÄ‚îÄ model/                #Ac√° se almacenar√°n las clases del modelo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Person
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Collective
‚îÇ   ‚îú‚îÄ‚îÄ components/           #Atomic Design
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ atoms/            #Componentes m√°s b√°sicos
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Button.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Icon.jsx
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ molecules/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ FormItem.jsx
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ organisms/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PersonalInfoForm.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ProofOfAddressForm.jsx
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ CollectiveForm.jsx
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ PersonForm.jsx
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ hooks/                 #ViewModel
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useLivenessCheck.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useIdVerification.jsx
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ contexts/              #Contexto del form, que datos han sido registrados
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ FormContext.jsx
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ pages/                 #Uso de las templates lista para formar una p√°gina completqa
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MainRegister.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ VerifyEmail.jsx
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ styles/               #Tailwind
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄglobals.css        #Configuraci√≥n de Tailwind
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/                #Funciones DRY
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ App.tsx               #Punto de Entrada
‚îÇ
‚îÇ
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ unit/
    ‚îî‚îÄ‚îÄ integration/
```

##### Diagrama del Front

#### Dise√±o del backend

#####


#### Dise√±o de los Datos


### Diagrama General del Frontend

Este si es general de todos los componentes

### Diagrama General del Backend

Este si es general de todos los componentes



## 5. Validaci√≥n de los requerimientos

- Validar que el dise√±o cubre todos los requerimientos funcionales y no funcionales del sistema
- Identificar ventajas y desventajas del dise√±o, proponiendo mitigaciones a los riesgos y limitaciones
