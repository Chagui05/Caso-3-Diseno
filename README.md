# Caso-3-Diseno


# 1. Planeamiento

En esta secci√≥n se detallan los aspectos relacionados con la comprensi√≥n del problema, la forma en que se dividir√° el trabajo dentro del equipo, los hitos en los que se estructura el desarrollo del proyecto y los mecanismos que permitir√°n evaluar si se est√° avanzando conforme a lo planificado.

## 1.1 Estructura del Equipo, Stakeholders, Key Players

### Estructura Interna

El equipo de trabajo consiste de 5 integrantes:

- Santiago Chaves Garbanzo
- Anthony Fuentes
- Luis David Blanco
- Gabriel Guti√©rrez
- Jefferson Salas Cordero

Todos tenemos asignaciones distintas dentro del proyecto, pero como vamos a trabajar con metodolog√≠a Kanban, cualquiera puede hacerse cargo de cualquier tarea. Basta con elegirla en ClickUp y ponerse a trabjar.

Adem√°s, una pieza muy importante en el proyecto ser√° Rodrigo N√∫√±ez, el cu√°l actuar√° como Product Owner de Data Pura Vida, y consultor de dise√±o de software en caso de que tengamos alg√∫n tipo de duda.

### Stakeholders y Key Players:

Se identificaron los principales actores que influyen o se ven afectados por el desarrollo de la plataforma. A continuaci√≥n, se presenta una matriz que los muestra:

![matrizstakeholders](img/matriz-stakeholders.png)

Como se puede observar, los principales interesados en que el proyecto avance son el Product Owner y el equipo de trabajo, ya que est√°n comprometidos con lograr que el producto final cumpla con los objetivos planteados desde el inicio.

La poblaci√≥n general, aunque no tiene un alto nivel de poder dentro del proyecto, demuestra un gran inter√©s. Una plataforma de este tipo promover√≠a la transparencia en el acceso a datos y aportar√≠a a un Costa Rica con mayor disponibilidad de informaci√≥n para todos (incluso aunque parte del contenido sea privado).

Por otro lado, el actor con mayor poder es el gobierno, al ser quien financia el proyecto. Su inter√©s se considera moderado, posiblemente por cierto escepticismo respecto al impacto que la plataforma podr√≠a tener sobre su reputaci√≥n.

Finalmente, aquellos actores que incurren en pr√°cticas como el lavado de dinero o evasi√≥n fiscal son los menos interesados, ya que el sistema ofrecer√≠a mayor visibilidad sobre los datos, lo cual podr√≠a exponer dichas irregularidades.

Ahora bien, ya que se conocen los stakeholders principales se puede evidenciar que los key players son tanto el Product Owner, como el Equipo de trabajo, ya que ser√°n los encargados de que el proyecto tenga exito.

### Sistemas y Ecosistemas de Software Existentes:

En Costa Rica no existe ning√∫n sistema ni ecosistema previo que funcione como antecedente directo de Data Pura Vida, por lo que no se anticipan problemas de integraci√≥n con plataformas existentes. Si bien existen herramientas puntuales, como el API del TSE para consultas por c√©dula, estas no representan un obst√°culo, ya que los requerimientos del proyecto contemplan la capacidad del sistema para aceptar datos provenientes de APIs externas.

## 1.2 Gesti√≥n de la Comunicaci√≥n y Documentaci√≥n del proyecto

Para la comunicaci√≥n interna del equipo se utilizar√° Slack. A trav√©s de esta herramienta se coordinar√° la asignaci√≥n de tareas en ClickUp, y tambi√©n se notificar√° cuando una tarea haya sido finalizada. Antes de marcarla como completada, la tarea deber√° pasar al estado de "Esperando Aprobaci√≥n" en ClickUp, para que Rodrigo pueda revisarla y aprobarla.

Adem√°s, se utilizar√° Discord para realizar al menos una reuni√≥n semanal, en la cual se discutir√°n avances generales del proyecto. En caso de surgir dudas m√°s complejas, se invitar√° al profesor para que pueda brindar orientaci√≥n.

La documentaci√≥n principal del proyecto se mantendr√° en el README del repositorio de GitHub Chagui05/Caso-3-Diseno. En ese archivo se incluir√°n todos los detalles relevantes. Si existieran anexos, como la hoja de requerimientos o la entrevista con el profesor, se agregar√°n al mismo repositorio en archivos separados con nombres descriptivos, y se har√° referencia a ellos desde el README. Toda la documentaci√≥n ser√° escrita en formato Markdown.

## 1.3 Entendimiento del problema

Para entender adecuadamente el problema, el equipo realiz√≥ una entrevista al Product Owner, basada en una serie de preguntas que surgieron tras revisar la especificaci√≥n del proyecto. Las respuestas obtenidas fueron registradas en el archivo RespuestaEntrevista, y permitieron extraer informaci√≥n clave, como por ejemplo:

- La plataforma debe permitir el registro de personas f√≠sicas primero, y luego dar la posibilidad de vincularlas como administradoras de organizaciones.
- La validaci√≥n de cuentas puede involucrar m√∫ltiples m√©todos: revisi√≥n manual, validaciones automatizadas y uso de servicios externos como SumSub.
- La inclusi√≥n de IBAN y datos de tarjetas en el registro busca filtrar usuarios no comprometidos y evitar registros superficiales.
- Se recomienda restringir el acceso por IP solo a la secci√≥n de registro, o permitir el registro de IPs confiables para quienes est√©n en el extranjero.
- Los datos cargados (sin importar el origen) deben convertirse a un formato unificado tras aplicar el proceso ETDL (relacional, documental, etc.).
- El sistema debe validar estructura, formato y contenido de los datasets, considerando reglas como formatos de fechas, booleanos y tipos de datos.
- El motor de visualizaci√≥n de dashboards debe permitir a los usuarios crear sus propios paneles, compartirlos y gestionarlos.
- La arquitectura de backend puede ser definida libremente por el equipo (monol√≠tica o microservicios), seg√∫n convenga al dise√±o general.
- Se permite la integraci√≥n con herramientas de IA, siempre que se respeten los criterios de autorizaci√≥n definidos por la administraci√≥n de la plataforma.
- El portal web de backoffice ser√° administrado por una organizaci√≥n gubernamental registrada dentro del sistema, actuando como custodio de la informaci√≥n.

A partir de esta informaci√≥n, se desarrollaron los siguientes diagramas de flujo que ilustran las tareas clave identificadas dentro del sistema:

- Diagrama de registro:

El siguiente diagrama presenta una visi√≥n general del proceso de registro en nuestra plataforma. No incluye detalles t√©cnicos ni especificaciones sobre los campos din√°micos que var√≠an seg√∫n el tipo de entidad registrada; su objetivo es ilustrar de forma abstracta y comprensible c√≥mo se estructura el flujo de registro dentro del sistema.

![matrizstakeholders](img/entendimientoRegistro.png)

- Diagrama de Ingesta y configuraci√≥n de un dataset

Subir y configurar un dataset en la plataforma no es nada trivial, por eso se armaron estos dos diagramas que separan el proceso en dos partes.

El primer diagrama muestra c√≥mo se le pide al usuario que suba el dataset: qu√© datos tiene que dar, qu√© informaci√≥n se necesita para la IA, y c√≥mo se valida el archivo que subi√≥ (formato, estructura, nombres de columnas, etc.).

![matrizstakeholders](img/subidaDataset1.png)

El segundo diagrama arranca una vez que el dataset ya fue validado. Ah√≠ se definen cosas como si el conjunto de datos va a ser p√∫blico, privado o de pago, y qu√© m√©todos de acceso y cobro se van a aplicar.

![matrizstakeholders](img/subidaDataset2.png)

Es importante aclarar que en el diagrama II no se detalla paso a paso lo que hace el motor ETDL, pero s√≠ se deja claro que va a encargarse de tareas como: detectar duplicados, relacionar datos con otros ya cargados, ajustar el modelo seg√∫n las conexiones que encuentre, y aplicar autom√°ticamente un flujo con extracci√≥n, transformaci√≥n, limpieza, detecci√≥n de contexto, modelado y carga con ayuda de AI.

### Componentes del Sistema

Con el fin de lograr una arquitectura modular, segura y mantenible, el sistema se divide en macrocomponentes. Cada uno aborda un conjunto espec√≠fico de requerimientos funcionales y no funcionales. En esta secci√≥n se listan los componentes y sus principales responsabilidades. La implementaci√≥n t√©cnica y subdivisi√≥n de estos se detalla m√°s adelante en el documento.

#### Bioregistro

Este m√≥dulo gestiona el proceso de incorporaci√≥n de personas f√≠sicas y jur√≠dicas a la plataforma. Abarca desde el llenado de formularios hasta la validaci√≥n de identidad y la emisi√≥n de credenciales digitales. Debe cumplir con regulaciones AML y est√°ndares avanzados de identidad digital.

Requerimientos:

- El componente debe permitir el registro de personas f√≠sicas, jur√≠dicas, instituciones, c√°maras, grupos y empresas.
- El formulario de registro debe adaptarse din√°micamente seg√∫n el tipo de entidad seleccionada.
- El registro de usuarios debe estar asegurado con MFA y biometr√≠a, cumpliendo est√°ndares de identidad digital avanzada.
- El componente debe solicitar y capturar informaci√≥n personal, societaria, legal y tributaria seg√∫n el tipo de entidad.
- El componente debe revisar que cuando se van a asignar personas f√≠sicas a la organizaci√≥n al crearla, efectivamente formen parte de dicho conjunto.
- El registro debe pasar por una etapa de validaci√≥n interna manual para el registro de empresas
- El componente debe implementar validaci√≥n autom√°tica por inteligencia artificial de los documentos subidos.
- El componente debe exigir a los representantes legales el registro como individuos con: identidad digital, biometr√≠a, prueba de vida y autenticaci√≥n multifactor (MFA).
- Cada organizaci√≥n debe recibir llaves de seguridad que le permitan delegar o revocar accesos a sus usuarios.
- Un usuario debe poder administrar m√∫ltiples organizaciones desde una √∫nica cuenta.
- El componente debe capturar datos preliminares de cuentas IBAN y/o tarjetas de cr√©dito como parte del registro.
- El componente debe enviar una notificaci√≥n por correo electr√≥nico cuando un registro sea aprobado.
- El componente debe exigir documentos espec√≠ficos seg√∫n el tipo de entidad: c√©dulas f√≠sicas o jur√≠dicas, actas, RTN, direcci√≥n, etc.
- El componente debe permitir registrar direcciones IP institucionales (listas blancas) para permitir acceso autorizado.
- El componente debe permitir √∫nicamente IPs costarricenses en el registro
- El sistema debe proteger las claves generadas mediante un esquema de llave tripartita, distribuidas entre Data Pura Vida y dos custodios.

#### La B√≥veda

La B√≥veda es el almac√©n central de datos del sistema, dise√±ado para ser seguro, escalable y auditable. Unifica todos los datos cargados, sin importar su formato de origen, y permite relaciones entre datasets. Cifra la informaci√≥n en tr√°nsito y reposo, controla el acceso por roles y entidades, y mantiene trazabilidad completa del uso y movimientos de los datos. Est√° pensada para soportar millones de registros con alto rendimiento y cumplir est√°ndares de gobierno de datos.

Requerimientos:

- La B√≥veda tiene que almacenar los datos en un solo formato, por m√°s de que las fuentes externas sean de distintos tipos (relacionales, documentales, csv, excel)
- La B√≥veda debe permitir especificar columnas que relacionan un dataset con otros datasets del ecosistema.
- La B√≥veda debe de estar monitoreada en todo momento para detectar movimientos sospechosos, para dar contenido de uso de un dataset, y para asegurar trazabilidad y diagn√≥stico r√°pido de fallas.
- Debe ser resiliente, auditable y alineado con est√°ndares de gobierno de datos.
- Debe permitir crecimiento din√°mico sin perder eficiencia.
- Debe escalar a millones de registros y miles de usuarios concurrentes.
- Mantener trazabilidad de datos usados, no usados y descartados.
- Todos los datos cargados deben estar protegidos mediante cifrado, incluso frente al personal t√©cnico ("ingenieros de la plataforma").
- Cifrar toda la data en tr√°nsito y en reposo, dejando trazabilidad auditable.
- Permitir almacenamiento masivo de datos estructurados y semiestructurados.
- Controlar accesos l√≥gicos por entidad, usuario o tipo de dato.
- Implementar control de acceso a nivel de rol (RBAC) y a nivel de fila (RLS) o equivalentes.

#### Centro de Carga

Este m√≥dulo permite a los usuarios cargar sus datasets a la plataforma. Desde ac√° pueden definir qu√© datos desean cifrar, especificar el formato de origen y configurar otros par√°metros clave para asegurar que la carga se procese correctamente.

Requerimientos:

- Permitir a los usuarios decidir qu√© datos compartir dentro del ecosistema.
- Requerir que cada dataset tenga un nombre √∫nico.
- Soportar m√∫ltiples m√©todos de carga de datos: archivos Excel, CSV, JSON, APIs y conexiones directas a bases de datos SQL y NoSQL.
- Requerir nombre, descripci√≥n y metadata √∫til para IA sobre las columnas del dataset.
- Permitir configurar los par√°metros de conexi√≥n de forma cifrada para cada medio de carga.
- Los par√°metros de conexi√≥n de bases de datos y APIs deben almacenarse de forma cifrada.
- Permitir configurar si el dataset es p√∫blico o privado, gratuito o pagado, permanente o con disponibilidad temporal.
- El sistema de permisos debe prevenir accesos no autorizados a datasets privados o pagos.
- Asignar permisos de acceso a los datasets privados.
- Permitir definir montos de acceso para datasets con modelo de cobro.
- Restringir acceso a datos por tiempo, volumen o frecuencia de consulta.
- Indicar si la carga es √∫nica o recurrente, completa o por deltas.
- Configurar par√°metros para carga por deltas: campos diferenciales, frecuencia (timed pull) o mediante callbacks.
- Habilitar control granular de acceso por instituci√≥n, persona o grupo.

#### Motor de Transformaci√≥n

Este m√≥dulo es clave para garantizar que los datasets se almacenen correctamente en la B√≥veda. Se encarga de recibir datos desde distintas fuentes, validar que el formato coincida con el indicado en el formulario de ingesta y, en caso contrario, rechazar la carga. Una vez superada esta validaci√≥n, aplica todo el proceso de ETDL y mapea los datos al formato interno de la B√≥veda.

Requerimientos:

- Validar el formato, estructura y contenido de cada dataset cargado sea correcto, o bien adaptarlo al interno de la B√≥veda (formatos de fecha, booleans, etc.).
- Validar el formato, estructura y contenido de cada dataset cargado coincida con lo especificado en el proceso de carga.
- Automatizar el proceso de carga mediante un motor de IA que aplique un flujo ETDL (extracci√≥n, transformaci√≥n, limpieza, detecci√≥n de contexto, modelado y carga).
- Aplicar IA para normalizar, redise√±ar modelos de datos y vincularlos autom√°ticamente.
- Detectar duplicidades, optimizar relaciones y ajustar el modelo de datos autom√°ticamente seg√∫n las interrelaciones detectadas.
- Monitorear el proceso completo con m√©tricas de transferencia, carga, limpieza, eliminaci√≥n, modelado, volumen, datos omitidos, datos consultados y tasa de √©xito.
- El sistema debe ser capaz de procesar cargas recurrentes y automatizadas sin intervenci√≥n manual.
- Soportar cargas delta con identificaci√≥n de cambios.
- Realizar merges eficientes sin p√©rdida de integridad.

#### Centro de Visualizaci√≥n y Consumo

Este m√≥dulo est√° compuesto por 3 subcomponentes clave:

1. **Generador de dashboards**: permite a los usuarios dise√±ar y crear gr√°ficos de forma r√°pida y amigable para visualizar cualquier dataset.

Requerimientos:

- El sistema debe permitir la construcci√≥n de dashboards personalizados de forma manual.
- El sistema debe permitir construir dashboards manualmente o mediante prompts inteligentes que generen visualizaciones autom√°ticas.
- El sistema debe permitir representar visualmente los datos en tablas, gr√°ficos, conteos, tendencias y predicciones.
- El sistema debe permitir a los usuarios guardar sus dashboards personalizados.
- El sistema debe permitir compartir dashboards con otros usuarios o hacerlos p√∫blicos dentro de la plataforma.
- La interfaz de construcci√≥n de dashboards debe ser segura, intuitiva y con capacidad de respuesta en tiempo real.

2. **Visualizaci√≥n y Consumo**: ofrece una interfaz para revisar esas visualizaciones y realizar an√°lisis de datos directamente sobre los dashboards.

Requerimientos:

- El sistema debe permitir visualizar todos los datasets accesibles como una fuente consolidada.
- El sistema debe bloquear toda exportaci√≥n directa de datos y gr√°ficos desde el portal.
- El sistema debe mostrar datos de forma preliminar en modo de construcci√≥n de dashboard y luego con datos reales al ejecutar consultas
- El sistema debe deshabilitar temporalmente el acceso a datasets cuando se superen los l√≠mites de consumo.
- El sistema debe registrar todas las transacciones y consumos de datos en un historial accesible para cada usuario.
- El sistema debe mostrar m√©tricas de uso: volumen de datos consultados, n√∫mero de consultas realizadas, tiempo restante o l√≠mites alcanzados.
- El sistema no debe permitir en ning√∫n momento la descarga directa de datasets o gr√°ficos generados.
- La visualizaci√≥n de datos debe realizarse exclusivamente dentro del portal, sin opciones de exportaci√≥n, captura o embedding externo.
- Los l√≠mites de consumo deben aplicarse en tiempo real, sin permitir bypasses o reintentos abusivos.

3. **Consumo para IA**: Este subcomponente es el regulador de consumo de IA, define l√≠mites y los m√©todos de ingesta disponibles desde el sistema para los usuarios.

Requerimientos:

- El sistema debe permitir el acceso sistema a sistema √∫nicamente para alimentar modelos de IA aprobados.
- La entrega de datos para modelos de IA debe ser monitoreada, registrada y limitada a contextos aprobados expl√≠citamente por Data Pura Vida.
- El sistema debe ofrecer plataformas limitadas y controladas para esta alimentaci√≥n de IA. Solo permitir√° 2 por usuario.
- El sistema debe minimizar al m√°ximo el riesgo de descargas indirectas mediante presunci√≥n de uso en IA.
- Los datos deben ser env√≠ados en un formato que no permita poder ser desencriptado para otro uso que no sea alimentar IA (por ejemplo uso de embeddings).

#### Marketplace

Este m√≥dulo est√° enfocado en ofrecer una interfaz amigable que permita a los usuarios encontrar datasets de forma eficiente, con descripciones claras y navegaci√≥n fluida. Adem√°s, incluye una secci√≥n adicional para buscar dashboards creados por otros usuarios, facilitando el descubrimiento y reutilizaci√≥n de visualizaciones dentro de la plataforma.

Requerimientos:

- La experiencia de compra de datasets debe ser fluida, transparente y accesible desde los dashboards personales.
- Incluir un m√≥dulo de compra donde se visualicen datasets disponibles bajo acceso pagado.
- Permitir seleccionar un dataset, visualizar precio, t√©rminos de uso, duraci√≥n del acceso y condiciones de cobro.
- Soportar m√∫ltiples m√©todos de pago: tarjeta de cr√©dito, d√©bito y otros mecanismos nacionales compatibles.
- Mostrar confirmaciones de transacci√≥n y activar el acceso seg√∫n condiciones (tiempo, volumen, frecuencia).
- El sistema debe mostrar opciones para renovar o ampliar los paquetes de acceso en caso de superar el l√≠mite.

#### Backoffice Administrativo

Este m√≥dulo concentra las herramientas de backoffice necesarias para la gesti√≥n integral de la plataforma. Su enfoque est√° en el control, la seguridad, la gobernanza de datos y la trazabilidad completa de las operaciones.

Requerimientos:

- Administrar usuarios: validaci√≥n de identidad, membres√≠a y roles.
- Gestionar reglas de carga de datos (formatos, estructuras, validaciones).
- Configurar conexiones externas (APIs, BDs, callbacks).
- Activar, desactivar y supervisar objetos de datos, pipelines y flujos.
- Revocar o regenerar llaves de seguridad (sim√©tricas, asim√©tricas, tri-partitas).
- Administrar custodios de llaves y flujos de confirmaci√≥n mancomunada.
- Auditar operaciones por usuario, fecha, acci√≥n y resultado.
- Generar reportes de uso, calidad, integraci√≥n y anomal√≠as.
- Monitorear el estado operativo de servicios y tareas.
- Extraer evidencias para procesos legales bajo autorizaci√≥n.
- Gestionar permisos y accesos mediante RBAC.
- Debe ofrecer una interfaz robusta y segura solo para personal autorizado.
- Debe permitir gesti√≥n flexible pero estricta de accesos y configuraciones.

### Prototipado

Se desarroll√≥ un prototipo funcional de la p√°gina del Bioregistro Verde con el objetivo principal de probar el comportamiento de los formularios din√°micos. Este prototipo no incluye procesos de prueba de vida ni captura de datos biom√©tricos, y tampoco recolecta la informaci√≥n final que se almacenar√° en el sistema definitivo. Su prop√≥sito es demostrar, a alto nivel, c√≥mo el formulario se adapta din√°micamente seg√∫n las selecciones del usuario.

A continuaci√≥n las im√°genes del flujo de recolecci√≥n de data de una persona f√≠sica:

![alt text](img/userReg0.png)

![alt text](img/userReg1.png)

![alt text](img/userReg2.png)

![alt text](img/userReg3.png)

Adem√°s, se adjunta el proceso de registar compa√±√≠a p√∫blica:

![alt text](img/orgPub0.png)
![alt text](img/orgPub1.png)
![alt text](img/orgPub2.png)
![alt text](img/orgPub3.png)
![alt text](img/orgPub4.png)
![alt text](img/orgPub5.png)

Si dese√° probar el prototipo visite el siguiente [link](https://gentle-signup-wizard.lovable.app/).

## 1.4 Customer Journeys

Este Service Blueprint representa el recorrido completo de un ciudadano dentro del ecosistema Data Pura Vida, desde el descubrimiento de la plataforma hasta la creaci√≥n, publicaci√≥n y monitoreo de un dashboard personalizado con datos p√∫blicos.

El diagrama detalla no solo las acciones del usuario, sino tambi√©n los touchpoints del sistema, los procesos internos (backstage) y las herramientas de soporte involucradas en cada etapa. Adem√°s, se integran las emociones del usuario para identificar oportunidades de mejora en la experiencia.

Las l√≠neas de interacci√≥n, visibilidad e interacci√≥n interna permiten visualizar claramente los l√≠mites entre lo que el ciudadano ve, lo que ocurre detr√°s del sistema y las herramientas tecnol√≥gicas que lo sustentan.

Este blueprint se organiza en **seis** fases principales:

1. Conciencia: El ciudadano conoce la existencia de la plataforma.

2. Ingreso: Accede y se autentica de forma segura.

3. Exploraci√≥n: Navega los datasets disponibles.

4. Creaci√≥n: Construye un dashboard visual con datos abiertos.

5. Publicaci√≥n: Comparte su dashboard con otros usuarios.

6. Monitoreo: Consulta m√©tricas de visualizaci√≥n y uso.b

![alt text](img/journey1.png)

Este Service Blueprint representa el recorrido completo de una empresa dentro del ecosistema Data Pura Vida, desde el descubrimiento de la plataforma hasta la publicaci√≥n y monitoreo de un dataset privado con fines de monetizaci√≥n.

Este blueprint se organiza en siete fases principales:

1. Conciencia: La empresa conoce la plataforma y sus posibilidades de monetizaci√≥n.

2. Registro: Se registra como entidad jur√≠dica mediante un proceso validado.

3. Carga de datos: Sube su dataset en formatos compatibles como archivos o APIs.

4. Configuraci√≥n: Define permisos, privacidad, cifrado y condiciones de acceso.

5. Validaci√≥n de valor: Eval√∫a la calidad y el potencial del dataset con ayuda de IA.

6. Publicaci√≥n: Hace p√∫blico el dataset con opciones de pago o acceso controlado.

7. Monitoreo: Supervisa ingresos, visualizaciones y transacciones en tiempo real.

![alt text](img/JourneyIP.png)

**Fases principales:**

1. Registro: Un representante oficial crea una cuenta institucional y sube los documentos requeridos.
2. Validaci√≥n: La plataforma aplica validaciones autom√°ticas y manuales con ayuda de IA.
3. Configuraci√≥n: La instituci√≥n decide qu√© datos compartir y con qu√© restricciones.
4. Carga: Se conecta una base de datos externa para carga automatizada.
5. Publicaci√≥n: El dataset queda disponible para actores autorizados mediante pago.
6. Seguimiento: La instituci√≥n consulta m√©tricas de acceso y consumo.

![alt text](img/journey2.png)

## 1.5 Plan de ejecuci√≥n del proyecto

### Plan de Dise√±o

El proyecto se estructura en cinco hitos principales que marcar√°n su progreso:

- Planeamiento del Proyecto
- Supuestos del Proyecto
- Stack Tecnol√≥gico
- Dise√±o de los Componentes
- Validaci√≥n de los requerimientos

Cada uno de estos hitos cuenta con un plazo definido para su ejecuci√≥n, lo cual se puede observar en el siguiente diagrama de Gantt:

![gantt](img/gantt.png)

Dentro de cada hito se contemplan varias tareas. Cada integrante del equipo debe seleccionar una tarea seg√∫n su disponibilidad. Un hito se considera finalizado √∫nicamente cuando todas sus tareas han sido completadas y se encuentran en el tablero de completado en ClickUp.

Adem√°s, como se indic√≥ anteriormente, se realizar√°n reuniones semanales para verificar que el proyecto avance conforme al plan establecido.

Adem√°s, como se dijo previamente, se har√°n reuniones semanales para verificar que el proyecto se est√© realizando seg√∫n lo dice el plan.

### Plan de Ejecuci√≥n para Desarrolladores

Este plan indica c√≥mo avanzar progresivamente en la construcci√≥n del sistema, desde preparar el entorno hasta desplegar y probar los m√≥dulos principales. No detalla c√≥mo funciona cada m√≥dulo, sino c√≥mo se implementan y conectan entre s√≠, con sus respectivos entregables por etapa.

#### 1. Preparaci√≥n del Entorno de Desarrollo

**Objetivo:** Sentar las bases para que todo el equipo trabaje de forma coordinada, segura y replicable.

**Actividades:**

- Establecer repositorio con control de versiones.
- Configurar ambientes separados para desarrollo, pruebas y producci√≥n.
- Montar infraestructura local (contenedores, redes internas, secretos).
- Habilitar flujos b√°sicos de CI/CD y documentaci√≥n t√©cnica inicial.

**Entregables:**

- Repositorio con estructura base.
- Manual de instalaci√≥n local y buenas pr√°cticas.
- Plantilla de CI/CD con al menos una validaci√≥n b√°sica.
- Ambiente de desarrollo replicable con un comando (ej. Docker Compose).

#### 2. Implementaci√≥n del Bioregistro

**Objetivo:** Habilitar la incorporaci√≥n de personas f√≠sicas y jur√≠dicas a la plataforma.

**Actividades:**

- Crear formulario de registro adaptativo por tipo de entidad.
- Implementar simulaciones de validaci√≥n autom√°tica y revisi√≥n manual.
- Gestionar jerarqu√≠as usuario‚Äìorganizaci√≥n y generaci√≥n de llaves.
- Activar sistema de notificaciones y control geogr√°fico b√°sico.

**Entregables:**

- Flujo funcional de registro completo.
- Formulario con l√≥gica adaptativa por entidad.
- Simulaci√≥n de validaciones autom√°ticas y manuales.
- Sistema de emisi√≥n de llaves y notificaci√≥n por correo.

#### 3. Habilitar el Centro de Carga de Datos

**Objetivo:** Permitir a los usuarios cargar datasets desde distintas fuentes.

**Actividades:**

- Desarrollar interfaz de carga de archivos (CSV, Excel, JSON).
- Capturar metadatos b√°sicos (nombre, descripci√≥n, privacidad, etc.).
- Simular conexi√≥n con bases de datos externas.
- Almacenar cargas en espacio temporal con trazabilidad.

**Entregables:**

- M√≥dulo de carga funcional con validaciones m√≠nimas.
- Interfaz para configuraci√≥n de metadatos y privacidad.
- Log de cargas realizadas para trazabilidad.
- Cargas almacenadas provisionalmente.

#### 4. Desarrollar el Motor de Transformaci√≥n

**Objetivo:** Procesar los datos cargados, limpiarlos y convertirlos a un formato interno.

**Actividades:**

- Validar estructura y contenido de cada carga.
- Aplicar l√≥gica b√°sica de transformaci√≥n (normalizaci√≥n, fechas, duplicados).
- Generar versiones limpias de los datos.
- Conectar a almacenamiento de datos validado (La B√≥veda).

**Entregables:**

- Flujo de transformaci√≥n activo con trazabilidad.
- Reportes de validaci√≥n y errores por dataset.
- Datos transformados almacenados de forma estructurada.
- M√©tricas b√°sicas del proceso (tiempo, √©xito, errores).

##### 5. Configurar La B√≥veda

**Objetivo:** Consolidar y proteger los datos procesados para su consumo posterior.

**Actividades:**

- Crear repositorio √∫nico para los datasets internos.
- Implementar segmentaci√≥n de acceso por rol, entidad y tipo de dato.
- Establecer cifrado b√°sico en tr√°nsito y reposo.
- Documentar relaciones entre datasets si aplica.

**Entregables:**

- Sistema de almacenamiento central con control de accesos.
- Datasets organizados y protegidos.
- Trazabilidad de acceso a cada dataset.
- Esquema inicial de relaciones entre datasets (si corresponde).

#### 6. Activar la Visualizaci√≥n y los Dashboards

**Objetivo:** Permitir a los usuarios explorar datos mediante gr√°ficos sin exportarlos.

**Actividades:**

- Crear constructor b√°sico de dashboards con tablas y gr√°ficos.
- Activar vistas previas con datos ficticios y reales.
- Controlar consumo (frecuencia, volumen, consultas).
- Habilitar compartir dashboards con otros usuarios.

**Entregables:**

- M√≥dulo visual con constructor de dashboards funcional.
- Dashboards guardables y compartibles.
- L√≥gica de l√≠mites de uso aplicada.
- Registro de interacciones y consultas realizadas.

#### 7. Simular el Consumo para Modelos de IA

**Objetivo:** Simular acceso regulado a datasets por sistemas externos autorizados.

**Actividades:**

- Definir reglas de uso y l√≠mites por usuario y contexto.
- Habilitar endpoints simulados para "entrenamiento" de IA.
- Registrar y auditar cada consulta o consumo.
- Aplicar restricciones estrictas para evitar abuso.

**Entregables:**

- API simulada para consumo por IA.
- Sistema de seguimiento y l√≠mites aplicado.
- Log de accesos con usuario, contexto y volumen consultado.
- Validaci√≥n de cumplimiento de reglas definidas.

#### 8. Prototipar el Marketplace de Datos

**Objetivo:** Permitir explorar, adquirir y acceder a datasets bajo condiciones.

**Actividades:**

- Crear buscador y navegador de datasets p√∫blicos y pagos.
- Mostrar precios, t√©rminos y opciones de compra.
- Simular proceso de adquisici√≥n y activaci√≥n de accesos.
- Gestionar historial de compras y permisos vigentes.

**Entregables:**

- Interfaz funcional del marketplace.
- Flujo de compra simulado con activaci√≥n de acceso.
- Historial de transacciones por usuario.
- Permisos temporales aplicados tras cada compra.

#### 9. Activar el Backoffice Administrativo

**Objetivo:** Brindar herramientas de gesti√≥n y supervisi√≥n al equipo administrador.

**Actividades:**

- Crear panel seguro para personal autorizado.
- Visualizar registros, cargas y actividad por componente.
- Permitir aprobaci√≥n/rechazo de registros y cargas.
- Generar reportes y estad√≠sticas de uso.

**Entregables:**

- Panel de administraci√≥n con control de usuarios y datos.
- Visualizaci√≥n de actividad y estado del sistema.
- Herramientas para revisi√≥n y auditor√≠a b√°sica.
- Generaci√≥n de reportes autom√°ticos.

#### 10. Pruebas Integradas y Simulaci√≥n de Casos Reales

**Objetivo:** Asegurar que todo funcione de forma integrada.

**Actividades:**

- Simular flujos completos: registro ‚Üí carga ‚Üí transformaci√≥n ‚Üí consumo.
- Usar casos reales o sint√©ticos para testear extremos del sistema.
- Validar reglas de permisos, l√≠mites y visualizaci√≥n.
- Documentar fallos, mejoras y tiempos de respuesta.

**Entregables:**

- Casos de prueba documentados.
- Resultados de pruebas con observaciones.
- Validaci√≥n de flujos completos con usuarios simulados.
- Lista de bugs o ajustes para corregir.

#### 11. Despliegue Controlado y Evaluaci√≥n

**Objetivo:** Publicar la plataforma en un entorno accesible para validaci√≥n final.

**Actividades:**

- Preparar un entorno de pruebas compartido (local o nube).
- Desplegar todos los m√≥dulos de forma conectada.
- Habilitar usuarios de prueba para feedback externo.
- Preparar una demo p√∫blica o privada para presentaci√≥n.

**Entregables:**

- Versi√≥n desplegada en entorno de pruebas.
- Acceso limitado para usuarios externos (testers/docentes).
- Feedback recolectado para iteraci√≥n.
- Material de presentaci√≥n o demo funcional lista.

## 1.6 WBS del sistema

Como parte del an√°lisis inicial del sistema **Data Pura Vida**, se realiz√≥ una descomposici√≥n de alto nivel para identificar los l√≠mites del sistema y los actores involucrados. A continuaci√≥n, se presenta el diagrama de contexto basado en las t√©cnicas descritas para la identificaci√≥n del sistema y sus l√≠mites:

![Work Breakdown Structure](img/WorkBreakdownStructure.jpg)

Esta representaci√≥n facilita el entendimiento general del sistema y servir√° como base para la posterior descomposici√≥n en subsistemas, componentes funcionales y dise√±o arquitect√≥nico detallado.

### Prop√≥sito del diagrama

- **Identificaci√≥n de l√≠mites del sistema:** El diagrama establece una frontera clara entre lo que est√° dentro y fuera del alcance del desarrollo, lo cual es crucial para evitar ambig√ºedades durante el dise√±o detallado.

- **Visualizaci√≥n de los actores externos:** Permite entender qui√©nes interact√∫an con el sistema y con qu√© prop√≥sito.

- **Detecci√≥n de puntos de integraci√≥n:** Ayuda a anticipar necesidades de interoperabilidad, seguridad, formatos de intercambio de datos y protocolos de comunicaci√≥n.

### Consideraciones adicionales

Este diagrama ser√° utilizado como punto de partida para:

- La descomposici√≥n en subsistemas o m√≥dulos funcionales, agrupando responsabilidades afines.

- La definici√≥n de casos de uso y escenarios de interacci√≥n.

- La elaboraci√≥n de la arquitectura t√©cnica, donde se identificar√°n servicios, componentes y flujos de datos internos.

En resumen, este modelo de contexto es una herramienta clave para asegurar un entendimiento compartido del dominio del problema y sentar las bases de una soluci√≥n t√©cnica coherente, escalable y alineada con los objetivos del proyecto.

## 1.7 Evaluaci√≥n de Riesgos

### Metodolog√≠a ISO 31000

La evaluaci√≥n de riesgos sigue los principios de **ISO 31000** para la gesti√≥n de riesgos del proyecto Data Pura Vida.

### Marco de Evaluaci√≥n:

La evaluaci√≥n de riesgos utiliza una matriz de probabilidad versus impacto basada en criterios espec√≠ficos del proyecto Data Pura Vida y su contexto de dise√±o de sistemas complejos.

### Escala de probabilidad:

- Muy Alta (100%) : Es pr√°cticamente seguro que el riesgo ocurrir√° durante el proyecto (9 de cada 10 proyectos similares)
- Alta (80%) : Es muy probable que el riesgo se materialice (7-8 de cada 10 casos)
- Medios (60%) : Hay una posibilidad moderada de que ocurra (5-6 de cada 10 casos)
- Baja (40%) : Es poco probable pero posible que suceda (3-4 de cada 10 casos)
- Muy Baja (20%) : Es muy poco probable que se materialice (1-2 de cada 10 casos)

### Escala de Impacto:

- Muy Alto (100%) : Falla completa del proyecto, redise√±o total necesario, o retraso superior a 4 semanas
- Alto (80%) : Compromete objetivos principales del proyecto, retraso de 2-4 semanas, o afecta m√∫ltiples componentes cr√≠ticos
- Medio (60%) : Afecta la calidad del dise√±o o genera retraso de 1-2 semanas, requiere trabajo adicional significativo
- Bajo (40%) : Impacto menor en cronograma (3-7 d√≠as) o calidad, se puede resolver con ajustes menores
- Muy Bajo (20%) : Impacto m√≠nimo (menos de 3 d√≠as), no afecta objetivos principales del proyecto

### Riesgos para el Dise√±o de Data Pura Vida

| ID      | Categor√≠a         | Riesgo                                               | Descripci√≥n Detallada                                                                                                                                                                           | Probabilidad        | Impacto             | Clasificaci√≥n   | Estrategia     | Plan de Respuesta                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ------- | ----------------- | ---------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- | ------------------- | --------------- | -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **R01** | **Dise√±o**        | **Complejidad arquitect√≥nica del ecosistema**        | Dise√±ar una arquitectura que integre efectivamente portal web, API backend, datalake, backoffice y m√∫ltiples sistemas de seguridad requiere experiencia en arquitecturas distribuidas complejas | **Muy Alta (100%)** | **Muy Alto (100%)** | **üî¥ EXTREMO**  | **MITIGAR**    | **Prevenci√≥n:** Definir m√°ximo 5 patrones arquitect√≥nicos (Semana 1), crear ADRs (Architecture Decision Records) para cada decisi√≥n, revisi√≥n arquitect√≥nica obligatoria cada viernes 1h con validaci√≥n t√©cnica<br>**Contingencia:** Crear spike de 16h para dise√±ar arquitectura simplificada (3 capas: Frontend-API-Data), eliminar microservicios y usar monolito modular, reducir integraciones complejas a APIs REST est√°ndar |
| **R02** | **Alcance**       | **Subestimaci√≥n del alcance del dise√±o**             | El tiempo asignado puede ser insuficiente para dise√±ar completamente todos los componentes t√©cnicos con el nivel de detalle requerido para un sistema de esta magnitud                          | **Muy Alta (100%)** | **Medio (60%)**     | **üü† ALTO**     | **MITIGAR**    | **Prevenci√≥n:** Planning Poker diario 30min, re-estimaci√≥n mi√©rcoles, time tracking obligatorio en ClickUp, descomponer tareas en m√°ximo 8h cada una<br>**Contingencia:** Si desv√≠o > 150% en 3 tareas: reducir nivel de detalle en diagramas de secuencia (de completos a conceptuales), simplificar especificaciones APIs (menos endpoints), priorizar componentes cr√≠ticos primero, redistribuir trabajo en 1 d√≠a               |
| **R03** | **Documentaci√≥n** | **Inconsistencias en la documentaci√≥n t√©cnica**      | Generar documentaci√≥n t√©cnica coherente entre arquitectura de alto nivel, especificaciones de APIs, modelos de datos, diagramas de seguridad y patrones de integraci√≥n                          | **Alta (80%)**      | **Alto (80%)**      | **üü† ALTO**     | **MITIGAR**    | **Prevenci√≥n:** Templates est√°ndar GitHub, peer review obligatorio, checklist calidad por componente<br>**Contingencia:** Auditor√≠a documental semanal viernes 2h, refactoring inmediato de documentos inconsistentes, responsable: Santiago Chaves                                                                                                                                                                                |
| **R04** | **Tiempo**        | **Cronograma optimista para la complejidad**         | El tiempo asignado puede ser insuficiente para dise√±ar completamente todos los componentes t√©cnicos con el nivel de detalle requerido para un sistema de esta magnitud                          | **Muy Alta (100%)** | **Medio (60%)**     | **üü† ALTO**     | **MITIGAR**    | **Prevenci√≥n:** Re-estimaci√≥n semanal con burndown charts, escalaci√≥n autom√°tica si > 20% desv√≠o, buffer de 2 d√≠as por semana<br>**Contingencia:** Redistribuir tareas inmediatamente, asignar 2 personas a componentes cr√≠ticos (Bio Registro y La B√≥veda), reducir documentaci√≥n detallada a documentaci√≥n funcional, completar dise√±o b√°sico de todos los componentes                                                           |
| **R05** | **T√©cnico**       | **Complejidad del motor de transformaci√≥n**          | Especificar t√©cnicamente un motor que procese autom√°ticamente m√∫ltiples formatos, detecte duplicados, relacione datos y aplique transformaciones inteligentes es altamente complejo             | **Media (60%)**     | **Muy Alto (100%)** | **üü† ALTO**     | **MITIGAR**    | **Prevenci√≥n:** Spike 16h Apache Spark + PySpark (Luis David), prototipo 3 casos (CSV‚ÜíPostgreSQL, JSON‚ÜíS3, API‚ÜíDynamoDB), validar 10MB en <30min<br>**Contingencia:** Motor simplificado con AWS Glue + transformaciones predefinidas, o integraci√≥n Talend Open Studio (setup 1 semana)                                                                                                                                           |
| **R06** | **Seguridad**     | **Dise√±o de sistema de cifrado tripartito**          | Especificar correctamente un sistema de llaves criptogr√°ficas divididas entre tres custodios, incluyendo protocolos de recuperaci√≥n y validaci√≥n mancomunada                                    | **Baja (40%)**      | **Muy Alto (100%)** | **üü† ALTO**     | **TRANSFERIR** | **Prevenci√≥n:** Consulta expertos criptograf√≠a (8h consultor√≠a), documentar est√°ndares FIPS 140-2, validaci√≥n externa con especialista<br>**Contingencia:** Implementar cifrado HSM tradicional AWS KMS, esquema dual en lugar de tripartito, mantiene seguridad pero reduce complejidad                                                                                                                                           |
| **R07** | **Integraci√≥n**   | **Interfaces entre componentes mal definidas**       | Riesgo de que las especificaciones de APIs, contratos de datos y protocolos de comunicaci√≥n entre portal, backend y datalake no sean completamente compatibles                                  | **Media (60%)**     | **Alto (80%)**      | **üü† ALTO**     | **MITIGAR**    | **Prevenci√≥n:** Contratos OpenAPI 3.0 obligatorios, reuniones sync bi-semanales martes/viernes, diagramas de secuencia por flujo<br>**Contingencia:** Workshop alineaci√≥n 4h si incompatibilidades detectadas, redise√±o contratos en 2 d√≠as, validaci√≥n cruzada inmediata                                                                                                                                                          |
| **R08** | **Escalabilidad** | **Arquitectura no preparada para la carga esperada** | El dise√±o puede no contemplar adecuadamente el manejo de millones de registros, miles de usuarios concurrentes y procesamiento de grandes vol√∫menes de datos                                    | **Baja (40%)**      | **Medio (60%)**     | **üü° MODERADO** | **MITIGAR**    | **Prevenci√≥n:** Definir l√≠mites t√©cnicos concretos por componente (Bio Registro: 100 req/min, La B√≥veda: 10GB/d√≠a), especificar patrones de escalabilidad (load balancers, auto-scaling), calcular capacidad m√≠nima requerida<br>**Contingencia:** Redise√±ar arquitectura con clustering activo/pasivo, implementar sharding en dise√±o de BD, especificar CDN y caching layers, definir estrategia de particionamiento horizontal  |
| **R09** | **Recursos**      | **Disponibilidad limitada del Product Owner**        | El Product Owner puede no estar disponible para validar decisiones arquitect√≥nicas cr√≠ticas o para resolver ambig√ºedades en los requerimientos t√©cnicos                                         | **Media (60%)**     | **Bajo (40%)**      | **üü° MODERADO** | **ACEPTAR**    | **Prevenci√≥n:** Agenda fija martes/viernes, decisiones escritas en Slack, timeboxing 24h para respuestas<br>**Contingencia:** Escalaci√≥n a stakeholders si > 48h sin respuesta, decisiones t√©cnicas por equipo con validaci√≥n posterior, documentar en GitHub para trazabilidad                                                                                                                                                    |
| **R10** | **Coordinaci√≥n**  | **Dise√±os de componentes desconectados**             | Los diferentes integrantes del equipo pueden dise√±ar sus componentes sin suficiente coordinaci√≥n, resultando en interfaces incompatibles o duplicaci√≥n de funcionalidades                       | **Media (60%)**     | **Medio (60%)**     | **üü° MODERADO** | **MITIGAR**    | **Prevenci√≥n:** Sincronizaci√≥n semanal viernes 1h, documentaci√≥n centralizada GitHub, daily stand-ups 15min<br>**Contingencia:** Workshop alineaci√≥n medio d√≠a si interfaces incompatibles, redise√±o coordinado 3 d√≠as m√°ximo, matriz de dependencias actualizada                                                                                                                                                                  |

## 1.8 Definici√≥n de KPIs
### KPIs por Hito del Proyecto

#### Hito 1: Planeamiento del Proyecto

**Per√≠odo**: 18-22 Mayo 2025 (Semana W20)

| KPI                              | M√©trica                            | Objetivo | M√©todo de Recolecci√≥n                              |
| -------------------------------- | ---------------------------------- | -------- | -------------------------------------------------- |
| **Cumplimiento de cronograma**   | % de tareas completadas a tiempo   | 100%     | ClickUp - estado de tareas vs. fechas planificadas |
| **Completitud de documentaci√≥n** | % de entregables documentados      | 100%     | Revisi√≥n de README y archivos en GitHub            |
| **Participaci√≥n del equipo**     | % de integrantes activos en tareas | 100%     | ClickUp - asignaci√≥n y progreso de tareas          |
| **Validaci√≥n del Product Owner** | % de entregables aprobados         | 100%     | Estado "Aprobado" en ClickUp                       |

#### Hito 2: Supuestos del Proyecto

**Per√≠odo**: 25-31 Mayo 2025 (Semana W21)

| KPI                            | M√©trica                              | Objetivo | M√©todo de Recolecci√≥n                            |
| ------------------------------ | ------------------------------------ | -------- | ------------------------------------------------ |
| **Cumplimiento de cronograma** | % de tareas completadas a tiempo     | 100%     | ClickUp - comparaci√≥n fecha planificada vs. real |
| **Calidad de supuestos**       | N√∫mero de supuestos validados con PO | 100%     | Documentaci√≥n de validaciones en Slack/GitHub    |
| **Identificaci√≥n de riesgos**  | N√∫mero de riesgos documentados       | ‚â•10      | Matriz de riesgos actualizada                    |

#### Hito 3: Stack Tecnol√≥gico

**Per√≠odo**: 1-7 Junio 2025 (Semana W22)

| KPI                               | M√©trica                                       | Objetivo | M√©todo de Recolecci√≥n                        |
| --------------------------------- | --------------------------------------------- | -------- | -------------------------------------------- |
| **Cumplimiento de cronograma**    | % de tareas completadas a tiempo              | 100%     | ClickUp - estado vs. cronograma              |
| **Decisiones tecnol√≥gicas**       | % de tecnolog√≠as seleccionadas y justificadas | 100%     | Documentaci√≥n t√©cnica en GitHub              |
| **Factibilidad t√©cnica**          | Prototipos de concepto funcionando            | ‚â•2       | Repositorio con ejemplos funcionales         |
| **Compatibilidad con requisitos** | % de requisitos cubiertos por stack           | 100%     | Matriz de trazabilidad requisitos-tecnolog√≠a |

#### Hito 4: Dise√±o de los Componentes

**Per√≠odo**: 8-14 Junio 2025 (Semana W23)

| KPI                               | M√©trica                                   | Objetivo | M√©todo de Recolecci√≥n                 |
| --------------------------------- | ----------------------------------------- | -------- | ------------------------------------- |
| **Cumplimiento de cronograma**    | % de tareas completadas a tiempo          | 100%     | ClickUp - seguimiento diario          |
| **Cobertura de componentes**      | % de componentes dise√±ados vs. requeridos | 100%     | Documentaci√≥n de arquitectura         |
| **Calidad del dise√±o**            | Revisiones aprobadas por PO               | 100%     | Estados de aprobaci√≥n en ClickUp      |
| **Integraci√≥n entre componentes** | % de interfaces definidas                 | 100%     | Diagramas de integraci√≥n documentados |

#### Hito 5: Validaci√≥n de los Requerimientos

**Per√≠odo**: 15-21 Junio 2025 (Semana W24)

| KPI                            | M√©trica                           | Objetivo | M√©todo de Recolecci√≥n             |
| ------------------------------ | --------------------------------- | -------- | --------------------------------- |
| **Cumplimiento de cronograma** | Entrega a tiempo                  | 100%     | Fecha de entrega final            |
| **Cobertura de requisitos**    | % de requisitos validados         | 100%     | Matriz de trazabilidad completa   |
| **Calidad de documentaci√≥n**   | Checklist de atributos completado | 100%     | Revisi√≥n contra checklist oficial |
| **Aprobaci√≥n final**           | Validaci√≥n del Product Owner      | 100%     | Confirmaci√≥n formal de aceptaci√≥n |

### KPIs Transversales del Proyecto

#### Gesti√≥n y Comunicaci√≥n

| KPI                         | M√©trica                        | Objetivo | Frecuencia de Medici√≥n |
| --------------------------- | ------------------------------ | -------- | ---------------------- |
| **Comunicaci√≥n efectiva**   | Respuestas en Slack < 24h      | 90%      | Semanal                |
| **Reuniones semanales**     | Asistencia a reuniones         | 100%     | Semanal                |
| **Actualizaci√≥n de tareas** | Tareas actualizadas en ClickUp | Diario   | Diario                 |
| **Resoluci√≥n de bloqueos**  | Tiempo promedio de resoluci√≥n  | <48h     | Semanal                |

#### Calidad y Riesgos

| KPI                        | M√©trica                             | Objetivo | Frecuencia de Medici√≥n |
| -------------------------- | ----------------------------------- | -------- | ---------------------- |
| **Gesti√≥n de riesgos**     | % de riesgos con plan de mitigaci√≥n | 100%     | Semanal                |
| **Incidencias cr√≠ticas**   | N√∫mero de riesgos materializados    | 0        | Semanal                |
| **Calidad de entregables** | % de entregables sin retrabajos     | 90%      | Por hito               |

### Mecanismos de Recolecci√≥n y C√°lculo

#### Herramientas de Monitoreo

1. **ClickUp**: Seguimiento autom√°tico de tareas, tiempos y estados
2. **Slack**: M√©tricas de comunicaci√≥n y tiempo de respuesta
3. **GitHub**: Commits, documentaci√≥n y versiones
4. **Reuniones semanales**: Revisi√≥n manual de KPIs y ajustes


# 2. Supuestos del proyecto

## 2.1 Est√°ndares y Regulaciones

Para el proyecto "Data Pura Vida", la revisi√≥n de est√°ndares y regulaciones nacionales e internacionales es crucial para garantizar la legalidad, seguridad, privacidad y gobernanza de los datos. A continuaci√≥n, se detalla la relevancia de cada una de las normativas mencionadas y c√≥mo se aplican a los requerimientos de la plataforma:

### 1. Ley 8968 (Costa Rica) - Ley de Protecci√≥n de la Persona frente al Tratamiento de sus Datos Personales

Esta es la normativa nacional fundamental que rige la protecci√≥n de datos personales en Costa Rica. "Data Pura Vida" debe cumplir √≠ntegramente con sus disposiciones, dado que el sistema manejar√° una gran cantidad de datos personales de personas f√≠sicas y jur√≠dicas.

#### Aplicaci√≥n a los Requerimientos de la Plataforma:

#### Bioregistro:

#### ART√çCULO 5.- Principio de consentimiento informado:\*\*REST, GraphQL,

El principio del consentimiento de informaci√≥n se regie por dos puntos importantes, a continuaci√≥n, se mencionan los dos puntos y su aplicaci√≥n dentro de la plataforma:

**Punto 1 - Obligaci√≥n de informar**
Durante el proceso de registro en **Bio Registro Verde**, el sistema debe presentar de forma destacada y f√°cil de entender la siguiente informaci√≥n:

- La existencia de la base de datos **Data Pura Vida**.

- La finalidad clara de la recolecci√≥n de datos (ej. validar identidad para operar en el ecosistema, permitir compartir/consumir datos, etc.).

- Los destinatarios de los datos (ej. otros usuarios del ecosistema con los que el usuario decida compartir, la administraci√≥n de "Data Pura Vida").

- La obligatoriedad de ciertos datos (ej. c√©dula, informaci√≥n tributaria) y las consecuencias de no proporcionarlos (ej. imposibilidad de completar el registro o acceder a ciertas funcionalidades).

- Los derechos ARCO (Acceso, Rectificaci√≥n, Cancelaci√≥n y Oposici√≥n) y c√≥mo ejercerlos dentro del portal. Esta informaci√≥n debe estar disponible antes de que el usuario env√≠e sus datos.

  **Punto 2 - Otorgamiento del consentimiento**
  El consentimiento para el tratamiento de datos debe ser expreso. "Bio Registro Verde" debe implementar un mecanismo de aceptaci√≥n clara y expl√≠cita, como:

- Un checkbox de "Acepto los T√©rminos y Condiciones y la Pol√≠tica de Privacidad" que el usuario debe marcar activamente.

- La documentaci√≥n de este consentimiento debe almacenarse de forma segura, vinculada al registro del usuario.

- La autenticaci√≥n avanzada (identidad digital, biometr√≠a, prueba de vida, MFA) y la validaci√≥n documental automatizada por IA refuerzan la seguridad del proceso de consentimiento, asegurando que la persona que da el consentimiento es quien dice ser.

#### ART√çCULOS 6 y 7 - Principio de calidad de la informaci√≥n; y Derechos que le asisten a la persona( Derechos ARCO ):

Estos principios garantizan que los datos sean apropiados y que los usuarios mantengan el control sobre su informaci√≥n.

**Aplicaci√≥n a la Plataforma:**

Los datos solicitados (informaci√≥n personal, societaria, legal y tributaria) deben ser estrictamente necesarios y pertinentes para la creaci√≥n y operaci√≥n de una cuenta dentro del ecosistema **Data Pura Vida**.

La implementaci√≥n de IA para verificar la completitud y validez de los documentos subidos (c√©dulas, actas, registros tributarios) es clave para asegurar la veracidad y exactitud de los datos, cumpliendo con el Art√≠culo 6 (Principio de Calidad de la Informaci√≥n). Esto tambi√©n ayuda a evitar la recolecci√≥n de datos fraudulentos.

El portal debe ofrecer mecanismos claros y accesibles para que los usuarios puedan acceder, rectificar o solicitar la eliminaci√≥n de sus datos personales, directamente desde su perfil o mediante un proceso de solicitud documentado, en cumplimiento con el Art√≠culo 7 (Derechos ARCO). Esto incluye la posibilidad de actualizar informaci√≥n o cerrar cuentas.

#### ART√çCULO 9 - Categor√≠as particulares de los datos:

Aunque los requerimientos actuales del "Bio Registro Verde" no mencionan expl√≠citamente la recolecci√≥n de "datos sensibles" (como salud, origen racial, etc.), si el alcance de la plataforma evolucionara para incluirlos, "Data Pura Vida" deber√° implementar garant√≠as adicionales y obtener un consentimiento a√∫n m√°s expl√≠cito y espec√≠fico para el tratamiento de estas categor√≠as, seg√∫n lo exige el Art√≠culo 9 (Datos Sensibles).

#### ART√çCULO 10 - Seguridad de los Datos:

Este art√≠culo impone la obligaci√≥n de proteger los datos de car√°cter personal y evitar su alteraci√≥n, destrucci√≥n accidental o il√≠cita, p√©rdida, tratamiento o acceso no autorizado, as√≠ como cualquier otra acci√≥n contraria a esta ley al responsable de la base de datos.

Los requerimientos de seguridad del **Bioregistro** son una respuesta directa al Art√≠culo 10 ( Seguridad de los datos):

- El uso de autenticaci√≥n avanzada (identidad digital, biometr√≠a, prueba de vida, MFA) son medidas de seguridad l√≥gicas para controlar el acceso.

- La asignaci√≥n y protecci√≥n de llaves de seguridad criptogr√°ficas (sim√©tricas y asim√©tricas), incluyendo el sistema de llave tripartita, son medidas de seguridad l√≥gicas esenciales para proteger la integridad y confidencialidad de la identidad y datos asociados.

- La restricci√≥n de acceso al portal solo desde direcciones IP ubicadas en Costa Rica, o mediante listas blancas de IPs institucionales, es una medida de seguridad l√≥gica que limita el acceso geogr√°fico y fortalece la protecci√≥n contra accesos no autorizados.

- El cifrado de datos en reposo y en tr√°nsito y el control de acceso estricto para ingenieros (para evitar acceso a datos en claro) son vitales para cumplir con el deber de confidencialidad y proteger la informaci√≥n sensible.

#### Feliz Compartiendo Datos:

#### ART√çCULO 4.- Autodeterminaci√≥n informativa:

La capacidad de los usuarios para gestionar sus datasets es central para este principio.

La secci√≥n **Feliz Compartiendo Datos** encarna el Art√≠culo 4 (Autodeterminaci√≥n Informativa) al permitir a los usuarios:

- Decidir qu√© datos compartir dentro del ecosistema
- Configurar la visibilidad del dataset (p√∫blico o privado).
- Definir el modelo de acceso (gratuito o pagado).
- Establecer control granular sobre el acceso por instituci√≥n, persona o grupo de actores.

Estas funcionalidades garantizan que el titular mantenga el control sobre el uso y la difusi√≥n de su informaci√≥n.

#### ART√çCULO 6 - Principio de calidad de la informaci√≥n:

La Ley 8968 exige que la recolecci√≥n y uso de datos sea proporcional a la finalidad.

Los requerimientos de **Feliz Compartiendo Datos** se alinean con el Art√≠culo 6 (Principio de Calidad de la Informaci√≥n) al promover la minimizaci√≥n y el prop√≥sito limitado:

- La opci√≥n de seleccionar campos espec√≠ficos a cifrar dentro del dataset permite a los usuarios proteger solo la informaci√≥n sensible, sin necesidad de cifrar todo, lo que se alinea con la minimizaci√≥n del tratamiento de datos sensibles.

- La capacidad de restringir acceso a datos por l√≠mites de tiempo, volumen o frecuencia de consulta asegura que el acceso y uso de los datos se realice √∫nicamente para la finalidad acordada y bajo las condiciones definidas por el titular.

#### ART√çCULO 14 - Transferencia de datos personales, regla general:

Si bien la "comercializaci√≥n" dentro del ecosistema se enfoca en el acceso y consumo interno, el Art√≠culo 20 es una consideraci√≥n preventiva. Si la plataforma habilitara en el futuro transferencias a entidades o servicios fuera de Costa Rica (por ejemplo, para alimentar modelos de IA en la nube en otros pa√≠ses), se deber√≠an cumplir las condiciones establecidas por este art√≠culo, que incluyen el consentimiento del titular y garant√≠as de seguridad adecuadas para los datos transferidos.

#### Descubriendo Costa Rica:

#### ART√çCULOS 4 y 10 - Autodeterminaci√≥n informativa; Seguridad de los datos:

La protecci√≥n de la autodeterminaci√≥n informativa y la seguridad son cruciales en la visualizaci√≥n.

La secci√≥n **Descubriendo Costa Rica** refuerza el Art√≠culo 4 (Autodeterminaci√≥n Informativa) y el Art√≠culo 11 (Seguridad de los datos) al:

- Impedir la descarga directa de datos en cualquier momento y bloquear exportaciones de gr√°ficos y contenidos. Esta medida es fundamental para mantener el control del titular sobre la informaci√≥n y prevenir usos no autorizados fuera del entorno seguro de la plataforma.

- Al obligar a la visualizaci√≥n exclusivamente dentro del portal, "Data Pura Vida" implementa una medida de seguridad l√≥gica que reduce el riesgo de fugas de datos y asegura que el uso de la informaci√≥n est√© bajo la gobernanza y protecci√≥n de la Ley 8968. Esto tambi√©n apoya el principio de limitaci√≥n de la finalidad.

#### Backend API y Datalake: Aplicaci√≥n de la Ley 8968 (Costa Rica)

#### Art√≠culo 10 y 30 - Seguridad de los datos; Faltas graves:

Estos son el muy importantes para la infraestructura de seguridad.

Los requerimientos del Backend API y el Datalake son directamente aplicables al Art√≠culo 10 (Seguridad de los datos) y al Art√≠culo 30 (Faltas Graves):

La exigencia de que "Data Pura Vida" implemente medidas t√©cnicas y organizativas para asegurar la protecci√≥n de los datos se refleja en:

- La protecci√≥n de la API mediante whitelist de IPs, validaci√≥n de tokens y MFA.

- Los m√≥dulos separados para gesti√≥n de credenciales, firmas y cifrado de datos.

- La trazabilidad, cumplimiento legal y control de cada transacci√≥n, lo que demuestra un enfoque proactivo en la gesti√≥n de la seguridad.

- La implementaci√≥n de RBAC (Role-Based Access Control) y RLS (Row-Level Security) en el Datalake, junto con el cifrado de datos sensibles en reposo y en tr√°nsito, son medidas de seguridad robustas para proteger la confidencialidad e integridad de la informaci√≥n, previniendo accesos indebidos.

- La prohibici√≥n expl√≠cita de que ingenieros o personal t√©cnico accedan a los datos en claro o sin autorizaci√≥n refuerza el principio de "conocimiento necesario" y el deber de confidencialidad del Art√≠culo 10.

La auditor√≠a detallada de todas las operaciones realizadas en el sistema (por usuario, acci√≥n, fecha y efecto) y el historial de consumo de datos son esenciales para:

- Demostrar el cumplimiento con la Ley 8968 ante la PRODHAB o en caso de una auditor√≠a.

- Permitir la trazabilidad necesaria para la rendici√≥n de cuentas y la identificaci√≥n de cualquier actividad irregular, ayudando a prevenir faltas graves como el tratamiento no autorizado de datos.

- Facilitar la extracci√≥n de evidencias para procesos legales o regulatorios.

### 2. GDPR (General Data Protection Regulation)

Aunque es una regulaci√≥n de la Uni√≥n Europea, el GDPR tiene un alcance extraterritorial. Si "Data Pura Vida" procesa datos de ciudadanos o residentes de la UE, o si ofrece bienes y servicios a ellos, entonces el GDPR es aplicable, independientemente de d√≥nde se encuentre el servidor o la empresa. Dado que Costa Rica es un destino tur√≠stico y centro de negocios internacional, es muy probable que haya interacci√≥n con datos de la UE. Adem√°s, el GDPR ha influenciado muchas leyes de privacidad a nivel mundial, por lo que su cumplimiento a menudo supera los requisitos de otras normativas locales.

#### Aplicaci√≥n a los Requerimientos de la Plataforma:

#### Bio Registro Verde:

**Bases Legales para el Tratamiento:** El registro debe establecer claramente la base legal para el procesamiento de cada tipo de dato (consentimiento, obligaci√≥n legal, inter√©s leg√≠timo, etc.) seg√∫n lo establece el GDPR (art√≠culos 6 y 7). El consentimiento debe ser libre, espec√≠fico, informado e inequ√≠voco, y f√°cilmente revocable.

**Privacy by Design and by Default:** El dise√±o del sistema debe integrar la privacidad desde el inicio (por ejemplo, el cifrado de datos, el control granular de acceso, la minimizaci√≥n de datos por defecto), como exige el art√≠culo 25 del GDPR.

**Derechos de los Interesados:** El GDPR otorga derechos robustos a los interesados (data subjects):

- **Derecho a la Informaci√≥n:** Transparencia sobre el procesamiento de datos (art√≠culos 13 y 14 del GDPR).

- **Derecho de Acceso:** Los usuarios deben poder acceder a sus datos personales (Art√≠culo 15).

- **Derecho de Rectificaci√≥n:** Correcci√≥n de datos inexactos (Art√≠culo 16).

- **Derecho de Supresi√≥n ("Derecho al Olvido"):** Eliminaci√≥n de datos bajo ciertas condiciones (Art√≠culo 17).

- **Derecho a la Limitaci√≥n del Tratamiento:** Restringir el procesamiento de datos (Art√≠culo 18).

- **Derecho a la portabilidad de los Datos:** Permite a los usuarios r recibir y transmitir sus datos en un formato estructurado (art√≠culo 20).

- **Derecho de Oposici√≥n:** Oponerse al tratamiento de sus datos (Art√≠culo 21).

- **Derechos en relaci√≥n con Decisiones Automatizadas y Perfilado:** El uso de IA debe considerar el derecho a no ser objeto de decisiones basadas √∫nicamente en procesamiento automatizado (art√≠culo 22).

#### Feliz Compartiendo Datos:

**Transferencias Internacionales de Datos:** Si los datos pudieran ser accedidos o transferidos fuera del Espacio Econ√≥mico Europeo, deben cumplirse los requisitos del Cap√≠tulo V del GDPR (art√≠culos 44 al 50), incluyendo garant√≠as como cl√°usulas tipo o reglas corporativas vinculantes.

**Evaluaci√≥n de Impacto de Protecci√≥n de Datos (DPIA):** Para el procesamiento de datos de alto riesgo (como la combinaci√≥n de grandes vol√∫menes de datos sensibles, uso de IA para perfilado), una DPIA ser√≠a obligatoria bajo el GDPR (Art√≠culo 35).

##### Backend API y Datalake:

**Oficial de Protecci√≥n de Datos (DPO):** Si se cumple con los criterios (ej. procesamiento a gran escala de categor√≠as especiales de datos o monitoreo sistem√°tico de interesados), Data Pura Vida deber√≠a designar un Oficial de Protecci√≥n de Datos (DPO) (Art√≠culo 37).

**Notificaci√≥n de Violaciones de Seguridad:** En caso de una brecha de seguridad que afecte datos personales, el GDPR exige la notificaci√≥n a la autoridad de control en un plazo de 72 horas y, en ciertos casos, tambi√©n a los interesados (Art√≠culos 33 y 34). Esto implica un robusto sistema de monitoreo y respuesta a incidentes.

### 3. ISO/IEC 27001 - Sistemas de Gesti√≥n de la Seguridad de la Informaci√≥n (SGSI)

Relevancia para **Data Pura Vida**: Aunque no es una ley obligatoria, la ISO/IEC 27001 es un est√°ndar internacional que proporciona un marco para establecer, implementar, mantener y mejorar continuamente un Sistema de Gesti√≥n de la Seguridad de la Informaci√≥n (SGSI). Obtener la certificaci√≥n ISO 27001 demostrar√≠a un compromiso serio con la seguridad de la informaci√≥n y la protecci√≥n de activos, generando confianza en un ecosistema de datos.

#### Aplicaci√≥n a los Requerimientos de la Plataforma:

La ISO 27001 se basa en la identificaci√≥n y gesti√≥n de riesgos de seguridad de la informaci√≥n. Esto es fundamental para "Data Pura Vida", dada la sensibilidad y el volumen de los datos. Se debe realizar una evaluaci√≥n de riesgos exhaustiva para determinar las medidas de seguridad necesarias.

Estan los controles de seguidad ubicadas en el anexo A de ISO 27001/ISO 27002 en la cual se ubican el est√°ndar para un conjunto de controles que son relevantes para todos los requerimientos de la plataforma:

- **A.5 Pol√≠ticas de seguridad de la informaci√≥n:** Definir pol√≠ticas claras para el uso y acceso de datos.
- **A.6 Organizaci√≥n de la seguridad de la informaci√≥n:** Definir roles y responsabilidades (ej. PM, roles del Backoffice).
- **A.6.2.3 Tratamiento de la seguridad en contratos con terceras personas:** Gesti√≥n de la seguridad con terceros si se utilizan servicios externos.
- **A.7 Gesti√≥n de activos:** Clasificaci√≥n de la informaci√≥n (p√∫blico/privado, gratuito/pagado), y protecci√≥n de activos.
- **A.8 Seguridad de los recursos humanos:** Seguridad antes, durante y despu√©s del empleo (fundamental para el personal de Data Pura Vida, incluyendo ingenieros y personal de backoffice).
- **A.10.6 Gesti√≥n de seguridad de redes:** Protecci√≥n de la informaci√≥n en redes.
- **A.10.10 Monitoreo:** Gesti√≥n de logs (auditor√≠a), monitoreo de sistemas, gesti√≥n de vulnerabilidades.
- **A.11 Control de acceso:** Implementaci√≥n de RBAC, RLS, MFA, validaci√≥n de tokens, whitelists de IPs, y la asignaci√≥n/revocaci√≥n de llaves de seguridad. La "llave tripartita" es un control de seguridad avanzado.
- **A.12.3 Controles criptogr√°ficos:** El uso de cifrado para datos en reposo y en tr√°nsito, y el cifrado de campos espec√≠ficos, son controles clave.
- **A.13 Gesti√≥n de incidentes de seguridad de la informaci√≥n:** Proceso para detectar, reportar, responder y aprender de los incidentes (crucial para notificar brechas como exige GDPR).
- **A.14.1 Aspectos de seguridad de la informaci√≥n de la gesti√≥n de la continuidad del negocio:** Planes de respaldo y recuperaci√≥n.
- **A.15 Cumplimiento:** Identificaci√≥n y cumplimiento de requisitos legales y contractuales.

**Mejora Continua (Ciclo PDCA):** ISO 27001 promueve un ciclo de Planificar-Hacer-Verificar-Actuar (PDCA), lo que se alinea con la necesidad de monitoreo continuo de m√©tricas, auditor√≠as y revisi√≥n de la direcci√≥n para garantizar la mejora continua de la seguridad y el cumplimiento normativo.

### 4. OECD Data Governance

La OCDE establece principios fundamentales de gobernanza de datos que sirven como referencia para proyectos como Data Pura Vida, orientados a maximizar el uso y la compartici√≥n responsable de datos, mientras se protege la privacidad y se fortalece la confianza.

#### Principios Fundamentales de la OCDE

#### Enfoque Integral (Whole-of-Government)

- Promueve la participaci√≥n de todos los actores (p√∫blicos y privados) y la coherencia entre sectores y niveles de gobierno.

#### Equilibrio de Beneficios y Riesgos

- Reconoce la necesidad de equilibrar los beneficios del acceso y uso de datos con los riesgos asociados (privacidad, seguridad, propiedad intelectual).

#### Diversidad de Datos y Respeto a Derechos

- Reconoce distintos niveles de sensibilidad y riesgo de los datos, y garantiza derechos como el acceso, la rectificaci√≥n y la autodeterminaci√≥n informativa.

#### Fortalecimiento de Capacidades y Confianza

- Fomenta la cultura de datos, el desarrollo de infraestructura y el establecimiento de relaciones de confianza entre actores.

#### Recomendaciones de la OCDE Aplicables

La OCDE ha emitido siete recomendaciones que sirven como marco para la gobernanza de datos y que deben integrarse a Data Pura Vida:

- Acceso a datos de investigaci√≥n financiados p√∫blicamente (2006).
- Acceso y uso de informaci√≥n del sector p√∫blico (2008).
- Protecci√≥n de privacidad y flujos transfronterizos de datos (2013).
- Gobernanza de datos de salud (2016).
- Estrategias de gobierno digital (2014).

#### Aplicaci√≥n a los Requerimientos de la Plataforma

#### Bio Registro Verde

Aplica el enfoque integral al registrar a todos los actores relevantes con autenticaci√≥n avanzada (MFA, biometr√≠a, prueba de vida).

Implementa controles criptogr√°ficos (llaves sim√©tricas y asim√©tricas) y segmentaci√≥n de acceso por roles, siguiendo los est√°ndares de confianza y seguridad.

#### Feliz Compartiendo Datos

Permite la clasificaci√≥n de datasets (p√∫blicos, privados, gratuitos o pagados) y la definici√≥n de controles de acceso granular, balanceando beneficios de compartici√≥n con protecci√≥n de derechos.

Soporta m√∫ltiples formatos de carga y mecanismos de conexi√≥n, fomentando la interoperabilidad.

#### Descubriendo Costa Rica

Limita la descarga directa y exportaci√≥n de datos, asegurando que el acceso a datos se haga solo en entornos seguros y controlados.

Construcci√≥n de dashboards personalizables con visibilidad granular, respetando la autonom√≠a de los usuarios y el principio de minimizaci√≥n.

#### Backend API y Datalake

Implementaci√≥n de MFA, whitelists de IPs y control de acceso estricto para proteger la confidencialidad y cumplir con las recomendaciones de seguridad de la OCDE.

Uso de IA para normalizaci√≥n, relaci√≥n de datos y detecci√≥n de duplicidades, reforzando la calidad y eficiencia de la gobernanza de datos.

La implementaci√≥n de **Data Pura Vida** no solo debe enfocarse en la funcionalidad, sino que debe tener la privacidad y seguridad integradas desde el dise√±o. El cumplimiento de la Ley 8968 es mandatorio para operar en Costa Rica. La incorporaci√≥n de principios del GDPR y ISO 27001 garantizar√° un nivel de protecci√≥n de datos de clase mundial y facilitar√° la confianza, mientras que las directrices de la OCDE proporcionar√°n la base para una gobernanza de datos efectiva y una promoci√≥n responsable del intercambio y uso de la informaci√≥n.

### 5. Checklist para el Equipo de Desarrollo de "Data Pura Vida"

Este checklist tiene como objetivo presentar los requisitos legales y de seguridad de **Data Pura Vida** en acciones concretas para el equipo de desarrollo, asegurando el cumplimiento con la Ley 8968, GDPR, ISO/IEC 27001 y los principios de la OCDE.

#### Datalake

##### Cifrado de Datos:

- [ ] Implementar cifrado en reposo para todos los datos sensibles en el Datalake.
- [ ] Implementar cifrado en tr√°nsito para todas las comunicaciones hacia y desde el Datalake.
- [ ] Asegurar que los campos espec√≠ficos marcados como sensibles puedan ser cifrados a nivel de campo.

##### Control de Acceso:

- [ ] Configurar RBAC (Role-Based Access Control) para todos los usuarios y servicios que interact√∫an con el Datalake, otorgando el m√≠nimo privilegio necesario.
- [ ] Implementar RLS (Row-Level Security) para asegurar que los usuarios solo puedan ver las filas de datos a las que tienen autorizaci√≥n expl√≠cita.
- [ ] Asegurar que ning√∫n ingeniero o personal t√©cnico pueda acceder a los datos en claro sin autorizaci√≥n.

##### Calidad y Gobernanza de Datos:

- [ ] Implementar mecanismos de validaci√≥n de datos en el punto de entrada para asegurar la calidad y exactitud.
- [ ] Desarrollar y aplicar algoritmos de IA para normalizaci√≥n, relaci√≥n de datos y detecci√≥n de duplicidades.

##### Auditor√≠a y Trazabilidad:

- [ ] Implementar auditor√≠a detallada de todas las operaciones de CRUD (Crear, Leer, Actualizar, Borrar) en el Datalake, registrando usuario, acci√≥n, fecha, hora y efecto.
- [ ] Mantener un historial de consumo de datos por parte de los usuarios y servicios.

#### Backend API

##### Seguridad de la API:

- [ ] Proteger la API con whitelist de IPs (si aplica, para IPs institucionales o de Costa Rica).
- [ ] Implementar un robusto sistema de validaci√≥n de tokens (ej. JWT) para todas las solicitudes.
- [ ] Exigir Multi-Factor Authentication (MFA) para el acceso a la API para usuarios administrativos o con privilegios elevados.

##### Gesti√≥n de Credenciales y Criptograf√≠a:

- [ ] Desarrollar m√≥dulos separados para la gesti√≥n de credenciales, firmas y cifrado de datos.
- [ ] Implementar el sistema de llave tripartita para la protecci√≥n de identidades y datos asociados.

##### Registro y Monitoreo:

- [ ] Asegurar la trazabilidad y registro de cada transacci√≥n que pase por la API.
- [ ] Implementar monitoreo continuo de la API para detectar actividades an√≥malas o intentos de acceso no autorizado.

##### Transferencia de Datos:

- [ ] Si hay transferencia de datos fuera de Costa Rica, asegurar que se cumplen las garant√≠as de seguridad.

#### Interfaz de Usuario (UI) - Bio Registro Verde

##### Consentimiento Informado (Ley 8968 Art√≠culo. 5, GDPR Art√≠culos. 6 y 7):

- [ ] Dise√±ar una secci√≥n clara y destacada en el registro para informar sobre:

  - La existencia de "Data Pura Vida" y su finalidad.
  - Los destinatarios de los datos.
  - La obligatoriedad de ciertos datos y sus consecuencias.
  - Los derechos ARCO y c√≥mo ejercerlos.

- [ ] Implementar un checkbox expl√≠cito de "Acepto los T√©rminos y Condiciones y la Pol√≠tica de Privacidad" que el usuario debe marcar activamente.
- [ ] Almacenar de forma segura la documentaci√≥n del consentimiento vinculada al registro del usuario.

##### Autenticaci√≥n y Validaci√≥n:

- [ ] Integrar identidad digital, biometr√≠a o prueba de vida en el proceso de autenticaci√≥n inicial.
- [ ] Implementar MFA para el acceso de los usuarios a sus cuentas.
- [ ] Integrar validaci√≥n documental automatizada por IA para verificar la completitud y validez de documentos (ej. c√©dulas, etc.).

##### Derechos ARCO (Acceso, Rectificaci√≥n, Cancelaci√≥n y Oposici√≥n) (Ley 8968 Art√≠culo. 7, GDPR Art√≠culos. 15-21):

- [ ] Proporcionar un mecanismo claro y accesible en el perfil del usuario para:

  - Acceder a sus datos personales.
  - Rectificar datos inexactos.
  - Solicitar la eliminaci√≥n de datos (Derecho al Olvido), con la l√≥gica de negocio asociada.

- [ ] Si aplica, ofrecer opciones para limitar el tratamiento y ejercer la portabilidad de datos.
- [ ] Considerar el derecho a oponerse a decisiones basadas √∫nicamente en procesamiento automatizado si la IA afecta decisiones legales significativas sobre el usuario.

##### Privacidad de Datos (Ley 8968 Art√≠culo. 6, GDPR Art√≠culo. 25):

- [ ] Asegurar que sistema integre la privacidad desde el inicio (ej. el cifrado de datos, el control granular de acceso, la minimizaci√≥n de datos por defecto).

#### Interfaz de Usuario (UI) - Feliz Compartiendo Datos

##### Autodeterminaci√≥n Informativa (Ley 8968 Art√≠culo. 4):

- [ ] Desarrollar funcionalidades para que el usuario pueda:
      Decidir qu√© datasets compartir.

  - Configurar la visibilidad del dataset (p√∫blico/privado).
  - Definir el modelo de acceso (gratuito/pagado).
  - Establecer control granular sobre el acceso por instituci√≥n, persona o grupo de actores.

- [ ] Permitir la selecci√≥n de campos espec√≠ficos a cifrar dentro del dataset compartido.

- [ ] Habilitar la capacidad de restringir el acceso a datos por l√≠mites de tiempo, volumen o frecuencia de consulta.

##### Interoperabilidad (Principios OCDE):

- [ ] Soportar m√∫ltiples formatos de carga y mecanismos de conexi√≥n para datasets.

#### Interfaz de Usuario (UI) - Descubriendo Costa Rica

##### Seguridad en Visualizaci√≥n (Ley 8968 Art√≠culo. 10):

- [ ] Bloquear la descarga directa de datos desde los dashboards o visualizaciones.
- [ ] Impedir la exportaci√≥n de gr√°ficos y contenidos a formatos externos.
- [ ] Asegurar que la visualizaci√≥n de datos solo sea posible dentro del entorno seguro del portal.

##### Control Granular y Personalizaci√≥n:

- [ ] Permitir la construcci√≥n de dashboards personalizables por los usuarios.
- [ ] Asegurar que la visibilidad granular aplicada en "Feliz Compartiendo Datos" se refleje correctamente en las visualizaciones.

#### Seguridad General y Operaciones

##### Pol√≠ticas y Procedimientos (ISO 27001 A.5, A.6, A.8):

- [ ] Colaborar con el equipo de PM/Seguridad para la implementaci√≥n de las pol√≠ticas de seguridad de la informaci√≥n.
- [ ] Asegurar que el personal de desarrollo (ingenieros, backoffice) cumpla con los controles de seguridad antes, durante y despu√©s del empleo.

##### Controles de Acceso L√≥gico (ISO 27001 A.11):

- [ ] Restringir el acceso al portal solo desde direcciones IP ubicadas en Costa Rica o a trav√©s de listas blancas de IPs institucionales.

##### Monitoreo y Gesti√≥n de Incidentes (ISO 27001 A.10.10, A.13, GDPR Art√≠culos. 33 y 34):

- [ ] Implementar monitoreo de sistemas y gesti√≥n de logs para todas las plataformas.
- [ ] Desarrollar un proceso claro y automatizado para la detecci√≥n, reporte y respuesta a incidentes de seguridad.
- [ ] Preparar la capacidad t√©cnica para notificar brechas de seguridad a la autoridad de control (PRODHAB, DPA de la UE) y a los interesados dentro de los plazos establecidos (ej. 72 horas para GDPR).

#### Cifrado General (ISO 27001 A.12.3):

- [ ] Asegurar el uso de cifrado para todos los datos en reposo y en tr√°nsito a trav√©s de la plataforma.

##### Pruebas de Seguridad:

- [ ] Realizar pruebas de penetraci√≥n y escaneos de vulnerabilidades de forma regular.
- [ ] Incluir pruebas de seguridad en el ciclo de vida de desarrollo de software.

##### Continuidad del Negocio (ISO 27001 A.14.1):

- [ ] Implementar planes de respaldo y recuperaci√≥n para todos los componentes cr√≠ticos del sistema.

#### Gobernanza de Datos y Cumplimiento

##### Auditor√≠a Interna y Externa:

- [ ] Estar preparado para auditor√≠as internas y externas para demostrar el cumplimiento con la Ley 8968, GDPR e ISO 27001.
- [ ] Asegurar la disponibilidad de evidencias (logs, configuraciones, pol√≠ticas) para procesos legales o regulatorios.

##### Documentaci√≥n:

- [ ] Mantener una documentaci√≥n actualizada de la arquitectura de seguridad, controles implementados y flujos de datos.

## 2.2 Pr√°cticas de Manejo de C√≥digo

Para garantizar que el c√≥digo fuente de Data Pura Vida sea seguro, mantenible y escalable, se adoptan tres marcos principales de buenas pr√°cticas:

### 1. OWASP Secure Coding Practices

Basados en los est√°ndares de OWASP Top 10 y OWASP ASVS:

- Validaci√≥n de entradas:

  - Se realiza con el objetivo de que solo los datos parametrizados correctamente entren al workflow de Data Pura Vida.
  - Se evita que malformaciones de datos entren en las bases de datos y realicen un efecto en cadena de malfuncionamientos.

- Control de acceso y privilegios m√≠nimos:

  - El objetivo es asegurar que cada usuario solo tenga acceso a los recursos estrictamente necesarios para cumplir su funci√≥n.
  - Se evitar√° la acumulaci√≥n de privilegios innecesarios a lo largo del tiempo mediante revisiones peri√≥dicas.
  - Todas las solicitudes ser√°n validadas en el backend, independientemente de su origen.
  - En lugar de depender exclusivamente de roles (RBAC) tambi√©n se hara el uso de ABAC (Attribute-Based Access Control).

- Almacenamiento y transmisi√≥n segura:
  - Todos los datos sensibles ser√°n cifrados en tr√°nsito mediante TLS 1.3 y en reposo mediante AES-256.
  - **TLS 1.3 (Transport Layer Security):** protocolo criptogr√°fico utilizado para proteger los datos en tr√°nsito entre el cliente y el servidor. Asegura la confidencialidad e integridad de la informaci√≥n, evitando ataques de MITM(Man in the middle). - **AES-256 (Advanced Encryption Standard):** algoritmo de cifrado sim√©trico que se usa para proteger los datos almacenados en el sistema. Utiliza una clave de 256 bits, lo que lo hace extremadamente resistente a ataques de fuerza bruta. Es uno de los est√°ndares m√°s seguros y reconocidos a nivel mundial.
    - El sistema implementar√° gesti√≥n segura de llaves y almacenamiento segmentado para evitar accesos no autorizados, incluso desde el personal t√©cnico.
- Manejo seguro de errores:

  - Los mensajes de error visibles para el usuario ser√°n gen√©ricos, evitando revelar informaci√≥n del sistema.
  - Los errores ser√°n registrados en logs internos enmascarados, permitiendo an√°lisis posterior sin comprometer datos sensibles.

- Protecci√≥n ante dependencias vulnerables:

  - Se integrar√° un escaneo automatizado de dependencias y librer√≠as de terceros en el pipeline de CI/CD.
  - Las versiones ser√°n actualizadas de forma peri√≥dica y se restringir√° el uso de paquetes sin mantenimiento o con vulnerabilidades conocidas.

- Autenticaci√≥n robusta:
  - El sistema implementar√° OAuth2 para autorizaci√≥n y JWT como mecanismo de sesi√≥n segura.
    - **OAuth2 (Open Authorization 2.0):** es un protocolo de autorizaci√≥n que permite a una aplicaci√≥n acceder a recursos en nombre del usuario sin necesidad de compartir sus credenciales.
    - **JWT (JSON Web Token):** es un formato compacto y seguro para transmitir informaci√≥n entre partes como un objeto JSON firmado. Se usa para manejar sesiones de forma segura, ya que contiene los datos del usuario y sus permisos codificados y firmados digitalmente, lo que permite validar su autenticidad sin necesidad de consultar una base de datos en cada petici√≥n.
  - Adem√°s, toda cuenta que acceda a √°reas cr√≠ticas deber√° utilizar autenticaci√≥n multifactor (MFA), y se exigir√° prueba de vida y biometr√≠a en el registro de representantes legales.

### 2. Clean code

El proyecto aplicar√° los principios fundamentales de Clean Code propuestos por Robert C. Martin y enriquecidos con buenas pr√°cticas reconocidas de la industria, con el objetivo de asegurar un c√≥digo legible, mantenible y confiable.

**Principios clave aplicados:**

- **Funciones peque√±as y espec√≠ficas:** Cada funci√≥n debe realizar una sola tarea de forma clara.

- **Nombres claros y sem√°nticos:** Variables, funciones y clases deben ser autodescriptivas.

- **Evitar c√≥digo innecesario:** Se eliminar√° c√≥digo muerto o comentarios obvios.

- **Regla del Boy Scout:** Dejar el c√≥digo m√°s limpio de como se encontr√≥.

- **Manejo expl√≠cito de errores:** Nunca se deben silenciar excepciones.

- **Evitar condicionales complejos:** Preferir polimorfismo sobre if o switch.

- **Separaci√≥n vertical:** Agrupar c√≥digo relacionado en bloques visuales cercanos.

- **Ley de Demeter:** Cada clase solo debe conocer sus dependencias directas.

**Implementaci√≥n concreta en el proyecto:**

- **Uso de linters y formatters:** Se utilizaran para validar estilo de c√≥digo en cada lenguaje usado (ESLint, SonarQube, entre otros).

- **Refactorizaci√≥n continua:** Esta pr√°ctica ser√° integrada como una actividad habitual dentro del flujo de trabajo del equipo.

- **Validaciones reutilizables:** Se implementar√°n middlewares espec√≠ficos para validar tokens, estructura de datos y permisos, evitando duplicaci√≥n de l√≥gica (principio DRY: Don‚Äôt Repeat Yourself).

- **Inyecci√≥n de dependencias:** Los servicios cr√≠ticos como cifrado, autenticaci√≥n y conexi√≥n a base de datos se gestionar√°n por inyecci√≥n de dependencias, lo que facilita el testing y desacopla las capas del sistema.

- **Controladores REST claros:** Los endpoints de la API seguir√°n convenciones sem√°nticas (`GET /datasets`, `POST /register`, etc.) y estar√°n separados en archivos por entidad, facilitando su lectura y mantenimiento.

- **Nombres de funciones como verbos y de clases como sustantivos:**
  Por ejemplo: `validateInput()`, `encryptDataset()`, `DatasetUploader`, `IbanVerifier`.

- **Uso de constantes centralizadas:** Valores como `MAX_UPLOAD_SIZE_MB`, `SUPPORTED_FORMATS`, `DEFAULT_LANGUAGE` estar√°n definidos en archivos de configuraci√≥n o constantes globales.

- **Errores bien definidos y gestionados:** Todos los errores tendr√°n estructuras estandarizadas `({code, message, details})` y ser√°n manejados con `try/catch` en backend, registrando trazas seguras en logs sin exponer detalles internos al usuario.

Estas pr√°cticas no solo mejoran la calidad del software, sino que tambi√©n reducen los costos de mantenimiento, facilitan las auditor√≠as de seguridad y mejoran la experiencia del equipo de desarrollo durante todo el ciclo de vida del sistema.

### 3. The Twelve-Factor App

El desarrollo de Data Pura Vida tambi√©n se alinea con los principios del manifiesto Twelve-Factor App, con el objetivo de garantizar una arquitectura moderna, portable, escalable y apta para despliegues en la nube o entornos h√≠bridos. A continuaci√≥n se detallan los factores aplicados:

1. **C√≥digo base:** El c√≥digo est√° centralizado en un √∫nico repositorio de GitHub con control de versiones. Cada microservicio o m√≥dulo mantiene su propio namespace l√≥gico.

2. **Dependencias expl√≠citas:** Se utiliza un package manager en cada stack (como npm, pip, o cargo) y un archivo de lock (package-lock.json, Pipfile.lock, etc.) para gestionar y auditar todas las dependencias.

3. **Configuraci√≥n separada:** Variables sensibles como claves de API, cadenas de conexi√≥n y llaves criptogr√°ficas se mantienen fuera del c√≥digo, gestionadas mediante variables de entorno y servicios seguros de secretos (como AWS Secrets Manager o .env en desarrollo).

4. **Servicios externos como recursos:** Todas las bases de datos, colas, y almacenamiento externo se consideran recursos gestionados a trav√©s de URLs o credenciales inyectadas din√°micamente, lo que permite su intercambio sin afectar el c√≥digo.

5. **Construcci√≥n, ejecuci√≥n y publicaci√≥n desacopladas:** Se definen pipelines de CI/CD que separan claramente la fase de build (npm run build), la fase de ejecuci√≥n (node app.js) y el entorno de despliegue (Docker/OCI).

6. **Procesos sin estado:** El backend es stateless. Toda la sesi√≥n del usuario se gestiona mediante JWT y los archivos temporales se almacenan en servicios externos como S3, no en disco local.

7. **Vinculaci√≥n de puertos:** Cada componente expone sus servicios por medio de puertos est√°ndar (PORT=3000) facilitando integraci√≥n y despliegue en contenedores.

8. **Concurrencia:** Se utilizan workers y multiprocesamiento para escalar horizontalmente los m√≥dulos de procesamiento intensivo, como el motor de validaci√≥n ETDL y el generador de dashboards.

9. **Descarte r√°pido:** Las apps manejan se√±ales del sistema operativo (SIGTERM, SIGINT) y liberan recursos como conexiones o archivos abiertos. Esto permite ciclos de despliegue seguros y r√°pidos.

10. **Entornos de desarrollo y producci√≥n iguales:** Se utilizan contenedores (Docker) para asegurar que el c√≥digo se ejecute de forma id√©ntica en desarrollo, staging y producci√≥n.

11. **Logs como streams:** Los logs no se almacenan en archivos, sino que se emiten a stdout/stderr y se redirigen a herramientas de agregaci√≥n como Elastic Stack o Grafana Loki.

12. **Procesos administrativos como tareas one-off:** Scripts de migraci√≥n, pruebas y carga inicial de datos se ejecutan como procesos independientes (npm run seed, python manage.py migrate), no embebidos en el ciclo de vida principal de la app.

### Buenas Pr√°cticas Complementarias de Codificaci√≥n Segura

| Objetivo                      | Pr√°ctica                                       | Aplicaci√≥n                                                                       |
| ----------------------------- | ---------------------------------------------- | -------------------------------------------------------------------------------- |
| **Visibilidad y detecci√≥n**   | Logs + monitoreo en tiempo real                | Uso de Prometheus y Alertmanager para monitoreo                                  |
| **Seguridad en dependencias** | Escaneo continuo y alertas autom√°ticas         | GitHub Dependabot activado                                                       |
| **Gesti√≥n de secretos**       | Manejo seguro de claves, tokens y credenciales | Uso de servicios como AWS Secrets Manager o archivos .env con acceso restringido |
| **Protecci√≥n de endpoints**   | CORS y rate-limiting                           | Configuraci√≥n estricta de origen cruzado (CORS) y l√≠mites de solicitudes por IP  |

### Validaci√≥n automatizada de c√≥digo

Con el fin de reforzar las pr√°cticas de codificaci√≥n segura, mantenible y coherente, el proyecto Data Pura Vida adoptar√° una estrategia de validaci√≥n automatizada. Esta estrategia abarcar√° tanto el estilo y la calidad del c√≥digo como la seguridad y el cumplimiento de reglas internas de desarrollo.

**1. Linter personalizado y estilizaci√≥n de c√≥digo:**

- Se usar√° `ESLint` como base para c√≥digo en Node.js, incorporando lo siguiente:

  - Principios de Clean Code.

  - Recomendaciones OWASP para codificaci√≥n segura.

  - Reglas adicionales generadas con ayuda de una IA especializada en refactorizaci√≥n.

- Adem√°s, se incluir√° `Prettier` para formateo del c√≥digo. Las reglas podr√°n ser vistas en archivos con los siguientes formatos:

  - `/.config/lint/eslintrc.js`

  - `/docs/estandares-codigo.md`

- A continuaci√≥n, un ejemplo de un archivo `.eslintrc.js` que funciona para un proyecto Node.js:

```js
module.exports = {
  root: true,
  env: {
    node: true,
    es2021: true,
  },
  extends: [
    "eslint:recommended",
    "plugin:security/recommended", // OWASP rules
    "airbnb-base", // Airbnb style guide
    "plugin:prettier/recommended",
  ],
  plugins: ["security", "prettier"],
  parserOptions: {
    ecmaVersion: 12,
    sourceType: "module",
  },
  rules: {
    // --- Clean Code ---
    "no-unused-vars": ["warn", { argsIgnorePattern: "^_" }],
    "no-console": "warn",
    "prefer-const": "error",
    "no-magic-numbers": [
      "warn",
      { ignore: [0, 1, -1], enforceConst: true, detectObjects: false },
    ],
    "consistent-return": "error",
    "no-else-return": "error",

    // --- Seguridad (OWASP & buenas pr√°cticas) ---
    "security/detect-object-injection": "warn",
    "security/detect-non-literal-fs-filename": "warn",
    "security/detect-eval-with-expression": "error",
    "security/detect-buffer-noassert": "error",

    // --- Legibilidad y mantenibilidad ---
    "max-lines": [
      "warn",
      { max: 300, skipBlankLines: true, skipComments: true },
    ],
    "max-depth": ["warn", 4],
    complexity: ["warn", 10],
    camelcase: "error",
    "newline-before-return": "warn",

    // --- Estilo general (Airbnb + Prettier) ---
    "prettier/prettier": [
      "error",
      {
        printWidth: 100,
        semi: true,
        singleQuote: true,
        trailingComma: "es5",
        arrowParens: "always",
      },
    ],

    // --- Reglas adaptadas al proyecto Data Pura Vida ---
    "no-param-reassign": ["error", { props: true }],
    "import/order": [
      "warn",
      {
        groups: [
          ["builtin", "external"],
          "internal",
          "parent",
          "sibling",
          "index",
        ],
        "newlines-between": "always",
      },
    ],
  },
  overrides: [
    {
      files: ["*.test.js", "*.spec.js"],
      env: {
        jest: true,
      },
    },
  ],
};
```

**2. An√°lisis de calidad y seguridad del c√≥digo:**

- Se integrar√° `SonarQube` para an√°lisis est√°tico continuo del c√≥digo.
- Su uso ser√° para detectar usos inseguros de variables de entorno, errores de autenticaci√≥n, entre otros.

**3. Validaci√≥n de dependencias:**

- Se har√° uso de `Dependabot` para revisar librer√≠as vulnerables.
- En estas validaciones se revisan versiones desactualizadas o inseguras de paquetes, licencias incompatibles con el proyecto, paquetes abandonados, entre otros.

## 2.3 Sistema de Versionamiento

Para el versionamiento de los distintos componentes de Data Pura Vida manejaremos un solo repositorio en GitHub, utilizando un enfoque inspirado en Git Flow, adaptado para flujos modernos con automatizaci√≥n CI/CD, de la siguiente forma:

- main: rama estable lista para production.

- dev: rama de integraci√≥n en ella se corren los tests

- feature/\*: son ramas ef√≠meras en las que se desarrolla una caracter√≠stica en espec√≠fico

- fix/\*: son ramas ef√≠meras en las que se solventan hotfixes o bugs.

Todo cambio realizado en las ramas de feature y hotfix, una vez est√©n listos, se deben fusionar a la rama dev, donde se ejecutar√°n las pruebas correspondientes. Luego, cuando todo est√© aprobado, se har√°n merge a la rama main para que se realice el despliegue a producci√≥n.

### Versionado

Se seguir√° un esquema de versionado sem√°ntico usando la notaci√≥n MAJOR.MINOR.PATCH, por ejemplo: 2.3.1. Esto permitir√° comunicar de forma clara el tipo de cambios introducidos:

MAJOR: Se incrementa cuando hay cambios incompatibles con versiones anteriores.

MINOR: Se incrementa al agregar funcionalidades nuevas que mantienen compatibilidad.

PATCH: Se incrementa al aplicar correcciones de errores menores o mejoras no disruptivas.

Ejemplos:

Cambiar la estructura del modelo de datos ‚Üí 2.0.0

Agregar una nueva funcionalidad al generador de dashboards ‚Üí 2.1.0

Corregir un bug en la visualizaci√≥n de gr√°ficos ‚Üí 2.1.1

### Estructura del repositorio

A continuaci√≥n, esta ser√° la estructura del repositorio:

```bash
/data-pura-vida/
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ bioregistro-verde/
‚îÇ   ‚îú‚îÄ‚îÄ la-boveda/
‚îÇ   ‚îú‚îÄ‚îÄ ingestor/
‚îÇ   ‚îú‚îÄ‚îÄ motor-de-transformacion/
‚îÇ   ‚îú‚îÄ‚îÄ centro-de-visualizacion-y-consumo/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generador-de-dashboards/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ consumo-para-ia/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualizaci√≥n-consumo/
‚îÇ   ‚îú‚îÄ‚îÄ marketplace/
‚îÇ   ‚îî‚îÄ‚îÄ backoffice/
‚îú‚îÄ‚îÄ shared/
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ auth/
‚îú‚îÄ‚îÄ infra/
‚îÇ   ‚îî‚îÄ‚îÄ terraform/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îî‚îÄ‚îÄ docker-compose.yml
```

- En la carpeta de components estar√°n albergados todos los componentes del sistema, junto con sus subcomponentes.

- El el directorio de shared se encontrar√°n librerias y herramientas comunes a todos los componentes.

- En terraform/ estar√° la estructura para el despliegue en AWS, todo cambio al app para poder verse reflejado en el cloud provider debe pasar por ac√°.

ejemplos de archivos en terraform son:

```hcl

# provider.tf, para poner la metada del cloud provider
provider "aws" {
  region = "us-east-1"
}

# s3.tf, para crear un bucket de S3
resource "aws_s3_bucket" "react_app_bucket" {
  bucket = "mi-bucket-react-app-unique-1234"
  acl    = "public-read"
}

# eks.tf, para crear un cluster de EKS
module "eks" {
  source          = "terraform-aws-modules/eks/aws"
  cluster_name    = "mi-cluster-eks"
  cluster_version = "1.27"

  subnets         = ["subnet-12345", "subnet-67890"] # las subnets donde va el cluster
  vpc_id          = "vpc-abcde123"

  node_groups = {
    default = {
      desired_capacity = 2
      max_capacity     = 3
      min_capacity     = 1

      instance_type = "t3.medium"
    }
  }
}

```

- En .github/ estar√°n ubicados los pipelines de GitHub Actions. Definir√° las reglas de despliegue del app, por ejmplo, cuando se haga un push a main de cierto componente, se encargar√° de prepararlo y hacer su deploy al cloud provider. A continuaci√≥n un ejemplo de un pipeline que monta un microservicio en EKS:

```yaml
name: Deploy-Microservice

on:
  push:
    branches:
      - main

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up AWS CLI
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Terraform Init and Apply
        working-directory: ./terraform/bioregistro-verde
        run: |
          terraform init
          terraform apply -auto-approve

      - name: Build Docker image
        run: |
          docker build -t ${{ secrets.ECR_REPO_URI }}/bioregistro-verde:latest .
          aws ecr get-login-password | docker login --username AWS --password-stdin ${{ secrets.ECR_REPO_URI }}
          docker push ${{ secrets.ECR_REPO_URI }}/bioregistro-verde:latest

      - name: Deploy to EKS
        run: |
          helm upgrade --install bioregistro-verde ./charts/bioregistro-verde \
          --set image.repository=${{ secrets.ECR_REPO_URI }}/bioregistro-verde \
          --set image.tag=latest
```

## 2.4 Sistemas de Terceros

Durante el desarrollo e integraci√≥n de la plataforma Data Pura Vida, se contempla el uso de m√∫ltiples sistemas de terceros que habilitan funciones clave como autenticaci√≥n, verificaci√≥n de identidad, procesamiento inteligente y orquestaci√≥n de datos. A continuaci√≥n, se describen los principales:

### Protocolos de Autenticaci√≥n

- OAuth2: Protocolo est√°ndar utilizado para autorizaci√≥n segura entre frontend, backend y terceros que acceden a APIs protegidas.
- JWT (JSON Web Token): Para transmisi√≥n segura de credenciales y validaci√≥n de sesiones, especialmente en dashboards y servicios personalizados.
- MFA: Autenticaci√≥n multifactor implementada mediante servicios externos como Google Authenticator o Auth0, fortaleciendo el inicio de sesi√≥n y la gesti√≥n de cuentas.

### Verificaci√≥n de Identidad y Seguridad
- SumSub: Plataforma externa para verificaci√≥n de identidad (KYC), validaci√≥n documental autom√°tica y prueba de vida para personas f√≠sicas o representantes institucionales.

### Proveedor de Nube

- AWS: Plataforma seleccionada para el despliegue de componentes, incluyendo servicios de hosting, bases de datos, colas de eventos, control de accesos, API Gateway, y otros servicios espec√≠ficos como S3, Lambda, DynamoDB, etc.

### Inteligencia Artificial y Recomendaciones

- Hugging Face / GPT Recommender: Integraciones exploradas para generar recomendaciones de datasets mediante modelos preentrenados de lenguaje natural.


## 2.5 Aspectos de Calidad/SLA

Para garantizar que **Data Pura Vida** funcione exitosamente como ecosistema nacional de datos de Costa Rica, se establecen cinco aspectos de calidad fundamentales con implementaciones t√©cnicas espec√≠ficas que guiar√°n el dise√±o y operaci√≥n del sistema.

### **2.5.1 Escalabilidad**

La escalabilidad es la capacidad del sistema para manejar un crecimiento progresivo de usuarios, datos y transacciones sin que se degrade el rendimiento o la calidad del servicio.

Data Pura Vida debe comenzar con las instituciones p√∫blicas principales y crecer gradualmente hasta servir a miles de usuarios simult√°neos, incluyendo ciudadanos, empresas, organizaciones sociales y entidades gubernamentales. El sistema debe soportar desde datasets iniciales de unas pocas instituciones hasta vol√∫menes masivos de informaci√≥n nacional.

**Capacidades de crecimiento requeridas:**

- Soporte para miles de usuarios trabajando al mismo tiempo sin ralentizaci√≥n
- Almacenamiento que puede expandirse desde gigabytes hasta terabytes de informaci√≥n
- Procesamiento capaz de manejar cientos de datasets nuevos diariamente durante per√≠odos de alta actividad
- Cobertura nacional con tiempos de respuesta r√°pidos desde cualquier provincia

**Configuraciones t√©cnicas espec√≠ficas para escalabilidad:**

**Balanceador de Carga y Gateway:**
El sistema utilizar√° un balanceador de carga configurado en la infraestructura cloud de AWS con par√°metros espec√≠ficos para garantizar distribuci√≥n eficiente del tr√°fico:

- Configuraci√≥n en 2 zonas de disponibilidad con verificaciones de salud cada 60 segundos
- Capacidad de 500 solicitudes por segundo por instancia con escalado autom√°tico hasta 6 instancias
- Tiempo l√≠mite de 30 segundos para solicitudes normales, 600 segundos para cargas de datasets grandes
- Terminaci√≥n SSL con TLS 1.3 y certificados renovados cada 6 meses

**Gesti√≥n de Conexiones y Concurrencia:**
Para manejar m√∫ltiples usuarios simult√°neos, se implementar√° un sistema de agrupaci√≥n de conexiones a base de datos:

- Agrupaci√≥n con m√≠nimo 3 conexiones activas, m√°ximo 12 conexiones por servicio
- Tiempo l√≠mite de conexi√≥n de 45 segundos, tiempo l√≠mite de adquisici√≥n de 90 segundos
- Tiempo l√≠mite inactivo de 60 segundos para liberar conexiones no utilizadas
- Tiempo l√≠mite de conexi√≥n de 10 segundos para establecimiento de nuevas conexiones

**Estrategia de Cach√© Multicapa:**
El almacenamiento temporal de datos frecuentemente accedidos mejorar√° el rendimiento mediante:

- **Nivel 1 - Cach√© de Aplicaci√≥n**: Cach√© en memoria con tiempo de vida de 15 minutos
- **Nivel 2 - Cach√© Distribuido**: Tecnolog√≠a de cach√© distribuido a definir en Stack Tecnol√≥gico
- **Nivel 3 - Cach√© de Base de Datos**: Optimizaci√≥n a nivel de base de datos
- Tiempo de vida espec√≠fico por tipo de dato: usuarios (2 horas), metadatos de datasets (8 horas), validaciones (48 horas)

**Procesamiento de Datos con Inteligencia Artificial:**
Para el Motor ETDL (Extracci√≥n, Transformaci√≥n y Carga) que utiliza inteligencia artificial, se configurar√°n colas de procesamiento:

- Cola tipo FIFO (Primero en Entrar, Primero en Salir) para garantizar orden en procesamiento
- Tiempo l√≠mite de visibilidad de 30 minutos (tiempo m√°ximo de procesamiento por dataset)
- Retenci√≥n de mensajes de 7 d√≠as para reintento de fallos
- Tama√±o de lote de 5 mensajes por procesamiento para eficiencia
- M√°ximo 3 datasets proces√°ndose simult√°neamente para evitar sobrecarga

**Control de Concurrencia para Inteligencia Artificial:**
Dado que modificar el dise√±o de modelos concurrentemente es peligroso, se implementar√°n sem√°foros y procesamiento as√≠ncrono:

- Sem√°foro √∫nico para modificaciones concurrentes del modelo de datos
- Solo 1 modificaci√≥n de esquema simult√°nea permitida
- Tiempo l√≠mite de 60 minutos para operaciones de modificaci√≥n de modelo
- Cola de espera para modificaciones pendientes con prioridad FIFO

**Mecanismos de escalabilidad:**

El sistema utilizar√° escalado autom√°tico, que significa que cuando detecta mayor actividad, autom√°ticamente asigna m√°s recursos computacionales (servidores adicionales) para mantener el rendimiento. Cada componente puede crecer independientemente seg√∫n su demanda espec√≠fica, y el sistema se optimiza continuamente bas√°ndose en los patrones de uso reales de los costarricenses.

### **2.5.2 Mantenibilidad**

La mantenibilidad se refiere a la facilidad con que el sistema puede ser actualizado, corregido y mejorado a lo largo del tiempo sin interrumpir el servicio a los usuarios.

Un sistema nacional debe poder evolucionar constantemente. Las regulaciones cambian, las necesidades del pa√≠s se transforman, y la tecnolog√≠a avanza. Data Pura Vida debe adaptarse a estos cambios sin afectar su operaci√≥n diaria.

**Compromisos de mantenibilidad:**

- Resoluci√≥n de problemas cr√≠ticos en m√°ximo cuatro horas
- Implementaci√≥n de mejoras sin interrumpir el servicio a usuarios
- Capacidad de revertir cambios problem√°ticos en menos de quince minutos
- Monitoreo proactivo que detecta problemas antes de que afecten a los usuarios

**Configuraciones t√©cnicas espec√≠ficas para mantenibilidad:**

**Sistema de Monitoreo Integrado:**
La observabilidad es la capacidad de entender el estado interno del sistema bas√°ndose en los datos que produce:

- **M√©tricas**: Retenci√≥n de 15 d√≠as con resoluci√≥n de 30 segundos para m√©tricas cr√≠ticas
- **Registros**: Retenci√≥n de 30 d√≠as con compresi√≥n autom√°tica despu√©s de 3 d√≠as
- **Trazas**: Seguimiento de solicitudes entre componentes para depuraci√≥n
- **Alertas**: 25 reglas configuradas para diferentes umbrales cr√≠ticos del sistema
- **Herramientas**: Stack de monitoreo a definir en Stack Tecnol√≥gico

**Integraci√≥n Continua y Despliegue Continuo:**
La pr√°ctica de integrar c√≥digo frecuentemente y desplegarlo autom√°ticamente incluye:

- **GitHub Actions**: Pipelines automatizados seg√∫n la estructura definida en .github/workflows/
- **Terraform**: Infraestructura como c√≥digo para despliegue en AWS seg√∫n /infra/terraform/
- **Estrategia de Despliegue**: Estrategia de despliegue seguro a definir en Stack Tecnol√≥gico
- Pruebas autom√°ticas con cobertura m√≠nima del 70% antes de despliegue
- Construcci√≥n y empaquetado de aplicaciones con etiquetado por commit SHA
- Verificaci√≥n de salud autom√°tica con tiempo l√≠mite de 5 minutos antes de activaci√≥n

**Migraciones de Base de Datos:**
Los scripts que permiten evolucionar la estructura de la base de datos de forma controlada:

- Scripts de migraci√≥n versionados con nomenclatura est√°ndar
- Reversi√≥n autom√°tica si la migraci√≥n falla
- Validaci√≥n con verificaci√≥n de suma de comprobaci√≥n en cada despliegue
- Respaldo autom√°tico antes de cada migraci√≥n cr√≠tica
- **Herramienta**: Sistema de migraci√≥n a definir seg√∫n tecnolog√≠a de base de datos seleccionada

**Estrategia de Reversi√≥n:**

- Reversi√≥n autom√°tica a revisi√≥n anterior en caso de fallo
- Tiempo l√≠mite configurado a 10 minutos m√°ximo para reversi√≥n completa
- Preservaci√≥n de √∫ltimas 5 revisiones para reversi√≥n selectiva
- Scripts de reversi√≥n semiautom√°ticos para cambios de base de datos

**Estrategias de mantenimiento:**

El sistema utiliza una arquitectura modular, lo que significa que cada componente puede actualizarse independientemente sin afectar los dem√°s. Los despliegues son automatizados con validaci√≥n previa, y existe documentaci√≥n que se actualiza autom√°ticamente. El sistema incluye observabilidad completa, que es la capacidad de monitorear en tiempo real el rendimiento, errores y patrones de uso.

### **2.5.3 Reutilizaci√≥n**

La reutilizaci√≥n maximiza el aprovechamiento de cada funcionalidad desarrollada, permitiendo que sea utilizada en m√∫ltiples componentes del sistema para optimizar recursos y garantizar consistencia.

Con recursos p√∫blicos limitados, cada desarrollo debe aprovecharse al m√°ximo. Cuando se crea una funcionalidad para validar documentos costarricenses, esta debe servir para todo el sistema, no solo para una parte espec√≠fica.

**Componentes reutilizables principales:**

- Sistema de autenticaci√≥n unificado que permite un solo acceso para toda la plataforma
- Validadores espec√≠ficos para documentos costarricenses (c√©dulas, IBAN, registros tributarios)
- Biblioteca de componentes visuales que garantiza interfaces consistentes
- Herramientas de cifrado estandarizadas para protecci√≥n de datos
- APIs (interfaces de programaci√≥n) comunes para integraciones con sistemas externos

**Configuraciones t√©cnicas espec√≠ficas para reutilizaci√≥n:**

**Librer√≠as Compartidas seg√∫n estructura del repositorio:**
Bas√°ndose en el directorio /shared/ definido en el README:

- **shared/utils/**: Utilidades comunes para todos los componentes
- **shared/auth/**: Sistema de autenticaci√≥n unificado usando OAuth2 y JWT

**OAuth2 (Autorizaci√≥n Abierta 2.0) y JWT (Token Web JSON):**
OAuth2 es un protocolo de autorizaci√≥n que permite a aplicaciones obtener acceso limitado a cuentas de usuario:

- Configuraci√≥n con proveedores Google y Microsoft seg√∫n requerimientos
- Alcances: 'openid', 'profile', 'email' para informaci√≥n b√°sica del usuario
- URI de redirecci√≥n configurado seg√∫n ambiente de despliegue

JWT es un est√°ndar para transmitir informaci√≥n de forma segura entre partes como un objeto JSON firmado:

- Expiraci√≥n de 8 horas con renovaci√≥n autom√°tica si el usuario est√° activo
- Emisor espec√≠fico 'data-pura-vida' con audiencia configurada para 'web', 'mobile', 'api'
- Algoritmo de firma RS256 para validaci√≥n de integridad con rotaci√≥n de claves mensual

**Autenticaci√≥n Multifactor (MFA):**
M√©todo de autenticaci√≥n que requiere m√∫ltiples formas de verificaci√≥n de identidad:

- **TOTP (Contrase√±a de Un Solo Uso Basada en Tiempo)**: Ventana de 3 per√≠odos (90 segundos) para flexibilidad
- **Integraci√≥n con SumSub**: Seg√∫n sistemas de terceros definidos para KYC (Conoce a Tu Cliente)
- **Biometr√≠a**: Integraci√≥n para prueba de vida en representantes legales
- 8 c√≥digos de respaldo generados autom√°ticamente con posibilidad de regeneraci√≥n
- C√≥digos QR de 256x256 p√≠xeles para mejor compatibilidad con dispositivos m√≥viles

**Plantillas de Infraestructura con Terraform:**
Seg√∫n directorio /infra/terraform/:

- M√≥dulos reutilizables para cada componente en /components/
- Variables configurables para nombre, imagen, CPU y memoria
- 2 r√©plicas por defecto con escalado autom√°tico configurado hasta 6 r√©plicas
- Solicitudes de recursos de 250m CPU y 256Mi memoria por defecto
- L√≠mites de recursos de 1 CPU y 1Gi memoria m√°ximo
- Verificaciones de salud autom√°ticas en endpoint /health con tiempo l√≠mite de 10 segundos

**Beneficios de la reutilizaci√≥n:**

- Desarrollo significativamente m√°s r√°pido al aprovechar funcionalidades ya construidas y probadas
- Experiencia de usuario consistente en todos los componentes
- Mantenimiento simplificado donde un cambio se propaga autom√°ticamente
- Mejora continua de la calidad a medida que los componentes se refinan con el uso

### **2.5.4 Eficiencia**

La eficiencia busca optimizar el uso de recursos computacionales y financieros para ofrecer el mejor rendimiento posible con el menor costo operativo, utilizando responsablemente los recursos p√∫blicos.

El sistema debe proporcionar respuestas r√°pidas y una experiencia fluida mientras utiliza los recursos de manera inteligente, evitando desperdicios y optimizando costos.

**Objetivos de eficiencia:**

- Tiempos de respuesta que los usuarios perciban como instant√°neos para operaciones comunes
- Uso √≥ptimo de la capacidad de los servidores, manteniendo un balance eficiente
- Almacenamiento inteligente con compresi√≥n autom√°tica para reducir costos
- Escalado din√°mico que ajusta recursos seg√∫n la demanda real

**Configuraciones t√©cnicas espec√≠ficas para eficiencia:**

**Optimizaci√≥n de Consultas de Base de Datos:**
Optimizaci√≥n espec√≠fica para La B√≥veda, que debe almacenar datos en formato unificado independientemente del origen (relacional, documental, CSV, Excel):

- **Particionado autom√°tico**: Organizaci√≥n por fecha para datasets hist√≥ricos
- **√çndices optimizados**: Para b√∫squedas de texto completo en espa√±ol (Costa Rica)
- **Relaciones entre datasets**: √çndices para columnas que relacionan datasets del ecosistema
- **Trazabilidad completa**: √çndices para auditor√≠a de datos usados, no usados y descartados

**Estrategia de Almacenamiento AWS:**
Seg√∫n proveedor cloud definido (AWS), se utilizar√°n servicios espec√≠ficos de almacenamiento:

- **Almacenamiento activo**: Para datos activos frecuentemente accedidos
- **Almacenamiento tibio**: Para datos de acceso ocasional
- **Almacenamiento fr√≠o**: Para archivo a largo plazo
- **Almacenamiento de archivo**: Para archivo permanente
- **Pol√≠ticas de ciclo de vida**: Migraci√≥n autom√°tica seg√∫n antig√ºedad y patrones de acceso

**Estrategia de Compresi√≥n espec√≠fica para datasets:**

- **LZ4**: Compresi√≥n r√°pida para datasets recientes (< 7 d√≠as) con proporci√≥n 2:1
- **ZSTD**: Compresi√≥n alta para datasets archivados (> 30 d√≠as) con proporci√≥n 5:1
- **BZIP2**: M√°xima compresi√≥n para almacenamiento fr√≠o (> 1 a√±o) con proporci√≥n 8:1
- Migraci√≥n autom√°tica seg√∫n pol√≠ticas de ciclo de vida de AWS

**Gesti√≥n de Recursos:**
Orquestaci√≥n de contenedores seg√∫n arquitectura seleccionada:

- **N√∫cleos de CPU**: 12 en solicitudes totales, 24 n√∫cleos de CPU en l√≠mites
- **Memoria**: 24Gi en solicitudes totales, 48Gi memoria en l√≠mites
- **Escalado autom√°tico**: Configurado por componente seg√∫n tecnolog√≠a seleccionada
- **Disparadores de escalado**: CPU mayor a 75% o Memoria mayor a 85%
- **Comportamiento de escalado**: M√°ximo 1 instancia por escalado hacia arriba cada 2 minutos, m√°ximo 1 por escalado hacia abajo cada 5 minutos

**Estrategias de optimizaci√≥n:**

El sistema implementa cach√© multicapa, que mantiene los datos m√°s consultados en memoria de acceso r√°pido para respuestas inmediatas. Utiliza consultas optimizadas dise√±adas para ser eficientes incluso con millones de registros, y compresi√≥n autom√°tica que reduce el espacio de almacenamiento sin p√©rdida de informaci√≥n. Incluye balanceo de carga inteligente que distribuye las consultas entre m√∫ltiples servidores para evitar sobrecargas.

### **2.5.5 Claridad y Gesti√≥n de Complejidad**

La claridad asegura que un sistema t√©cnicamente sofisticado sea comprensible y f√°cil de usar tanto para usuarios finales como para desarrolladores que lo mantienen.

Data Pura Vida debe ocultar su complejidad t√©cnica detr√°s de interfaces simples e intuitivas. Los usuarios no deben necesitar conocimiento t√©cnico para aprovechar sus capacidades.

**Principios de claridad:**

- Interfaces consistentes con navegaci√≥n predecible y uniforme en toda la plataforma
- Mensajes comprensibles que explican claramente qu√© est√° ocurriendo, especialmente en casos de error
- Documentaci√≥n autom√°tica que se mantiene actualizada sin intervenci√≥n manual
- Configuraci√≥n organizada de forma l√≥gica y comprensible

**Configuraciones t√©cnicas espec√≠ficas para claridad:**

**Dise√±o de APIs (Interfaces de Programaci√≥n):**
API RESTful (Transferencia de Estado Representacional) es un estilo arquitect√≥nico para dise√±ar interfaces web que permite la comunicaci√≥n entre sistemas de forma sencilla:

- **Endpoints sem√°nticos**: /api/v1/datasets, /api/v1/users, /api/v1/dashboards
- **M√©todos HTTP**: GET (consultar), POST (crear), PUT (actualizar completo), PATCH (actualizar parcial), DELETE (eliminar)
- **C√≥digos de Estado**: 200 (OK), 201 (Creado), 400 (Solicitud Incorrecta), 401 (No Autorizado), 404 (No Encontrado)
- **Tipo de Contenido**: application/json para intercambio de datos
- **Versionado**: /api/v1/ como prefijo para mantener compatibilidad hacia atr√°s

**Manejo de Errores Estructurado:**

- **C√≥digo sem√°ntico**: DATASET_NOT_FOUND, INVALID_CEDULA_FORMAT, UNAUTHORIZED_ACCESS
- **Mensaje descriptivo**: En espa√±ol para usuarios finales costarricenses
- **Detalles t√©cnicos**: Campo espec√≠fico con error y valor esperado
- **ID de Correlaci√≥n**: Para seguimiento y depuraci√≥n entre componentes
- **Marca de Tiempo**: Formato ISO 8601 para consistencia internacional

**Categor√≠as de Error espec√≠ficas para Costa Rica:**

- **Errores de validaci√≥n**: Formato de c√©dula costarricense, IBAN nacional, RTN
- **Errores de autenticaci√≥n**: MFA, biometr√≠a, prueba de vida
- **Errores de autorizaci√≥n**: Permisos RBAC (Control de Acceso Basado en Roles), acceso por IP costarricense
- **Errores de l√≥gica de negocio**: Reglas espec√≠ficas del ecosistema nacional de datos

**Organizaci√≥n del C√≥digo:**
Seg√∫n estructura del repositorio definida:

```
/data-pura-vida/
‚îú‚îÄ‚îÄ components/               # Cada componente independiente
‚îÇ   ‚îú‚îÄ‚îÄ bioregistro-verde/   # M√°ximo 200 l√≠neas por archivo
‚îÇ   ‚îú‚îÄ‚îÄ la-boveda/           # Separaci√≥n clara de responsabilidades
‚îÇ   ‚îú‚îÄ‚îÄ motor-de-transformacion/ # Funciones espec√≠ficas para ETDL
‚îÇ   ‚îî‚îÄ‚îÄ centro-de-visualizacion-y-consumo/
‚îú‚îÄ‚îÄ shared/                  # C√≥digo reutilizable
‚îÇ   ‚îú‚îÄ‚îÄ utils/              # Utilidades comunes
‚îÇ   ‚îî‚îÄ‚îÄ auth/               # Autenticaci√≥n centralizada
```

**Est√°ndares de C√≥digo Limpio:**

- **Funciones**: M√°ximo 50 l√≠neas, una responsabilidad espec√≠fica
- **Clases**: Principio de responsabilidad √∫nica, m√°ximo 300 l√≠neas
- **Variables**: Nombres autodescriptivos en espa√±ol/ingl√©s seg√∫n contexto
- **Complejidad ciclom√°tica**: M√°ximo 15 por funci√≥n (moderadamente complejo)
- **Documentaci√≥n**: JSDoc para funciones p√∫blicas, comentarios en espa√±ol para l√≥gica compleja

**Gesti√≥n de Configuraci√≥n:**
Configuraci√≥n espec√≠fica por ambiente:

- **Desarrollo**: Configuraci√≥n local con bases de datos de prueba
- **Pruebas**: Ambiente de pruebas en AWS con datos simulados
- **Producci√≥n**: Ambiente productivo con datos reales costarricenses
- **Gesti√≥n de secretos**: Servicios AWS para credenciales sensibles
- **Variables de ambiente**: Configuraci√≥n espec√≠fica por ambiente sin valores fijos en c√≥digo

**Gesti√≥n de complejidad:**

El sistema utiliza separaci√≥n de responsabilidades, donde cada componente tiene una funci√≥n espec√≠fica y bien definida. Implementa abstracciones √∫tiles que ocultan la complejidad t√©cnica detr√°s de interfaces simples, y aplica patrones reconocibles con soluciones consistentes para problemas similares. Incluye escalamiento gradual que presenta funcionalidades b√°sicas primero y avanzadas despu√©s.

**Implementaci√≥n pr√°ctica:**

Las APIs (interfaces de programaci√≥n) utilizan nomenclatura sem√°nticamente clara que explica exactamente qu√© hacen. El manejo de errores es estructurado, proporcionando mensajes que incluyen qu√© ocurri√≥ y c√≥mo solucionarlo. La arquitectura se organiza en capas claras que separan presentaci√≥n, l√≥gica de negocio y datos.

### **2.5.6 M√©tricas y SLAs Espec√≠ficos**

**M√©tricas de Rendimiento alineadas con requerimientos**

**SLAs de Tiempo de Respuesta:**

- **Endpoints de API**: 95% de operaciones completadas en menos de 800ms
- **Generaci√≥n de Tableros**: Menos de 5 segundos para gr√°ficos simples
- **Carga de Datasets**: M√°ximo 30 minutos para procesamiento ETDL
- **Operaciones de B√∫squeda**: Menos de 3 segundos para b√∫squedas en espa√±ol

**Tiempos de Procesamiento ETDL espec√≠ficos:**

- **Datasets peque√±os** (menor a 1MB): m√°ximo 10 minutos de procesamiento
- **Datasets medianos** (1-10MB): m√°ximo 30 minutos
- **Datasets grandes** (10-100MB): m√°ximo 2 horas
- **Datasets extra-grandes** (mayor a 100MB): m√°ximo 8 horas con procesamiento por lotes

**SLAs de Disponibilidad para ecosistema nacional**

**Disponibilidad del Sistema:**

- **Sistema general**: 99.5% tiempo activo anual (m√°ximo 43.8 horas inactivo/a√±o)
- **Bio Registro Verde**: 99.7% tiempo activo (componente cr√≠tico para acceso)
- **La B√≥veda**: 99.6% tiempo activo (almacenamiento central de datos)
- **Centro Visualizaci√≥n**: 99.0% tiempo activo (menor criticidad, m√°s tolerante a fallas)
- **Ventana de mantenimiento**: Domingos 02:00-06:00 GMT-6 (horario costarricense)

**Planificaci√≥n de Capacidad espec√≠fico para Costa Rica**

**Proyecciones de Crecimiento basadas en adopci√≥n nacional:**

**A√±o 1 (Instituciones p√∫blicas principales):**

- 25,000 usuarios registrados (funcionarios p√∫blicos y ciudadanos early adopters)
- 5,000 datasets (datos de ministerios principales, TSE, BCCR)
- 2TB almacenamiento (documentos b√°sicos, bases de datos institucionales core)
- 5M llamadas de API mensuales

**A√±o 3 (Sector privado integrado):**

- 100,000 usuarios (empresas medianas, algunas c√°maras, organizaciones)
- 25,000 datasets (expansi√≥n gradual con datos comerciales selectos)
- 12TB almacenamiento (crecimiento org√°nico con sector privado)
- 25M llamadas de API mensuales

**A√±o 5 (Ecosistema en maduraci√≥n):**

- 250,000 usuarios (adopci√≥n amplia pero no total)
- 75,000 datasets (ecosistema robusto pero en crecimiento)
- 50TB almacenamiento (volumen significativo de datos hist√≥ricos)
- 100M llamadas de API mensuales

### **Aseguramiento de Calidad espec√≠fica para Costa Rica**

**Puertas de Calidad de C√≥digo:**

- **Cobertura m√≠nima**: 70% para c√≥digo nuevo, 75% objetivo general
- **Validaciones nacionales**: 85% cobertura para validadores de c√©dula, IBAN, RTN
- **Cumplimiento de seguridad**: Cumplimiento b√°sico Ley 8968, consideraci√≥n GDPR e ISO 27001
- **Internacionalizaci√≥n**: Soporte principal para espa√±ol costarricense

**Pruebas de Rendimiento espec√≠fico:**

- **Pruebas de carga**: 500 usuarios concurrentes (estimaci√≥n conservadora)
- **Pruebas de restricci√≥n geogr√°fica**: Validaci√≥n de acceso preferencial desde IPs costarricenses
- **Recuperaci√≥n ante desastres**: Tiempo de recuperaci√≥n m√°ximo 8 horas
- **Integridad de datos**: 98% integridad en transferencias entre componentes (tolerancia a errores menores)


# 3. Stack Tecnol√≥gico

En cada una documentar versiones de frameworks, SDKs, lenguajes y herramientas utilizadas, as√≠ como sus restricciones y licencias

## Frontend

- **React.js**: Un framework de javascript especializado en web apps
- **Vite**: Empaquetador de react.
- **Tailwind CSS**: Librer√≠a para acelerar la creaci√≥n de estilos mediante utilidades predefinidas.
- **Axios**: Libreria de javascript que permite hacer llamadas a rest APIs.
- **Formik + Yup**: Dos librer√≠as de Javascript que har√°n la escritura de formularios m√°s simple. Formik para la estructura de formularios, Yup para validaci√≥n
- **Cognito**: Servicio de AWS que ser√° usado para el registro de personas.
- **Plotly**: Librer√≠a para gr√°ficos interactivos y avanzados con soporte para fuentes din√°micas y control total. Presione [aqu√≠](https://www.chartjs.org/docs/latest/samples/information.html) para ver los gr√°ficos que ofrecen.
- **AWS S3:** Servicio de almacenamiento escalable donde se alojan los archivos est√°ticos de la aplicaci√≥n React (HTML, CSS, JS, im√°genes, etc.).
- **AWS Cloudfront:** Red de distribuci√≥n de contenido (CDN) que entrega los archivos desde S3 con baja latencia y alta velocidad, mejorando el rendimiento y la disponibilidad global.

## Backend

- **Python**: Lenguaje de programaci√≥n versatil, con variedad de librer√≠as y frameworks especializados en ETL e IA.
- **FastAPI**: Framework as√≠ncrono en Python ideal para construir APIs r√°pidas y escalables.
- **RabbitMQ**: Broker de mensajer√≠a para comunicaci√≥n as√≠ncrona entre m√≥dulos backend.
- **EKS**: Servicio de Kubernetes gestionado por AWS para despliegue escalable y seguro del backend.
- **Apache Spark**: Framework especializado en procesamiento distribuido para ETL, validaci√≥n y transformaci√≥n de datos (usando PySpark).
- **Apache Airflow**:Orquestador de workflows para automatizar y monitorear procesos ETL con Spark, asegurando orden, trazabilidad y escalabilidad.
- **Helm**: Herramienta para gestionar despliegues Kubernetes mediante plantillas din√°micas.
- **Docker**: Ser√° usado para crear im√°genes de los distintos m√≥dulos del backend.

## Data

- **PostgreSQL:** Almacenamiento relacional de datos estructurados, ideal para usuarios y clientes.
- **DynamoDB:** Base de datos NoSQL para gestionar metadatos din√°micos y de alto rendimiento.
- **Redis:** Base de datos Clave Valor que sirve para caching o guardar informaci√≥n con TTL.
- **AWS S3:** Almacenamiento de objetos escalable y seguro para grandes vol√∫menes de datos no estructurados, como archivos.
- **Opensearch**: Elasticsearch gestionado por AWS que permite tener bases de datos time series altamente escalables.
- **AWS Glue:** Servicio ETL gestionado para la transformaci√≥n y preparaci√≥n de datos en flujos automatizados.\*tentativo, puede que prefiramos implementar nuestro propio cluster de spark en EKS organizado con airflow.
- **AWS RDS**: Es el servicio que AMazon ofrece para poder albergar bases de datos de PostgreSQL o MYSQL
- **AWS SageMaker:** Plataforma integral para crear, entrenar y desplegar modelos de machine learning de forma segura y escalable.
- **AWS KMS (Key Management Service):** Servicio de administraci√≥n de claves criptogr√°ficas para cifrar y proteger datos sensibles en todos los servicios de AWS.

## AI

- **Hugging Face Transformers:** Uso de modelos preentrenados (ej. all-mpnet-base-v2) para generar embeddings sem√°nticos de texto.
- **LangChain:** Orquestaci√≥n de agentes inteligentes y manejo de flujos de lenguaje natural.
- **OpenAI (GPT-4):** Procesamiento de lenguaje natural, generaci√≥n de texto y clasificaci√≥n sem√°ntica.
- **Amazon SageMaker**: Entrenamiento, ajuste fino y despliegue de modelos personalizados de machine learning.
- **Hugging face**: para modelos ya entrenados que nos puedan servir (all-mpnet-base-v2 genera embeddings que podr√≠a servir para entrenar IA)

## Sistemas de Terceros

- **SumSub:** Sistema para poder realizar las comprobaciones KYC, AML y sdk para realizar pruebas de vida.
- **AWS:** Ser√° nuestro cloud provider, y usaremos distintos servicios como S3, Glue, Cognito, etc.
- **Stripe:** Sistema que permite manejar los pagos dentro de nuestro sitio web.
- **Hugging Face:** Fuente para usar m√≥delos de IA ya entrenados.

## Cloud

### **Proveedor Principal**

- **Amazon Web Services (AWS)**: Plataforma de computaci√≥n en la nube para toda la infraestructura de Data Pura Vida.

### Servicios de Computaci√≥n

- **Amazon EKS:** Kubernetes gestionado para contenedores del backend
- **AWS Lambda:** Funciones serverless para procesos espec√≠ficos

### **Servicios de Red**

- **AWS Application Load Balancer:** Balanceador de carga
- **Amazon CloudFront:** CDN para contenido est√°tico
- **AWS VPC:** Red privada virtual para aislar recursos

### **Servicios de Gesti√≥n**

- **AWS IAM:** Gesti√≥n de identidades y permisos
- **AWS CloudWatch:** Monitoreo y m√©tricas (ya definido en DevOps)
- **AWS CloudTrail:** Auditor√≠a de acciones

### **Servicios Adiocionales**

- **Amazon SES:** Una opci√≥n gestionada por AWS para poder hacer env√≠o de correos electr√≥nico.

## DevOps y Testing

### Infraestructura como C√≥digo (IaC)

- **AWS CloudFormation:** plantilla oficial de AWS para definir infraestructura como c√≥digo.

- **Terraform:** herramienta para definir y aprovisionar la infraestructura en AWS mediante archivos .tf, asegurando consistencia entre ambientes y facilitando el versionamiento y rollback de cambios.

- **Helm:** gestor de paquetes para Kubernetes que permite definir despliegues mediante chart templates reutilizables, simplificando el despliegue de servicios backend.

### Integraci√≥n y Despliegue Continuo (CI/CD)

- **Github:** Para guardar codigo y control de versiones.

- **AWS CodePipeline:** herramienta nativa de AWS para construir pipelines de integraci√≥n y despliegue continuo.

- **GitHub Actions:** seguir√° siendo utilizado como integrador externo, especialmente para validar PRs, ejecutar linters, y disparar eventos hacia CodePipeline mediante webhooks.

### Observabilidad y Monitoreo

- **AWS CloudWatch:** permite monitorear y supervisar toda la infraestructura desplegada en AWS, como RDS, DynamoDB y S3. Dado que todo el alojamiento en la nube se realizar√° en AWS, no es necesario utilizar otras herramientas externas como DataDog o Prometheus.

- **Grafana + CloudWatch + Prometheus:** para dashboards visuales personalizados directamente desde CloudWatch Metrics para los servicios de AWS, y Prometheus para los microservicios en EKS.

### Pruebas Automatizadas

- **Pytest:** framework de pruebas para Python usado en pruebas unitarias para el backend.

- **Jest :** para pruebas unitarias de componentes React en el frontend.

- **Gatling:** para hacer pruebas de carga en la aplicaci√≥n antes de poder pasarla a producci√≥n.

- **Postman + Newman:** se usar√°n para pruebas manuales y autom√°ticas de la API REST. Newman permite integrar las colecciones en el CI.

### Validaci√≥n de C√≥digo y Estilo

- **ESLint:** verificaci√≥n autom√°tica de estilo y seguridad en el frontend, con reglas personalizadas ancladas en el repositorio (.eslintrc.js).

- **Amazon CodeGuru Reviewer:** analiza c√≥digo Python, detectando problemas de rendimiento y vulnerabilidades usando machine learning.

- **SonarQube:** se usar√° para realizar an√°lisis est√°tico del c√≥digo backend y frontend, identificando autom√°ticamente bugs, vulnerabilidades y malas pr√°cticas. Estar√° integrado al pipeline de CI/CD para bloquear pull requests con problemas cr√≠ticos y generar reportes de calidad y seguridad.

### Seguridad

- **AWS Secrets Manager:** gesti√≥n segura de claves API, credenciales y tokens con rotaci√≥n autom√°tica y control de acceso granular.

- **Dependabot:** para monitoreo de paquetes vulnerables desde GitHub. Se integra con CodePipeline para ejecutar pruebas de validaci√≥n al actualizar dependencias.

# 4. Dise√±o de los componentes

En esta secci√≥n se detallar√° el dise√±o de los componentes previamente definidos en la secci√≥n de planeamiento. A cada uno se le aplicar√° un an√°lisis de frontend, backend y datos, seg√∫n corresponda. Adem√°s, existe la posibilidad de incluir prototipos en forma de pruebas de concepto. Tambi√©n se especificar√° c√≥mo se llevar√° a cabo el proceso de pruebas e integraci√≥n, despliegue y mantenimiento.

Antes de comenzar cabe por dejar en claro algunas especificaciones generales que se ver√°n a lo largo de todo el dise√±o de los componentes:

- Todos los microservicios del backend estar√°n desplegados en un cluster de EKS.

- Se tendr√° un API general para todo el backend, para poder acceder a las funcionalidades de todos los microservicios se debde consultar a dicha API (ser√° RESTful). Adem√°s, estar√° construida en FastAPI, para favorecernos de sus caracter√≠sticas asincr√≥nicas que la hacen sumamente r√°pida y apta para manejar carga pesada. Estar√° desplegada en el cluster de EKS, como un deployment con N replicas (Antes de pasar a producci√≥n se le realizar√°n pruebas de carga con Gatling, para poder determinar exactamente cuantas replicas ocupar√°).

# 4.1. Bioregistro

Este componente es el punto de entrada al sistema, tiene como prop√≥sito registrar distintos tipos de usuarios y adaptarse din√°micamente a sus requerimientos de autenticaci√≥n.

Los tipos de usuarios que se podr√°n registrar en la plataforma son los siguientes :

- **Usuarios con C√©dula F√≠sica**: Esto incluye a cualquier persona f√≠sica que tenga c√©dula costarricense.

- **Usuarios con C√©dula Jur√≠dica**: Esta capa incluye una amplia variedad de colectivos que pueden aportar datasets de valor.
  - **Empresas privadas**: Incluye PYMES y Sociedades An√≥nimas (S.A).
  - **Empresas p√∫blicas y entes estatales**: Abarca instituciones aut√≥nomas, empresas estatales, empresas municipales, y Ministerios.
  - **C√°maras y gremios**: Incluye c√°maras empresariales y gremios profesionales o t√©cnicos.
  - **Universidades y centros acad√©micos**: Comprende universidades p√∫blicas y privadas, as√≠ como sus escuelas, facultades y centros de investigaci√≥n.

Asimismo, se adjunta una descripci√≥n de que es cada uno de los colectivos listados junto con que aporte pueden dar a Data Pura Vida,que informaci√≥n se les va a solicitar para poder garantizar que son empresas verdaderas y solicitadas por sus representantes reales (Cabe aclarar que todo documento PDF debe venir con firma digital):

### **Empresas privadas**

Organizaciones con fines de lucro que operan en sectores diversos. Se dividen principalmente en:

**PYMES (Peque√±as y Medianas Empresas)**:
Empresas de menor escala que operan en comercio, manufactura ligera, servicios digitales, turismo, entre otros. Dentro de ellas pueden haber subdivisiones.

- **Actividad diaria**: ventas, atenci√≥n al cliente, manejo de inventario, facturaci√≥n, pagos, operaciones locales.
- **Datos potenciales**: consumo local, comportamiento de clientes, tiempos de entrega, cadenas de distribuci√≥n.
- **Documentos necesarios para Identificarla**:
  - Certificaci√≥n de personer√≠a jur√≠dica: Contiene la informaci√≥n de la empresa como su nombre, c√©dula jur√≠dica, tipo de sociedad, nombre del representante.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante.
  - C√©dula jur√≠dica: Debe coincidir con la certificaci√≥n de personer√≠a jur√≠dica.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.
  - Acta Constitutiva y Estatutos: Es un acta que establece la existencia legal de la empresa.
  - Constancia de incripci√≥n en el PYME: Demuestra que est√° registrada en el MEIC y cumple los requisitos para ser PYME.
  - Departamento a Registrar: Se debe registrar a que departamento de la empresa pertenece el registro.

**Sociedades An√≥nimas (S.A.)**
Empresas grandes con estructura formal, juntas directivas y accionistas. Comunes en construcci√≥n, industria, finanzas o tecnolog√≠a.

- **Actividad diaria**: operaci√≥n por departamentos, contrataci√≥n de proveedores, desarrollo de productos, comercio exterior.
- **Datos potenciales**: operaciones financieras, productividad, log√≠stica, desempe√±o empresarial.
- **Documentos necesarios para Identificarla**:
  - Certificaci√≥n de personer√≠a jur√≠dica: Contiene la informaci√≥n de la empresa como su nombre, c√©dula jur√≠dica, tipo de sociedad, nombre del representante.
  - C√©dula jur√≠dica: Debe coincidir con la certificaci√≥n de personer√≠a jur√≠dica.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.
  - Estatutos sociales: Documento que establece la organizaci√≥n, funcionamiento, objeto social, entre otra informaci√≥n valiosa sobre la empresa.
  - Certificado de Registro Mercantil: Documento que certifica la inscripci√≥n de la empresa en el Registro Mercantil.
  - Certificado de Existencia: Documento legal que certifica la existencia de la empresa.
  - Departamento a Registrar: Se debe registrar a que departamento de la empresa pertenece el registro.

### **Empresas p√∫blicas y entes estatales**

Entidades que operan con fondos p√∫blicos y ofrecen servicios esenciales.

**Instituciones aut√≥nomas**

Ejemplos: CCSS, ICE, INS, TSE.

- **Actividad diaria**: prestaci√≥n de servicios de salud, energ√≠a, seguros, agua, telecomunicaciones.
- **Datos potenciales**: cobertura geogr√°fica, consumo, atenci√≥n m√©dica, reclamos ciudadanos.
- **Documentos necesarios para Identificarla**:
  - C√©dula Jur√≠dica: adjuntar la c√©dula jur√≠dica con el formato "4-000-NNNNNN"
  - Nota oficial con membrete institucional: Nombre completo de la persona que actuar√° como representante de la instituci√≥n, con firma digital de un funcionario autorizado.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.
  - Acta de Resoluci√≥n Interna: Un acta firmada que describa la resoluci√≥n dada internamente en el ente estatal.
  - Departamento a Registrar: Se debe registrar a que departamento de la empresa pertenece el registro.

**Empresas estatales**
Ejemplos: RECOPE, RACSA.

- **Actividad diaria**: importaci√≥n, distribuci√≥n de bienes estrat√©gicos, operaci√≥n con entes reguladores.
- **Datos potenciales**: consumo nacional, log√≠stica, demanda energ√©tica.
- **Documentos necesarios para Identificarla**:
  - C√©dula Jur√≠dica: adjuntar la c√©dula jur√≠dica con el formato "3-NNN-NNNN"
  - Certificaci√≥n de personer√≠a jur√≠dica: Contiene la informaci√≥n de la empresa como su nombre, c√©dula jur√≠dica, nombre del representante.
  - Nota oficial con membrete institucional: Nombre completo de la persona que actuar√° como representante de la instituci√≥n, con firma digital de un funcionario autorizado.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante del √≥rgano.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.
  - Acta de Resoluci√≥n Interna: Un acta firmada que describa la resoluci√≥n dada internamente en el ente estatal.
  - Departamento a Registrar: Se debe registrar a que departamento de la empresa pertenece el registro.

**Empresas municipales**
Entidades creadas por municipalidades para servicios locales, un ejemplo es la ESPH (Empresa de Servicios P√∫blicos de Heredia).

- **Actividad diaria**: recolecci√≥n de residuos, parqueo, mantenimiento urbano, servicios culturales.
- **Datos potenciales**: desarrollo cantonal, planificaci√≥n urbana, gesti√≥n ambiental.
- **Documentos necesarios para Identificarla**:
  - C√©dula Jur√≠dica: adjuntar la c√©dula jur√≠dica.
  - Certificaci√≥n de personer√≠a jur√≠dica: Contiene la informaci√≥n de la empresa como su nombre, c√©dula jur√≠dica, nombre del representante.
  - Nota oficial con membrete institucional: Nombre completo de la persona que actuar√° como representante de la instituci√≥n, con firma digital de un funcionario autorizado.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.
  - Acuerdo Municipal: Un acta firmada que describa la resoluci√≥n dada internamente en el ente municipal.

### **√ìrganos del Poder Ejecutivo**

Ejemplos: MEP, MINAE, MOPT

- **Actividad diaria**: Formulaci√≥n e implementaci√≥n de pol√≠ticas p√∫blicas, ejecuci√≥n de programas nacionales, regulaci√≥n sectorial, gesti√≥n presupuestaria y administrativa.
- **Datos potenciales**: Indicadores educativos, ambientales, de infraestructura y transporte; estad√≠sticas de cobertura, acceso y calidad de servicios; y datos geoespaciales y sectoriales seg√∫n competencia del ministerio.
- **Documentos necesarios para Identificarla**:
  - Oficio firmado por jefatura autorizada: Documento firmado por la jefatura con la autorizaci√≥n.
  - C√©dula del representante legal: C√©dula del representante del √≥rgano.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.
  - Nombre y Apellido del representante del √≥rgano.

### **C√°maras y gremios**

Organizaciones que agrupan empresas o profesionales.

**C√°maras empresariales**
Ejemplos: C√°mara de la Construcci√≥n, C√°mara de Tecnolog√≠as de Informaci√≥n, C√°mara de Exportadores de Flores.

- **Actividad diaria**: representaci√≥n del sector, capacitaciones, generaci√≥n de estudios y estad√≠sticas.
- **Datos potenciales**: empleo, productividad, retos sectoriales, inversi√≥n.
- **Documentos necesarios para Identificarla**:
  - Certificaci√≥n de personer√≠a jur√≠dica: Contiene la informaci√≥n de la empresa como su nombre, c√©dula jur√≠dica, nombre del representante.
  - C√©dula Jur√≠dica: adjuntar la c√©dula jur√≠dica, que coincida con la de la certificaci√≥n de personer√≠a jur√≠dica.
  - Carta firmada por el comit√© o jefatura: Oficio que demuestre autorizaci√≥n de la c√°mara empresarial.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.

**Gremios profesionales o t√©cnicos**
Ejemplos: colegios de m√©dicos, ingenieros, abogados.

- **Actividad diaria**: certificaci√≥n profesional, formaci√≥n continua, vigilancia del ejercicio profesional.
- **Datos potenciales**: matr√≠cula, servicios ofrecidos, formaci√≥n, cobertura geogr√°fica.
- **Documentos necesarios para Identificarla**:
  - Certificaci√≥n de personer√≠a jur√≠dica: Contiene la informaci√≥n de la empresa como su nombre, c√©dula jur√≠dica, nombre del representante.
  - C√©dula Jur√≠dica: adjuntar la c√©dula jur√≠dica, que coincida con la de la certificaci√≥n de personer√≠a jur√≠dica.
  - Acta de asamblea constitutiva: Oficio que demuestre autorizaci√≥n de la c√°mara empresarial.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.

### **Universidades y centros acad√©micos**

Instituciones de educaci√≥n superior, tanto p√∫blicas como privadas, dedicadas a la formaci√≥n profesional, la investigaci√≥n cient√≠fica y la extensi√≥n social. Dentro de estas operan subdivisiones como facultades, escuelas y centros de investigaci√≥n (CI).

**Universidades p√∫blicas y privadas**

- **Actividad diaria**: matr√≠cula, gesti√≥n de carreras, proyectos de investigaci√≥n y extensi√≥n.
- **Datos potenciales**: rendimiento acad√©mico, estad√≠sticas de graduaci√≥n, impacto social.
- **Documentos necesarios para Identificarla**:
  - C√©dula Jur√≠dica: adjuntar la c√©dula jur√≠dica, que coincida con la de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombramiento Interno: Carta que oficializa el encargado de hacer el registro en la plataforma.
  - Carta de la Unidad Interna: Oficio que da la autorizaci√≥n del unidad interna de la universidad:
  - Unidad Interna: Escuela, Facultad, Centro de Investigaci√≥n.
  - Nombre de la unidad: Ingresar nombre espec√≠fico.
  - C√©dula del representante legal: Debe coincidir con el de la certificaci√≥n de personer√≠a jur√≠dica.
  - Nombre y Apellido del representante.
  - Correo Institucional: correo electr√≥nico del encargado de la instituci√≥n.

### Llaves Tripartita

Una parte fundamental del sistema es la gesti√≥n tripartita de llaves. Se adopt√≥ un esquema de tres claves: una asignada al representante designado por la empresa para su registro, otra para cada usuario secundario de la empresa, y una tercera bajo control de Data Pura Vida.
El mecanismo se basa en la comparaci√≥n de una Data Encryption Key (DEK), la cual es cifrada con una Key Encryption Key (KEK) espec√≠fica para cada parte involucrada.
Los detalles sobre cu√°ndo, d√≥nde y c√≥mo se utiliza este esquema se ampliar√°n en la secci√≥n de definici√≥n del backend, pero por ahora esta descripci√≥n representa la visi√≥n de alto nivel.

## Dise√±o del Frontend

### Plataforma de Autenticaci√≥n

Primero se describir√° c√≥mo se realizar√° en el frontend el proceso de registro e inicio de sesi√≥n para personas f√≠sicas, ya que los conjuntos no tienen un inicio de sesi√≥n propio, sino que solo cuentan con creaci√≥n de cuenta, la cual es manejada posteriormente por personas f√≠sicas que la agregan a su cuenta personal.

Dado que la plataforma ser√° alojada en AWS, usaremos AWS Cognito para el registro y posterior inicio de sesi√≥n de las personas en la plataforma. Del servicio se utilizar√°n las siguientes caracter√≠sticas:

- Uso de JWT Tokens para autorizar el acceso a las APIs en EKS.
- Uso de Cognito User Pools que registren email, tel√©fono, c√©dula, nombre y apellidos.
- Uso de choice-based authentication para que los usuarios elijan c√≥mo iniciar sesi√≥n (contrase√±a, Email OTP o SMS OTP).
- Uso del AuthFlow de USER_PASSWORD_AUTH, que incluir√° un MFA obligatorio con Email OTP o SMS OTP.
- Uso del SDK de Cognito para agilizar este proceso. Sin embargo, el formulario de registro ser√° manejado program√°ticamente por nosotros, ya que el est√°ndar que ofrece Cognito no se adapta a nuestro caso de uso.

No hay que dejar de lado que un paso muy importante en nuestro sistema es la captura de imagen de la c√©dula y la prueba de vida, para comprobar que la persona que solicita la cuenta sea real. Por lo tanto, en el flujo de registro de una persona f√≠sica, este paso se realizar√° antes de autorizar el registro en el sistema por medio de cognito.

Para implementar esta l√≥gica se usar√° el sistema de terceros SumSub, ya que provee herramientas para:

- Verificaci√≥n de ID (c√©dula en nuestro caso).
- Prueba de vida y detecci√≥n de deepfake.
- Prueba de direcci√≥n, para verificar que la direcci√≥n f√≠sica del usuario sea real.
- Revisi√≥n de AML.
- Revisi√≥n de KYC.

Para realizar todas estas tareas en nuestro frontend se utilizar√° el WebSDK que SumSub provee para React, que cuenta con todas las herramientas necesarias para implementar las opciones mencionadas.

El proceso de registro de las empresas ser√° distinto, ya que requiere realizar tres tareas principales:

- Validaci√≥n y completitud de los datos.
- Validaci√≥n de personas asignadas.
- Asignaci√≥n de llaves tripartitas.

Por ello no se usar√° Cognito para las empresas. Sin embargo, para la validaci√≥n SumSub ofrece una soluci√≥n de KYB (Know Your Business), que permite:

- Revisar el registro nacional para seleccionar la empresa.
- Verificar al encargado del registro mediante prueba de vida y verificaci√≥n de c√©dula.
- Generar cuestionarios personalizados donde se pueden adjuntar documentos legales que SumSub aprobar√°.

Por lo tanto, la validaci√≥n de empresas tambi√©n ser√° implementada con SumSub. El almacenamiento de informaci√≥n y la delegaci√≥n de llaves tripartitas ser√°n discutidos m√°s adelante en la secci√≥n del backend.

Por otro lado, cabe aclarar que para poder llevar a cabo las validaciones con SumSub es necesario dirigirse a la p√°gina de SumSub y ah√≠ generar flows. Los desarrolladores tendr√°n que crear estos flows con base en las especificaciones dadas sobre que informaci√≥n se le debe solicitar a cada tipo de usuario (los distintos tipos de jur√≠dico y el f√≠sico) que fue especificada previamente en este subcap√≠tulo.

### Arquitectura del Cliente

Nuestra arquitectura de cliente consistir√° en Client Side Rendering con rendering est√°tico, con una √∫nica capa dedicada a la web. Esta decisi√≥n se toma porque los bundles de React generados en el build de cada proyecto ser√°n almacenados en un bucket de S3, el cual ser√° servido a los clientes mediante el CDN provisto por CloudFront.

Por otro lado, uno de los requerimientos de este m√≥dulo es que solo puede ser accedido con IPs Costarricenses (El registro), por lo que cuando se desee acceder a la p√°gina de registro Cloudfront ejecutar√° un Lambda@Edge Function que revisar√° la IP del usuario y en caso de no ser de Costa Rica, no servir√° dicha ruta del App.

Adem√°s, para acceder al backend se utilizar√° una √∫nica API, desarrollada en FastAPI. Se entrar√° en m√°s detalles de dicha API m√°s adelante.

### Patrones de Dise√±o de Objetos

A Continuaci√≥n el diagrama de clases del frontend del Bioregistro:

![Patrones de Dise√±o de Objetos](img/FrontBioregistro.png)

- **Caja Verde**: La caja verde representa el patr√≥n de Chain of Responsability. Est√° asociado a los distintos tipos de forms que existen en el sistema, y gracias a la naturaleza del CoR, permite declararlos din√°micamente. Inclusive, permite que si en un futuro se desea agregar otra capa, sea sumamente sencillo.
- **Cajas Celeste**: Las cajas celestes representan el strategy pattern, ya que por medio de herencia se aisla los distintos tipos de forms para colectivos, y de colectivos.
- **Caja Roja**: Esta caja roja cumple dos funciones, de Singleton y de Facade. De singleton porque de esta manera solo existe una instancia que se conecta al API en todo momento. Adem√°s funciona como Facade ya que aisla toda la l√≥gica de conexi√≥n con el API del backend en una sola clase.

### Componentes Visuales

**Patrones y Principios**

- **Responsive Design**: Aunque el enfoque principal de nuestro sistema est√° en el uso desde web desktop, es importante implementar un dise√±o responsivo para que los usuarios puedan realizar el registro, prueba de vida y verificaci√≥n de c√©dula de forma c√≥moda desde la c√°mara de sus celulares. Este dise√±o responsivo se lograr√° aprovechando las opciones que ofrece Tailwind CSS para distintos tama√±os de pantalla, utilizando prefijos como sm:, md:, lg:, y xl:, que permiten adaptar los estilos seg√∫n el dispositivo.

- **SOLID**:

  - Single Responsibility: Cada componente en el bioregistro solo tendr√° una responsabilidad. Por ejemplo, el formulario que detecta si es persona f√≠sica o un conjunto solo emplea esa tarea, o los componentes de verificaci√≥n de SumSub son distintos y cada uno hace su propia tarea: uno para la prueba de vida, otro para la verificaci√≥n de id, y as√≠ para todo componente.
  - Open Closed Principle: Los componentes de registro son din√°micos y est√°n separados, gracias a esto, si en un futuro se desea agregar otro tipo de organizaci√≥n, tan solo se debe desarrollar dicho componente y de ah√≠ la conexi√≥n con el resto del flujo ser√° directa.
  - Liskov Substitution Principle: La herencia debe ser utilizada solo cuando es necesaria. Por ejemplo, para los formularios de documentos para empresas si es valioso usar una superclase, pero no tiene sentido agruparlos en una clase madre con el formulario de prueba de vida.
  - Interface Segregation Principle: No usaremos una interf√°z enorme para agrupar a todo tipo de formulario, solo si es necesario se definir√°n, y cuando se haga se ha≈ïan lo m√°s espec√≠ficas posibles. Por ejemplo, una interf√°z madre para los procesos de SumSub que ocupen uso de camara.
  - Dependency Inversion Principle: Las clases nunca deben depender de implementaciones, deben usar las intefaces. Por ejemplo, la clase que agrupe los formularios para registro de organizaciones debe poder permitir cualquier tipo de cuestionario de documentos, independientemente de con que colectivo se este trabajando.

- **Dry principle**: En la medida de lo posible se usar√° la menor cantidad de c√≥digo repetido. Dos ejemplos de esto son: gracias a que usaremos atomic design, componentes como botones o labels ser√°n reutilizado no solo en el bioregistro, pero en todo el sistema; y otro ejemplo es que tanto en el registro de colectivos como de personas se pide prueba de vida, entonces se utilizar√° la misma clase para manejar ambas tareas.

- **Separation of Concern**: Se cumple este principio ya que las distintas capas del frontend estar√°n bien definidas. En la capa de datos solo se gestionar√°n los objetos como personas y colectivos. Luego por medio de CustomHooks se gestionar√° la l√≥gica del ViewModel, por ejemplo, las funciones como GetLivenessCheck o AttachPDF. Y Finalmente la capa de Vista se dedicar√° a tan solo eso, hacer el render de los componentes.

- **Atomic Design**: Este es un patr√≥n muy com√∫n en React, y se ver√° reflejado porque los componentes ser√°n creados empezando por √°tomos, como b√≥tones e inputs; Luego con mol√©culas, que por ejemplo podr√≠a ser un item de formulario que tenga un label, bot√≥n e input; Para despu√©s crear Organismos como los Formularios Completo; Para que despu√©s se junten todos en una p√°gina que ser√° la que finalmente renderize todo.

- **MVVM**: Estamos usando React, as√≠ que MVC no era una opci√≥n, y el flujo que tenemos planeado de comunicaci√≥n entre hooks y componentes se adapta a un MVVM. Se aplicar√° de la siguiente forma:
  - Model: Lo implementaremos en la conexi√≥n con la API (ser√°n funciones), y tambi√©n en los objetos que luego se insertar√°n en la base de datos (no vamos a poner la l√≥gica de negocio ac√°, como recomiendan empresas como Microsoft, para que nuestra app no viole el principio de responsabilidad √∫nica ni la separaci√≥n de responsabilidades) como los distintos tipos de organizaci√≥n, o las personas f√≠sicos.
  - View: Ser√° toda la parte visual de los componentes, que van a seguir atomic design.
  - ViewModel: Se implementar√° en los custom hooks reutilizables que gestionan la l√≥gica de negocio.

**Toolkits y Standards**:

- **Vite**: Se usar√° como servidor local para el desarrollo, y tambi√©n para hacer el bundle de la aplicaci√≥n.
- **React Router**: Herramienta que permite manejar un app de react por medio de rutas.
- **ESlint**: Se usar√° para mantener un est√°ndar de c√≥digo y evitar errores comunes.

### Estructura de Carpetas

```bash
frontend/
‚îú‚îÄ‚îÄ public/                   #Assets como imagenes
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/                  #Ac√° estar√°n las funciones del API
‚îÇ   ‚îú‚îÄ‚îÄ model/                #Ac√° se almacenar√°n las clases del modelo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Person
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Collective
‚îÇ   ‚îú‚îÄ‚îÄ components/           #Atomic Design
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ atoms/            #Componentes m√°s b√°sicos
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Button.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Icon.jsx
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ molecules/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ FormItem.jsx
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ organisms/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PersonalInfoForm.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ProofOfAddressForm.jsx
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ CollectiveForm.jsx
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ PersonForm.jsx
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ hooks/                 #ViewModel
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useLivenessCheck.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useIdVerification.jsx
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ contexts/              #Contexto del form, que datos han sido registrados
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ FormContext.jsx
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ pages/                 #Uso de las templates lista para formar una p√°gina completqa
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MainRegister.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ VerifyEmail.jsx
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ styles/               #Tailwind
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄglobals.css        #Configuraci√≥n de Tailwind
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/                #Funciones DRY
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ App.tsx               #Punto de Entrada
‚îÇ
‚îÇ
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ unit/
    ‚îî‚îÄ‚îÄ integration/
```

### Diagrama del Front

A continuaci√≥n se presenta el diagrama del frontend de Bioregistro. En √©l se muestra c√≥mo el contenido est√°tico generado por React se almacena en un bucket de S3, donde residen todos los componentes visuales, su ViewModel a trav√©s de funciones y custom hooks, y las clases modelo como Person y Collective.

Tambi√©n se indica que los componentes visuales est√°n estilizados con Tailwind CSS. La interacci√≥n con el backend se realiza mediante el m√≥dulo apiConnector.

Finalmente, se incluye una Lambda@Edge function que, antes de que CloudFront entregue el HTML, verifica si la IP de acceso corresponde a Costa Rica, como parte de un filtro geogr√°fico.

![image](img/DiagramaFrontRegistro.png)

## Dise√±o del backend

### Microservicios

A continuaci√≥n se dar√° una explicaci√≥n de todos los microservicios correspondientes al Bioregistro.

**1. identity-verification-service**

Este servicio se encarga de gestionar todo el flujo de autenticaci√≥n a trav√©s de la plataforma SumSub.

Para personas f√≠sicas, realiza los siguientes procesos:

- Verificaci√≥n de c√©dula.
- Prueba de vida y detecci√≥n de deepfakes.
- Verificaci√≥n de direcci√≥n f√≠sica.

Para colectivos, se utilizar√° la funcionalidad Full KYB 2.0 de SumSub que es una verificaci√≥n por IA y por personas f√≠sica que incluye:

- Consulta al Registro Nacional para identificar el colectivo.
- Revisi√≥n de documentos legales seg√∫n el tipo de entidad.
- Revisi√≥n de que representantes trabajen en dicha empresa.

Ahora bien dentro de √©l existir√°n los siguientes componentes:

- SumSubController: Expone los endpoints del servicio para que el API General pueda acceder a √©l, estos ser√°n:
  - /sumsub/person/token: Para mandar al crear el Applicant Id en SumSub.
  - /sumsub/person/webhook: Para recibir aprobaciones de personas desde SumSub.
  - /sumsub/collective/token: Para mandar al crear el Applicant Id en SumSub.
  - /sumsub/collective/check-documents/collective-type : Manda los documentos legales de SumSub a Auto-KYB para verificarlos. Existe un endpoint por cada tipo de colectivo.
  - /sumsub/collective/manual-verification: Para que los colectivos tengan la opci√≥n de solicitar una verificaci√≥n manual.
  - /sumsub/collective/webhook: Para recibir aprobaciones de colectivos desde SumSub.
- CollectiveService: Se encarga de abstraer las llamadas a los workflows de SumSub seg√∫n el tipo de colectivo, y hacer el registro del applicant.
- PersonService: Se encarga de registrar las personas en SumSub y generar UUIDs para los usuarios.
- WebHookProcessor: Se encarga de procesar los resultados de las respuestas de SumSub.
- CollectiveVerificationRouter: Middleware que se encarga de ver si se hace verifaci√≥n manual o por medio de SumSub a los colectivos.

A continuaci√≥n se muestra el flujo completo de interacci√≥n entre frontend y este componente para verificar una persona f√≠sica:

1. La persona inicia el proceso de verificaci√≥n:

- Frontend llama a: POST /sumsub/person/token:
  ```json
  {
    "email": "email de la persona",
    "Nombre": "nombre de la persona",
    "Apellido1": "Primer apeliido de la persona",
    "Apellido2": "Segundo apellido de la persona",
    "Telefono": "telefono de la persona",
    "direccion": "direcci√≥n donde vive la persona"
  }
  ```
- El SumSubController dirige la carga al PersonService que se encargar√° de registrar el Applicant en SumSub y enviarle un UUID interno. Obtendr√° de respuesta el Id interno de SumSub que se usar√° para realizar la verificaci√≥n.

  - Tambi√©n en la tabla de SumSubApplicants se registrar√° el UUID interno, una fila llamada Approved en False, y todas las credenciales dadas. Esto permitir√° que cuando las personas traten de registrarse solo puedan una ves este flag sea cambiado a True (M√°s detalles sobre el registro ser√°n explicados en el registration-service).

- Se retorna al frontend:
  ```json
  {
    "SumSubId": "id-de-sumsub",
    "InternalId": "uuid-del-sistema"
  }
  ```

2. El sdk de SumSub realiza la prueba de vida, la verificaci√≥n de id, y la prueba de direcci√≥n f√≠sica:

- En este punto el proceso puede durar desde minutos a horas, por lo que se detiene el proceso.

3. Llamada al Webhook desde SumSub:

- Una vez SumSub haya finalizado el proceso de verificaci√≥n procedera a llamar al endpoint (En el dashboard de SumSub se puede configurar una uri hacia donde mandar las verificaciones) del webhook por medio de una solicitud POST a /sumsub/person/webhook con la siguiente informaci√≥n:

  ```json
  {
    "event": "applicantApproved",
    "applicantId": "sumsub-uuid",
    "externalUserId": "uuid-interno-del-sistema",
    "timestamp": "2025-06-06T15:00:00Z"
  }
  ```

- Se env√≠a dicha informaci√≥n a WebHookProcessor para que empiece el proceso de aprobaci√≥n:

  - Se pone el estado en SumSubApplicants como approved en True.
  - Se genera un token UUID, el cual ser√° guardado en Redis junto al UUID del usuario en SumSubApplicants, de la siguiente forma:

    ```python
    import redis
    import uuid

    r = redis.Redis(host='localhost', port=6379, db=0)

    token = str(uuid.uuid4())
    sumsub_id = "abc123" #Este id viene desde el webhook

    r.setex(f"registration_token:{token}", 86400, sumsub_id) #Para que persista por 24 horas
    ```

  - Ya que se tiene el token se env√≠a un mensaje por medio de RabbitMQ al Notification Service para que env√≠e un correo con un link al registro, que lleve de query parameter el token:
    ```txt
    https://data-pura-vida.com/register/person?token=<token_uuid>
    ```
  - M√°s adelante, en el registration-service se dir√° como se manejar√° el registro con base en dicho token de redis.

4. El proceso de verificaci√≥n fue exitoso, se continua a registro.

Ahora, se muestra el flujo completo de interacci√≥n entre frontend y este componente para verificar un colectivo:

1. La persona representante del colectivo inicia el proceso de verificaci√≥n:

- Frontend llama a: POST /sumsub/collective/token:
  ```json
  {
    "email": "persona@ejemplo.com"
  }
  ```
- El SumSubController dirige la carga al CollectiveService que se encargar√° de registrar el Applicant en SumSub y enviarle un UUID interno. Obtendr√° de respuesta el Id interno de SumSub que se usar√° para realizar la verificaci√≥n.

- Se retorna al frontend:
  ```json
  {
    "SumSubId": "id-de-sumsub",
    "InternalId": "uuid-del-sistema"
  }
  ```

2. El sdk de SumSub realiza la b√∫squeda de Colectivo en el registro nacional

3. El usuario adjunta al formulario los documentos legales seg√∫n el tipo de colectivo, y los representantes que ya deben de estar previamente registrados en el sistema (Cabe aclarar que el administrador de la empresa que est√° haciendo la gesti√≥n del registro tambi√©n debe de estar registrado en el sistema de Data Pura Vida)

- El frontend lo env√≠a por medio de /sumsub/collective/check-documents/collective-type

  ```json
  {
    "applicantId": "sumsub-uuid",
    "Representatives": "[Lista de objetos de tipo PersonaF√≠sica]",
    "Admin": "{Objeto de tipo PersonaFisica correspondiente al que est√° gestionando el registro del colectivo}",
    "Documents": "[Los documentos legales seg√∫n el tipo de colectivo]"
  }
  ```

- El SumSubController dirige la ejecuci√≥n al CollectiveService.

- Primero se revisar√° que ya el colectivo no haya sido registrado en el sistema.

- Despues se checkear√° si los usuarios insertados en Representatives y Admin efectivamente existen en la base de datos de RDS. En dado caso se insertan registros a SumSubCollectiveApplicant, la c√∫al guardar√° el UUID del sistema, el Id del administrador, y el estado de aprobaci√≥n del colectivo. Y tambi√©n se guardar√°n en SumSubCollective un FK a los representantes en Representatives y al registro en SumSubCollectiveApplicant.

- Luego se encargar√° de enviar a los WorkFlows de SumSub la informaci√≥n de las empresas.

- Tambi√©n se encarga de subir los documentos legales a un S3 Bucket bajo un directorio que tenga como nombre el UUID. Dicha interacci√≥n se hace por medio del uso de Boto3 en python.

- En este punto el proceso puede durar desde minutos a horas, por lo que se detiene el registro de empresa en el frontend.

4. Llamada al Webhook desde SumSub:

- Una vez SumSub haya finalizado el proceso de verificaci√≥n proceder√° a llamar al endpoint del webhook por medio de una solicitud POST a /sumsub/collective/webhook con la siguiente informaci√≥n:

  ```json
  {
    "event": "applicantApproved",
    "applicantId": "sumsub-uuid",
    "externalUserId": "uuid-interno-del-sistema",
    "timestamp": "2025-06-06T15:00:00Z"
  }
  ```

- Se env√≠a dicha informaci√≥n a WebHookProcessor para que empiece el proceso de aprobaci√≥n:

  - Se pone el estado en SumSubCollectiveApplicant como approved en True.
  - Se genera un token UUID, el cual ser√° guardado en Redis junto al UUID del colectivo en SumSubCollectiveApplicant, de la siguiente forma:

    ```python
    import redis
    import uuid

    r = redis.Redis(host='localhost', port=6379, db=0)

    token = str(uuid.uuid4())
    sumsub_id = "abc123" #Este id viene desde el webhook

    r.setex(f"collective-register:{token}", 86400, sumsub_id) #Para que persista por 24 horas
    ```

- Ya que se tiene el token se env√≠a un mensaje por medio de RabbitMQ al Notification Service para que env√≠e un correo con un link a la creaci√≥n de llaves tripartitas, que lleve de query parameter el token:

  ```txt
  https://data-pura-vida.com/collective-register?token=<token_uuid>
  ```

  - M√°s adelante, en el registration-service se dir√° como se manejar√° el registro con base en dicho token de redis.

4. El proceso de verificaci√≥n fue exitoso, se continua a creaci√≥n de las llaves tripartita.

Ahora bien, en el caso de colectivos, puede suceder que SumSub no encuentre al colectivo en sus bases de datos. Si esto ocurre, se habilita una opci√≥n de revisi√≥n manual, la cual env√≠a un mensaje al notification-service a trav√©s de RabbitMQ. Este servicio notificar√° a los administradores, quienes podr√°n completar la verificaci√≥n manual desde el portal web del backoffice.

Previamente fue mencionado, pero a modo de aclaraci√≥n cabe decir que los templates de revisi√≥n ser√°n creados desde el SumSub Dashboard. con base en la informaci√≥n listada al inicio del cap√≠tulo. Posteriormente en el c√≥digo podr√°n ser llamados de esta forma por medio de un request al API similar a este:

```python
import requests
import time
import hmac
import hashlib
import base64

flow_name = "kyb_legal_doc_flow"

ts = str(int(time.time()))
sig_data = ts + 'POST' + '/resources/applicants?levelName=' + flow_name
signature = base64.b64encode(
    hmac.new(SECRET_KEY.encode(), sig_data.encode(), digestmod=hashlib.sha256).digest()
).decode()

headers = {
    "X-App-Token": APP_TOKEN,
    "X-App-Access-Sig": signature,
    "X-App-Access-Ts": ts,
    "Content-Type": "application/json"
}

data = {
    "externalUserId": external_user_id,
    "info": {
        "companyName": "Colectivo Pura Vida S.A.",
        "registrationNumber": "CR-123456789",
        "country": "CR",
        "email": "legal@colectivopv.cr"
    }
}
response = requests.post(
    f"https://api.sumsub.com/resources/applicants?levelName={flow_name}",
    headers=headers,
    json=data
)
```

**2. auth-service**

Este servicio es un facade de autenticaci√≥n sobre Cognito, por el cu√°l los usuarios deber√°n pasar siempre antes de iniciar sesi√≥n. En el habr√°n los siguientes componentes:

- AuthController: Expone los endpoints del servicio para que el API General pueda acceder a √©l, estos ser√°n:
  - /auth/login: Para realizar el login por medio de contrase√±a
  - /auth/login/otp: Para realizar el login por medio de OTP
  - /auth/login/mfa: Para realizar el login por medio de MFA
  - /auth/login/mfa/resend Para poder reenviar los tokens MFA en caso de ser necesario
  - /auth/login/verify-mfa Para poder revisar que el MFA sea satisfactorio
  - /auth/logout: Para la gesti√≥n del Logout de la aplicaci√≥n
- CognitoService: Se encarga de abstraer las llamadas de signup, login, challenge y refresh.
- MFAService: L√≥gica para MFA (enviar y validar OTP por SMS/email).
- AuthChoiceHandler: Implementa l√≥gica de choice-based auth (elegir entre OTP o pass).

A continuaci√≥n se muestra el flujo completo de inicio de sesi√≥n con MFA en la arquitectura:

1. El usuario inicia sesi√≥n:

- Frontend llama a: POST /auth/login:

  ```json
  {
    "identifier": "santi@gmail.com",
    "authMethod": "password", // o "otp"
    "password": "****" // solo si es m√©todo "password"
  }
  ```

- AuthController recibe el request y llama a AuthChoiceHandler para enrutar seg√∫n authMethod.

2. Verificaci√≥n de credenciales (si es con contrase√±a)

- Si authMethod es "password":
  - AuthChoiceHandler llama a CognitoService.initiateAuth()
  - Cognito verifica credenciales.
    - Si est√°n bien pero MFA est√° activado, responde con un Session y un ChallengeName: SMS_MFA o similar.
    - Si el usuario no tiene MFA activado, responde con el JWT Token directamente.

3. El frontend reacciona a la respuesta

- Si recibe ChallengeName y Session, el frontend muestra pantalla MFA.
- Luego procede a enviar una llamada a /auth/login/mfa para que el MFAService envi√© un mensaje por medio de rabbitMQ al notification-service. Para que as√≠ se env√≠e un correo electr√≥nico con el pin.

4. Usuario env√≠a su c√≥digo MFA

- Frontend llama a: POST /auth/login/veriyf-mfa con:

  ```json
  {
    "code": "123456",
    "session": "eyJraWQiOi...",
    "identifier": "santi@gmail.com",
    "deliveryMethod": "email"
  }
  ```

- AuthController pasa a MFAService.verifyCode()
  - Llama a CognitoService.respondToAuthChallenge()
  - Si todo bien, devuelve los JWT tokens (ID, access, refresh).

5. Tokens son devueltos al frontend

- Frontend los guarda y los manda en cada request siguiente al backend.

Ahora bien, en caso de que el usuario decida iniciar sesi√≥n por medio de OTP el proceso es similar lo que cambia es que el primer request pide "OTP", y el sistema va a generar uno que se enviar√° por medio de SMS al usuario para que posteriormente pueda iniciar sesi√≥n.

Cabe aclarar que las interacciones entre los componentes de este microservicio se realizar√°n por medio de REST APIs. Por lo que cada uno de ellos estar√° escritos en FastAPI y recibir√° las solicitudes por medio de dicha interf√°z. Para cada componente se tendr√° un archivo con los endpoints y la l√≥gica del api, y otros con la l√≥gica de negocio de cada uno.

**3. registration-service**

Ahora bien, el registration-service es el encargado de registrar tanto personas como colectivos en el sistema.

Con respecto al registro de personas se encarga de cargarlas a Cognito y tambi√©n en la base de datos del bioregistro en RDS, mientras que con colectivos solo se registra en RDS, y los documentos legales que se hab√≠an guardado previamente en un S3 Bucket, se pasan al bucket oficial de documentos legales llamado "collective_data".

En el habr√°n los siguientes componentes:

- RegistrationController: Expone los endpoints del servicio para que el API General pueda acceder a √©l, estos ser√°n:
- /register/person: Registro de una persona f√≠sica.
- /register/collective: Registro de una organizaci√≥n/colectivo.
- /register/collective/key-generation: Endpoint para llamar al KeyGenerationHandler.
- /register/check-token: Revisa el token UUID generado por el identity-verification-service.
- /register/person/generate-token: Genera un nuevo token UUID para poder registrar al usuario en el sistema.
- /register/collective/generate-token: Genera un nuevo token UUID para poder registrar al colectivo en el sistema.
- TokenManager: Este componente se encargar√° de operar con los tokens.
- PersonRegistrationService: Este componente se encargar√° del crear el usuario en cognito y rds.
- CollectiveRegistrationService: Este componente se encargar√° del crear el usuario en rds, dynamo y cargar documentos al bucket adecuado.
- KeyGenerationHandler: Este componentes se encarga de comunicarse con el microservicio de key-management-service para crear las llaves tripartita

A continuaci√≥n se presenta el flujo de registro de una persona f√≠sica:

1. Verificaci√≥n de token UUID:

- Apenas el usuario entra al sitio web de registro (Si lo hace de forma correcta fue siguiendo el link que se envi√≥ a su correo en el identity-verification-service)

- Se hace un POST con /register/check-token, y se pasa el control a TokenManager para que se verifica si el query parameter de token: registration_token:<TOKEN_UUID> existe.

- En dado caso se usa como clave en redis con el prefijo de registration_token, y si retorna un UUID de la tabla de SumSubApplicant signfica que ya el usuario fue aprobado. Si no retorna nada significa que o bien el UUID Token cumpli√≥ su TTL de 24 horas, o que se est√° intentando ingresar al registro de manera no oficial.

- Se retorna al frontend:

  ```json
  {
    "status": "approved"
  }
  ```

2. El usuario registra su contrase√±a en el frontend

- Hace un POST a /register/person:

  ```json
  {
    "token": "El mismo Token UUID de redis",
    "Password": "Contrase√±a del usuario"
  }
  ```

- Solo se solicita el password porque las credenciales ya hab√≠an sido obtenidas por medio del identity-verification-service. Si se volvieran a pedir, estariamos arriesgando que un usuario use credenciales reales en el identity-verification-service, pero en este servicio invente informaci√≥n.

- Primero se hace el registro del usuario en la cognito pool, y se extrae el UUID usado en dicha pool, para usarlo tambi√©n en RDS, de esta forma se guarda simetr√≠a entre ambos sistemas. Se hace de la siguiente forma:

  ```Python
  import boto3

  client = boto3.client('cognito-idp', region_name='us-east-1')

  #Se crea el usuario en cognito
  response = client.admin_create_user(
      UserPoolId='user-pool-id',
      Username='correo@ejemplo.com',
      UserAttributes=[
          {'Name': 'email', 'Value': 'correo@ejemplo.com'},
          {'Name': 'email_verified', 'Value': 'true'},
      ],
      MessageAction='SUPPRESS'
  )

  #Se registra su contrase√±a
  client.admin_set_user_password(
      UserPoolId='user-pool-id',
      Username='correo@ejemplo.com',
      Password='LaContraseniaQuePidioElUsuario123',
      Permanent=True
  )

  # Se extrae el UUID generado por Cognito
  sub = next(attr['Value'] for attr in response['User']['Attributes'] if attr['Name'] == 'sub')
  ```

3. Registro en el Sistema

- Una vez se obtiene el UUID de Cognito, tambi√©n se obtiene el UUID de la tabla de SumSubApplicant volviendo a sacarlo de redis con el token por medio del TokenManager.

- Ahora con la informaci√≥n de SumSubApplicant y el UUID de Cognito se registra el usuario en la tabla de PersonaFisica.

4. Proceso de registro de persona f√≠sica exitoso.

Otro proceso posible es el de creaci√≥n de un nuevo token en caso de que el TTL haya muerto (el proceso de solicitar un nuevo token como colectivo es el mismo, solo cambia el path):

1. Desde el Frontend el usuario hace:

- POST /register/person/generate-token

  ```json
  {
    "email": "correo con el que se gestion√≥ la verificaci√≥n"
  }
  ```

- Esto lo enruta al TokenManager.

2. Verificaci√≥n de Aprobaci√≥n

- Con base en el correo que se envi√≥ desde el frontend se revisa la tabla de SumSubApplicant, para ver si en verdad existe un registro con dicha informaci√≥n, y en todo caso que realmente est√© Aprobado.

- En caso de estar aprobado el TokenManager crea otro token en redis y se conecta al notification-service por medio de RabbitMQ y solicita el env√≠o de un nuevo correo.

3. Ya el usuario puede volver a intentar con el nuevo correo.

Finalmente se presenta el flujo de registro de un Colectivo:

1. Verificaci√≥n de token UUID:

- Apenas el usuario administrador del colectivo entra al sitio web de registro (Si lo hace de forma correcta fue siguiendo el link que se envi√≥ a su correo en el identity-verification-service)

- Se hace un POST con /register/check-token, y se pasa el control a TokenManager para que verifice si el query parameter de token: collective-register:<TOKEN_UUID> existe.

- En dado caso puede ser usado como clave en redis con el prefijo de registration_token, y si retorna un UUID de la tabla de SumSubApplicant signfica que ya el usuario fue aprobado. Si no retorna nada significa que o bien el UUID Token cumpli√≥ su TTL de 24 horas, o que se est√° intentando ingresar al registro de manera no oficial.

- Se retorna al frontend:

  ```json
  {
    "status": "approved"
  }
  ```

2. Se llama a la creaci√≥n de KEKs (Key Encryption Key) y DEKs parciales

- Hace un POST a /register/collective/key-generation

  ```json
  {
    "token": "El mismo Token UUID de redis"
  }
  ```

- Se enruta al KeyGenerationHandler que llamar√° por medio de su REST API al key-management-service. Se le enviar√° el token UUID de redis para que haga la gesti√≥n de llaves tripartita.

- Se espera como valor de retorno:

  ```json
  {
    "admin_dek": "La DEK asignada al administrador del colectivo",
    "dpv_dek": "La DEK asignada a data pura vida",
    "representatives_dek": "[IdDelRepresentate : DEK del representante]"
  }
  ```

- Cabe aclarar que cada DEK es un dictionary (en el key-management-service se muestra de que consiste), que debe ser guardado en postgres como JSONB.

3. Registro de informaci√≥n:
   Desde el frontend se hace

- POST /register/collective

- Se procede a hacer el registro de toda la informaci√≥n correspondiente al colectivo.

- Se crean los registros correspondientes a los representantes en la tabla de Representantes, en ella se guardan sus respectivas DEKs.

- Se crea el registro del colectivo en la tabla de Colectivo con su respectiva DEK.

- Se crea el registro en DEKDataPuraVida con la DEK del sistema y una referencia al colectivo que le corresponde.

- Se pasan todos los documentos del S3 Bucket temporal (Se conoce el directorio ya que es el mismo UUID de la tabla SumSubCollectiveApplicant) a "collective_data". Adem√°s se guarda referencia a dicha informaci√≥n en DynamoDB, y se usa el mismo Id que el usado en RDS para guardar el Colectivo para mantener simetr√≠a.

- Se crea un rol de IAM para que el colectivo pueda acceder a los datasets que suba, m√°s explicaci√≥n sobre como sirve esto se ver√° en el componente de la b√≥veda:

  - Se crea el json sobre el rol:

    ```json
    {
      "RoleName": "DPV_DataAccess_Colectivo1234",
      "AssumeRolePolicyDocument": {
        "Version": "2012-10-17",
        "Statement": [
          {
            "Effect": "Allow", # esta pol√≠tica significa que es un rol de permisi√≥n
            "Principal": {
              "Federated": "cognito-identity.amazonaws.com" # El rol se asgina a suarios federados que vienen de cognito
            },
            "Action": "sts:AssumeRoleWithWebIdentity", # Es para que un usuario obtenga dicho rol temporalmente mediante su JWT Token de sts con duraci√≥n limitada
            "Condition": {
              "StringEquals": {
                "cognito-identity.amazonaws.com:aud": "REGION:IDENTITY_POOL_ID" # Solo se puede usar el token si viene de ese cognito pool
              }
            }
          }
        ]
      },
      "Description": "Rol para acceder al dataset de Empresas 2024 en DPV"
    }
    ```

  - Se crea el rol en IAM de aws:

    ```python
    import boto3
    import json

    iam = boto3.client('iam')

    role_name = 'DPV_DataAccess_Colectivo1234'

    policy_document = {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Action": [
                    "redshift-data:ExecuteStatement",         # Ejecutar SELECTs y otros SQL
                    "lakeformation:GetDataAccess",            # Permiso de LakeFormation para consultar datos
                ],
                "Resource": "*"
            }
        ]
    }
    response = iam.create_role(
        RoleName=role_name,
        AssumeRolePolicyDocument=json.dumps(assume_role_policy),
        Description="Rol para acceder a dataset Empresas 2024"
    )

    print("Rol creado:", response['Role']['Arn'])
    ```

Esos fueron los flujos principales del microservicio de registration-service

Cabe aclarar que las interacciones entre los componentes de este microservicio se realizar√°n por medio de REST APIs. Por lo que cada uno de ellos estar√° escritos en FastAPI y recibir√° las solicitudes por medio de dicha interf√°z. Para cada componente se tendr√° un archivo con los endpoints y la l√≥gica del api, y otros con la l√≥gica de negocio de cada uno.

**4. key-management-service**

El key-management-service es un componente clave del bioregistro, ya que se encarga de la creaci√≥n y distribuci√≥n de las llaves en el esquema tripartito.

Durante el registro de una empresa, el servicio genera una Key Encryption Key (KEK) para cada parte involucrada: una para los representantes, otra para el administrador de la empresa, y una tercera para Data Pura Vida.

Estas KEKs se env√≠an directamente a los usuarios y no se almacenan en la base de datos del sistema, lo cual desacopla el proceso de encriptaci√≥n del acceso a los datasets de la empresa, permitiendo as√≠ client-side encryption.

En el habr√°n los siguientes componentes:

- KeyManagementController: Expone los endpoints del servicio para que el API General y otros microservicios puedan acceder a √©l, estos ser√°n:
- /encrypt/collective: Recibe el Token UUID desde el registration-service.
- /encrypt/verify/user: Por medio de este endpoint el usuario representante manda su kek para su aprobaci√≥n.
- /encrypt/verify/admin: Por medio de este endpoint el usuario administrador manda su kek para aprobar a un representante.
- EncryptionManager: Este componente se encarga del proceso de encripci√≥n.
- DecryptionManager: Este componente se encarga del proceso de desencriptado.
- Generator: Este componente se encarga de generar las DEKs y KEKs.
  - Verificator: Se encarga de verificar a un representante.

A continuaci√≥n algunos flujos del microservicio que muestr√°n cuando y donde se usa. Primeramente, el proceso de generaci√≥n de KEKs y DEKs.

1. Llega el request a creaci√≥n desde el registration-service:

- Por medio de POST /encrypt/collective

  ```json
  {
    "token": "El mismo Token UUID de redis"
  }
  ```

- el KeyManagementController pasa el control al Generator.

- Con dicho token se saca el UUID que se encuentra en redis por medio de: collective-register:<TOKEN_UUID>.

2. El UUID es obtenido exitosamente

- Ya con dicho UUID, se busca en la tabla de SumSubCollectiveApplicant para poder encontrar cual es la empresa y cual es el usuario administrador. De ah√≠ tambi√©n se revisa en SumSubCollective para verificar cuales son los usuarios representantes a los que se les desea asignar una KEK.

- Se obtienen los Ids de los usuarios representantes en la tabla de PersonaF√≠sica, y el del administrador de la empresa.

3. Creaci√≥n de keys

- El Generator llama al EncryptionManager por medio del API de FastAPI que posee y le env√≠a los representantes para que sepa cuantas KEKs/DEKs debe generar:

  ```json
  {
    "representatives": "[Los ids en la base de datos de dichos usuarios]"
  }
  ```

- Cabe aclarar que el proceso de encripci√≥n a utilizar es un AES-GCM, que posee la robustes de AES y adem√°s da un tag que dice la validez de la encripci√≥n, para evitar que se hagan modificaciones (es como un checksum)

  ```Python
  from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
  from cryptography.hazmat.backends import default_backend
  import os
  import base64

  # Funci√≥n para cifrar la DEK con un KEK usando AES-GCM
  def encrypt_dek_with_kek(dek: bytes, kek: bytes):
      iv = os.urandom(12)  # de 96 bits
      encryptor = Cipher(
          algorithms.AES(kek),
          modes.GCM(iv),
          backend=default_backend()
      ).encryptor()

      ciphertext = encryptor.update(dek) + encryptor.finalize()

      return { # se pasan a base64 para poder ser transmitidos en htttp
          'iv': base64.b64encode(iv).decode(),
          'ciphertext': base64.b64encode(ciphertext).decode(),
          'tag': base64.b64encode(encryptor.tag).decode()
      }

  def generar_tripleta_deks(representatives):

      # 1. Generaci√≥n de clave maestra
      dek = os.urandom(32)  # se usan 256 bits

      # 2. Se crean KEKs para la empresa y data pura vida
      kek_empresa = os.urandom(32)
      kek_dpv = os.urandom(32)

      # 3. Cifrar la DEK con cada KEK
      data_empresa = {
          "dek": encrypt_dek_with_kek(dek, kek_empresa),
          "kek": kek_empresa
      }

      data_dpv = {
          "dek": encrypt_dek_with_kek(dek, kek_dpv),
          "kek": kek_dpv
      }

      # 4. Se crean las distintas keks y deks para los representantes
      data_representatives = []
      for elem in representatives:
          kek = os.urandom(32)
          data_representatives = {
              "id" = elem,
              "kek" = kek,
              "dek" = encrypt_dek_with_kek(dek, kek)
          }

      # 5. Se retorna los resultados
      return {
          'representantes': data_representatives,
          'empresa': data_empresa,
          'dpv': data_dpv,
      }
  ```

- Esta return lo obtiene el controller del EncryptionManager y genera un jsondump al cual le aplica codificaci√≥n en base64 para que pueda ser pasado por medio de http.

4. Distribuci√≥n y Guardado:

- Ya con el resultado del EncryptionManager el Generator se encarga de generar un mensaje por medio de RabbitMQ al notification-service para que envie por correo a los usuarios tanto representantes como el admin su kek.

- Ya que se distribuyeron las KEKs se devuelven los DEKs al registration service para que as√≠ pueda terminar el registro.

5. finaliz√≥ el proceso de creaci√≥n de llaves tripartitas

Ahora, el otro punto importante en el key-management-service es el proceso de verificaci√≥n de KEKs para poder aprobar un usuario representante.

1. Interacci√≥n del usuario representante:

- Desde el frontend hace un POST /encrypt/verify/user

  ```json
  {
    "user_kek": "kek del usuario en base64"
  }
  ```

- Luego de esto el KeyManagementController enruta al Verificator para que se encargue de primero que todo obtener el id del usuario de la tabla de Representantes, y crea una entrada en redis (del mismo modo que con los tokens UUID en el registration-service) con un TTL de 48 horas:

  ```redis
    check_kek:<TOKEN_UUID> : [<ID_DEL_USUARIO>, <KEK_DEL_USUARIO>]
  ```

- Posteriormente se env√≠a un mensaje por RabbitMQ al notification-service para que env√≠e un mensaje "push" a las notificaciones dentro del portal web al administrador de la empresa que diga "El usuario <USUARIO> est√° esperando su aprobaci√≥n>", adem√°s en dicho mensaje se adjunta el TOKEN_UUID, para que posteriormente se vuelva enviar desde el frontend.

2. Interacci√≥n del administrador

- Desde el frontend hace un POST /encrypt/verify/user

  ```json
  {
    "admin_kek": "kek del usuario en base64",
    "token": "token uuid en redis"
  }
  ```

- Luego de esto el KeyManagementController enruta al Verificator para que se encargue de obtener todo de redis por medio del token.

- Una vez se obtiene la kek del usuario representante se saca la kek de Data pura vida desde DEKDataPuraVida para as√≠ empezar el proceso de validaci√≥n de keks.

  ```python
  from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
  from cryptography.hazmat.backends import default_backend
  import base64

  def decrypt_dek_with_kek(encrypted_data: dict, kek: bytes):

      iv = base64.b64decode(encrypted_data['iv'])
      ciphertext = base64.b64decode(encrypted_data['ciphertext'])
      tag = base64.b64decode(encrypted_data['tag'])

      decryptor = Cipher(
          algorithms.AES(kek),
          modes.GCM(iv, tag),
          backend=default_backend()
      ).decryptor()

      return decryptor.update(ciphertext) + decryptor.finalize()}



  def verify_keks(kek_user, kek_admin, kek_dpv, dek_user, dek_admin, dek_dpv ):

      dek1 = decrypt_dek_with_kek(dek_user, base64.b64decode(kek_user))
      dek2 = decrypt_dek_with_kek(dek_admin, base64.b64decode(kek_admin))
      dek3 = decrypt_dek_with_kek(dek_dpv, base64.b64decode(kek_dpv))

      if dek1 == dek2 == dek3:
          print("Aprobado test tripartita")
      else:
          print("Fall√≥ el test tripartita")
  ```

3. En caso de que las tres llaves coincidan entonces se aprueba la validaci√≥n y se actualiza el estado del representante en Postgres a Aprobado. Adem√°s se comunica con rabbitMQ y el notification-service para que envie un correo al usuario para que sepa que su kek fue aprobado.

Esos fueron los flujos principales del microservicio de key-management-service.

Cabe aclarar que las interacciones entre los componentes de este microservicio, cuando no se hizo explicita en la explicaci√≥n, es porque se realizar√°n por medio de REST APIs. Por lo que cada uno de ellos estar√° escrito en FastAPI y recibir√° las solicitudes por medio de dicha interf√°z. Para cada componente se tendr√° un archivo con los endpoints y la l√≥gica del api, y otros con la l√≥gica de negocio de cada uno.

**5. notification-service**

Este componente es el encargado de desacoplar la l√≥gica de notificaci√≥n a usuarios, ya sea por medio de correos electr√≥nicos o notificaciones internas de la aplicaci√≥n, del resto del microservicio.

No expone ninguna interfaz HTTP para comunicarse con otros microservicios; todo su tr√°fico se gestiona exclusivamente a trav√©s de colas en RabbitMQ. Las colas que utilizar√° son las siguientes:

- manual-verification: Por ac√° se reciben mensajes para poder notificar al backoffice que deben aprobar manualmente una empresa.
- register: Para enviar correos con el link al registro una vez identity-verification-service haya terminado la validaci√≥n de usuarios.
- mfa-mail: Por esta cola se reciben solicitudes de generar correos con el pin para MFA.
- send-token: Para poder reenviar un token_uuid del identity-verification-service.
- send-kek: Para enviar por correo las keks.
- verify-kek: Para enviar una notificaci√≥n a traves de las notificaciones dentro de la p√°gina, para que un administrador apruebe un usuario.
- approve-dek: Para notificarle al key-management-service que apruebe el estado del usuario en Representantes a Approve.

Cabe aclarar que los componentes del bioregistro no publicar√°n mensajes directamente en las cola, se usar√° una estructura de exchange como la siguiente:

![image](img/bioexchange.png)

De esta forma se desacopla a√∫n m√°s la comunicaci√≥n entre los otros componentes y notification-service, para que as√≠ en caso de ser necesarias modificaciones a la aquitectura en un futuro, el proceso sea m√°s flexible.

Ahora bien, para realizar el env√≠o de correos electr√≥nicos se usar√° AWS SES, y se habilitar√° en us-east-1 y se le configurar√° un dominio especial del equipo de soporte de data pura vida.

Una vez configurado AWS SES desde la consola de aws se tendr√°n que definir plantillas en html para los distintos tipos de correo. A continuaci√≥n de la estructura para un correo de register:

```html
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Confirmaci√≥n de Registro</title>
  </head>
  <body style="font-family: Arial, sans-serif;">
    <h2>¬°Bienvenido/a a Data Pura Vida!</h2>
    <p>Hola {{ nombre }},</p>
    <p>Tu proceso de verificaci√≥n ha sido aprobado exitosamente.</p>
    <p>Pod√©s ingresar al sistema usando el siguiente enlace:</p>
    <p>
      <a
        href="{{ link }}"
        style="padding: 10px 15px; background-color: #008f39; color: white; text-decoration: none;"
        >Acceder a la plataforma</a
      >
    </p>
    <p>Gracias por confiar en nosotros.</p>
    <p>‚Äî El equipo de Data Pura Vida</p>
  </body>
</html>
```

Luego, una vez seleccionada la plantilla html se enviar√° el correo de la siguiente manera:

```python
import boto3
from botocore.exceptions import ClientError

def render_verification_email(nombre, link):
    template = env.get_template("verification_approved.html")
    return template.render(nombre=nombre, link=link)

def send_email(to_address, subject, body_html, body_text):
    client = boto3.client('ses', region_name='us-east-1')
    try:
        response = client.send_email(
            Source='supporteam@datapuravida.com',
            Destination={'ToAddresses': [to_address]},
            Message={
                'Subject': {'Data': subject},
                'Body': {
                    'Text': {'Data': body_text},
                    'Html': {'Data': body_html}
                }
            }
        )
        print("Correo enviado:", response['MessageId'])
        return response
    except ClientError as e:
        print("Error al enviar correo:", e.response['Error']['Message'])
        raise
```

En cuanto al manejo de notificaciones dentro de la aplicaci√≥n web, el flujo en el notification-service ser√° el siguiente:

1. Cuando llega un mensaje a las colas manual-verification o verify-kek:

- Existe un webhook en la API de FastAPI que a su vez consume mensajes de esas colas.

- Si el usuario est√° activo (conectado v√≠a WebSocket), el webhook env√≠a la notificaci√≥n directamente a trav√©s de la conexi√≥n abierta.

- Si el usuario no est√° activo, el webhook no puede enviar la notificaci√≥n en tiempo real, por lo que guarda el mensaje en una tabla DynamoDB llamada "Notifications".

- Cuando el usuario se conecta, el webhook consulta la tabla "Notifications" para verificar si hay notificaciones pendientes para ese usuario.

- En caso de encontrar notificaciones, las recupera, las env√≠a al usuario y luego las elimina de DynamoDB. Si no hay notificaciones, no se realiza ninguna acci√≥n adicional.

### Diagramas de Clases

En esta secci√≥n se presentar√°n los distintos diagramas de clase correspondientes a cada microservicio descrito en la secci√≥n anterior. Para cada uno se explicar√° adem√°s cuales patrones de dise√±o fueron implementados. Adem√°s, cabe aclarar que en algunos microservicios aparecer√°n clases que ya se hab√≠an utilizado en otros. A nivel del diagrama, estas clases se muestran duplicadas para mayor claridad, pero en el c√≥digo ser√°n reutilizadas.

**1. identity-verification-service**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Amarillo: Representa un observer.
- Naranja: Representa un dependency injection.
- Verde: Representa un strategy.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el SumSubController, que act√∫a como facade para que otros microservicios y el API general del backend se comuniquen con este microservicio. Este controlador delega las llamadas a un observer mediante el EventManager, encargado de notificar a la l√≥gica de negocio correspondiente seg√∫n el tipo de llamada realizada al SumSubController.

Dentro de esa l√≥gica se encuentran el WebHookProcessor, PersonService y CollectiveService, que reciben como dependencias los servicios de la segunda capa de facade.

En esta segunda capa se encuentran:

- RabbitMQMessager: abstrae el env√≠o de mensajes al exchange del bioregistro.
- SumSubService: encapsula toda la comunicaci√≥n con el sistema externo de SumSub.
- ApplicantService: se encarga de las operaciones sobre los aplicantes en RDS.
- TokenManager: gestiona la generaci√≥n de tokens y su almacenamiento en Redis.
- DocumentManager: cambia din√°micamente entre S3Factory y DynamoFactory para guardar documentos.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesBioregistro1.png)

**2. auth-service**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Amarillo: Representa un observer.
- Naranja: Representa un dependency injection.
- Verde: Representa un strategy.

Ahora bien, las clases tienen un funcionamiento muy simple, el punto de entrada es AuthController, que act√∫a como facade para que el API general pueda acceder al microservicio. Luego el EventManager se encarga de distribuir seg√∫n lo que se pidio al AuthController. En este caso es el AuthChoiceHandler el que escucha, y decide cual es el tipo de login que se solicit√≥. Luego est√°n las clases de MFAService y CognitoService que se encargan de comunicarse con Cognito

![identity clases](img/ClasesBioregistro2.png)

**3. registration-service**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Amarillo: Representa un observer.
- Naranja: Representa un dependency injection.
- Verde: Representa un strategy.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el RegistrationController, que act√∫a como facade para que otros microservicios y el API general del backend se comuniquen con este microservicio. Este controlador delega las llamadas a un observer mediante el EventManager, encargado de notificar a la l√≥gica de negocio correspondiente seg√∫n el tipo de llamada realizada al RegistrationController.

Dentro de esa l√≥gica se encuentran el TokenManager, PersonRegistrationService y CollectiveRegistrationService, que reciben como dependencias los servicios de la segunda capa de facade.

En esta segunda capa se encuentran:

- RabbitMQMessager: abstrae el env√≠o de mensajes al exchange del bioregistro.
- Cognito: encapsula toda la comunicaci√≥n con Cognito.
- RegApplicantService: se encarga de las operaciones sobre los aplicantes en RDS.
- TokenCoordinator: gestiona la generaci√≥n de tokens y su almacenamiento en Redis.
- DocumentManager: cambia din√°micamente entre S3Factory y DynamoFactory para guardar y traer documentos.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesBioregistro3.png)

**4. key-management-service**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Amarillo: Representa un observer.
- Naranja: Representa un dependency injection.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el KeyManagementController, que act√∫a como facade para que otros microservicios y el API general del backend se comuniquen con este microservicio. Este controlador delega las llamadas a un observer mediante el EventManager, encargado de notificar a la l√≥gica de negocio correspondiente seg√∫n el tipo de llamada realizada al KeyManagementController.

Dentro de esa l√≥gica se encuentran el Generator y Verificator, que reciben como dependencias los servicios de la segunda capa de facade.

En esta segunda capa se encuentran:

- RabbitMQMessager: abstrae el env√≠o de mensajes al exchange del bioregistro.
- SumSubPersonService: encapsula las llamadas a las tablas de personas en RDS.
- EncryptionManager: se encarga de las operaciones de encripcion de DEkS y creacion de KEKs.
- DecryptionManager: se encarga de las operaciones Desencripcion de DEKs.
- DekService: gestiona el acceso a DEKs parciales en RDS.
- RedisController: Controla las operaciones de extraccion de UUIDs, guardado y salvado de KEKs.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesBioregistro4.png)

**5. notification-service**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Celeste: Muestra un factory.
- Verde: Representa un strategy.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

Se cuenta con una clase abstracta QueueListener que provee la l√≥gica y conexi√≥n a RabbitMQ. Esta clase es reutilizada por dos componentes principales:

- EmailSender: escucha mensajes destinados a ser reenviados por correo electr√≥nico a trav√©s de AWS SES utilizando el SESService.
- NotificationConsumer: detecta la llegada de nuevas notificaciones que deben mostrarse dentro de la aplicaci√≥n.

Adem√°s, existe una capa encargada de escuchar conexiones de usuarios al frontend para enviar notificaciones en tiempo real mediante el WebSocketController. En segundo plano, el NotificationConsumer verifica si llegan nuevas notificaciones. Si el usuario est√° conectado, se le muestran inmediatamente; de lo contrario, se almacenan en DynamoDB a trav√©s del NotificationManager, para que en la pr√≥xima conexi√≥n el WebSocketController se las muestre.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Cada conexi√≥n es manejada utilizando el patr√≥n Singleton.

![identity clases](img/ClasesBioregistro5.png)

### Servicios en AWS

A continuaci√≥n se presentan todos los servicios AWS con los que se operar√° en los microservicios del Bioregistro, adem√°s se listar√°n las configuraciones de hardware para cada uno

**EKS**
Ser√° el lugar donde estar√°n contenerizados los distintos microservicios.

- **Configuraci√≥n de Hardware:**
  - **Versi√≥n de Kubernetes:** 1.29 (o la m√°s reciente compatible).
  - **Tipo de nodo:** Amazon EC2.
  - **Tipo de instancia:** t3.medium (2 vCPU, 4 GB RAM) o superior.

**RDS**
Base de datos relacional para almacenar datos estructurados de la aplicaci√≥n. Se entrar√° en m√°s detalle en el dise√±o de los datos.

- **Configuraci√≥n de Hardware:**
  - **Motor:** PostgreSQL (o MySQL, MariaDB seg√∫n necesidad).
  - **Versi√≥n:** PostgreSQL 15 (o la m√°s reciente estable compatible).
  - **Tipo de instancia:** db.t3.medium (2 vCPU, 4 GB RAM) o superior.
  - **Almacenamiento:** General Purpose SSD (gp3) con tama√±o escalable seg√∫n la carga.
  - **Multi-AZ:** Activado para alta disponibilidad.

**DynamoDB**
Base de datos NoSQL escalable para almacenamiento de datos con acceso r√°pido y flexible. Se entrar√° en m√°s detalle en el dise√±o de los datos.

- **Configuraci√≥n:**
  - **Modo de capacidad:** On-Demand.
  - **Streams:** Habilitados para replicaci√≥n o integraci√≥n con otros servicios.

**S3**
Almacenamiento de objetos para archivos, backups y datos est√°ticos.

- **Configuraci√≥n:**
  - **Versionado:** Activado para control de versiones y recuperaci√≥n de datos.
  - **Lifecycle policies:** Para transici√≥n a almacenamiento m√°s barato (Glacier) o eliminaci√≥n autom√°tica.

**AWS SES**
Servicio para env√≠o de correos electr√≥nicos confiables y escalables.

- **Configuraci√≥n:**
  - **Regi√≥n:** us-east-1.
  - **Identidad verificada:** Dominios y correos electr√≥nicos verificados.
  - **Pol√≠ticas de env√≠o:** Limitaciones y tasas configuradas para evitar bloqueos.
  - **Autenticaci√≥n:** SPF, DKIM y DMARC configurados para mejorar entregabilidad.

**Amazon ElastiCache (Redis)**
Se usar√° para albergar el servicio de redis. Se entrar√° en m√°s detalle en el dise√±o de los datos.

- **Configuraci√≥n de Hardware:**
  - **Modo de cluster:** Cluster mode enabled para sharding o disabled para despliegues peque√±os.
  - **Versi√≥n:** Redis 7.x o la m√°s reciente estable compatible.
  - **Tipo de instancia:** cache.t3.medium (2 vCPU, 4 GB RAM) o superior.
  - **Multi-AZ:** Activado para alta disponibilidad (opcional).
  - **Seguridad:** VPC privada, grupos de seguridad restrictivos y cifrado en tr√°nsito y en reposo activados.

### Sistema de Monitoreo

El monitoreo del componente Bioregistro se implementar√° siguiendo una estrategia de observabilidad integral que permita supervisar en tiempo real el comportamiento, rendimiento y seguridad del microservicio. Esta estrategia se alinea con las tecnolog√≠as definidas en el stack tecnol√≥gico del proyecto.

**Arquitectura de Observabilidad**
El sistema de monitoreo se estructurar√° en tres pilares fundamentales que trabajar√°n de manera coordinada para proporcionar visibilidad completa del componente:

**1. M√©tricas y Rendimiento**
AWS CloudWatch ser√° el servicio central para recopilar y almacenar m√©tricas operacionales del Bioregistro. Se monitorizar√°n aspectos cr√≠ticos como:

- **M√©tricas de negocio:** Cantidad de registros procesados por tipo de entidad (personas f√≠sicas vs jur√≠dicas), tasa de √©xito en validaciones documentales, tiempo promedio del proceso completo de registro, y cantidad de llaves tripartitas generadas diariamente.
- **M√©tricas de infraestructura:** Utilizaci√≥n de recursos del pod en EKS (CPU, memoria, red), latencia de las conexiones a bases de datos PostgreSQL y DynamoDB, y throughput de solicitudes HTTP procesadas.
- **M√©tricas de integraci√≥n:** Disponibilidad y tiempo de respuesta de servicios externos como SumSub y Cognito, tasa de fallos en llamadas a APIs externas, y volumen de datos intercambiados con sistemas de terceros.

**Prometheus** complementar√° a **CloudWatch** recopilando m√©tricas espec√≠ficas del microservicio a trav√©s de un endpoint dedicado. Esto permitir√° obtener m√©tricas m√°s granulares sobre el comportamiento interno de la aplicaci√≥n, como contadores de operaciones espec√≠ficas, histogramas de distribuci√≥n de tiempos, y gauges para valores instant√°neos.

**2. Visualizaci√≥n y Dashboards**
**Grafana** se utilizar√° como plataforma principal de visualizaci√≥n, integr√°ndose tanto con CloudWatch como con Prometheus para crear dashboards interactivos que permitan:

- **Dashboard operacional:** Vista en tiempo real del estado general del Bioregistro, mostrando registros activos, distribuci√≥n geogr√°fica de solicitudes (verificando cumplimiento de restricci√≥n de IPs costarricenses), y estado de salud de todos los componentes.
- **Dashboard de validaciones:** Monitoreo espec√≠fico del proceso de validaci√≥n documental, incluyendo √©xito/fallo de verificaciones con SumSub, tiempos de procesamiento de IA para validaci√≥n de documentos, y distribuci√≥n de tipos de documentos procesados.
- **Dashboard de seguridad:** Seguimiento de eventos relacionados con seguridad como intentos de acceso no autorizados, generaci√≥n y uso de llaves criptogr√°ficas, y auditor√≠a de accesos a datos sensibles seg√∫n requerimientos de la Ley 8968.

**3. Logs y Trazabilidad**
El sistema de logging aprovechar√° **CloudWatch Logs** para centralizar todos los registros generados por el Bioregistro. Se implementar√° un esquema de logging estructurado que facilite:

- **Trazabilidad completa:** Cada transacci√≥n tendr√° un identificador √∫nico de correlaci√≥n que permitir√° seguir su flujo desde el inicio del registro hasta la emisi√≥n de credenciales digitales.
- **Auditor√≠a regulatoria:** Logs espec√≠ficos para cumplimiento normativo, registrando accesos a datos personales, modificaciones de informaci√≥n sensible, y ejercicio de derechos ARCO por parte de los usuarios.
- **Diagn√≥stico de problemas:** Niveles de log diferenciados (INFO, WARN, ERROR) con contexto suficiente para identificar r√°pidamente la causa ra√≠z de cualquier incidencia.

**Sistema de Alertas y Notificaciones**
Se configurar√° un sistema proactivo de alertas utilizando CloudWatch Alarms que notificar√° al equipo de operaciones cuando se detecten condiciones an√≥malas:
**Alertas cr√≠ticas (respuesta inmediata requerida):**

- Fallo total del servicio o indisponibilidad del endpoint de health check
- Tasa de error superior al 20% en ventana de 5 minutos
- Fallo en la conexi√≥n con servicios cr√≠ticos (Cognito, SumSub, bases de datos)
- Detecci√≥n de m√∫ltiples intentos de acceso desde IPs no autorizadas

**Alertas de advertencia (revisi√≥n prioritaria):**

- Degradaci√≥n del rendimiento con latencias superiores a 3 segundos
- Uso de recursos por encima del 80% de capacidad
- Incremento inusual en validaciones fallidas
- Acumulaci√≥n de tareas en cola de procesamiento manual

**Alertas informativas (seguimiento regular):**

- Resumen diario de m√©tricas operacionales
- Reporte semanal de tendencias y patrones
- Notificaciones de mantenimiento programado

**Monitoreo de Cumplimiento y Seguridad**
Dado el car√°cter sensible de los datos manejados por el Bioregistro, se implementar√°n controles espec√≠ficos de monitoreo para garantizar el cumplimiento normativo:

- **Seguimiento de consentimientos:** Monitoreo del ciclo de vida de los consentimientos otorgados por usuarios, incluyendo fechas de otorgamiento, actualizaciones y revocaciones.
- **Auditor√≠a de accesos:** Registro detallado de todos los accesos a datos personales, identificando qui√©n accedi√≥, cu√°ndo, desde d√≥nde y con qu√© prop√≥sito.
- **Monitoreo de retenci√≥n de datos:** Seguimiento automatizado de los per√≠odos de retenci√≥n de datos seg√∫n las pol√≠ticas establecidas, con alertas para datos pr√≥ximos a expirar.
- **Verificaci√≥n de cifrado:** Monitoreo continuo del estado de cifrado de datos en tr√°nsito y reposo, asegurando que no existan brechas de seguridad.

**Health Checks y Disponibilidad**
El microservicio implementar√° m√∫ltiples niveles de verificaci√≥n de salud que ser√°n monitoreados continuamente:

- **Liveness probe:** Verificaci√≥n b√°sica de que el servicio est√° activo y respondiendo, ejecutada cada 10 segundos por Kubernetes.
- **Readiness probe:** Verificaci√≥n comprehensiva de que todas las dependencias est√°n disponibles y el servicio puede procesar solicitudes, incluyendo conectividad con bases de datos, servicios externos y disponibilidad de recursos.
- **Deep health checks:** Verificaciones peri√≥dicas m√°s exhaustivas que validan la integridad de configuraciones, certificados SSL, y correcta operaci√≥n de funciones cr√≠ticas.

**An√°lisis y Mejora Continua**
El sistema de monitoreo no solo detectar√° problemas, sino que proporcionar√° insights para la mejora continua:

- An√°lisis de tendencias: Identificaci√≥n de patrones en el uso del sistema para optimizar recursos y predecir necesidades futuras.
- Detecci√≥n de anomal√≠as: Uso de las capacidades de CloudWatch para identificar comportamientos inusuales que podr√≠an indicar problemas emergentes.
- Reportes de capacidad: Proyecciones basadas en datos hist√≥ricos para planificar el crecimiento de la infraestructura.
- Optimizaci√≥n de costos: An√°lisis del uso de recursos para identificar oportunidades de optimizaci√≥n sin comprometer el rendimiento.

### Modelo de seguridad detallado

El m√≥dulo de Bioregistro maneja informaci√≥n altamente sensible relacionada con personas naturales y jur√≠dicas (incluyendo representantes legales, personas con poder legal, etc.). Su backend ser√° asegurado mediante un conjunto de mecanismos orientados a prevenir accesos no autorizados, garantizar integridad, confidencialidad, trazabilidad y disponibilidad continua de los datos.

**1. Control de Acceso Granular**

**OAuth2 + JWT:** Toda operaci√≥n sobre el bioregistro requerir√° un token v√°lido con permisos espec√≠ficos. Estas herramientas ser√°n implementado en el frontend por parte de AWS Cognito, pero su flujo de trabajo seguir√° en el backend.

**RBAC (Role Based Access):** Se le otorgar√° permisos a los usuarios seg√∫n el rol que desempe√±en dentro del sistema; esto con el fin de limitar acceso a solo los recursos necesarios y evitar privilegios excesivos. Existiran 4 tipos de roles:

| Rol del Usuario | Descripci√≥n                                                      | Permisos sobre recursos del Bioregistro |
| --------------- | ---------------------------------------------------------------- | --------------------------------------- |
| `bio:viewer`    | Visualiza registros existentes                                   | Lectura en PostgreSQL y DynamoDB        |
| `bio:editor `   | Crea y modifica registros, sin aprobarlos                        | Lectura y escritura parcial             |
| `bio:approver`  | Aprueba, certifica o valida registros                            | Escritura total + validaci√≥n cruzada    |
| `bio:admin`     | Gesti√≥n completa del m√≥dulo, incluyendo usuarios y configuraci√≥n | Acceso total y eliminaci√≥n              |

- La equivalencia de estos roles en la base de datos se puede mapear de la siguiente manera:
  - Viewer: Son los usuarios en la tabla llamada AccesoDataset.
  - Editor: Esto hace referencia a los representantes de los colectivos, ubicados en la tabla de Representantes.
  - Approver: Hace referencia a los administradores del colectivo, se ubican en la tabla de cada colectivo.

**Asociacion de RBAC a las bases de datos del sistema:**

- **PostgreSQL:** Usado para almacenar entidades estructuradas.

  - Personas f√≠sicas/jur√≠dicas, Certificados, Estados de validaci√≥n, Trazas de auditor√≠a
  - Se usan los roles exactamente como en la tabla anterior.
  - En la capa de acceso, se verifica el rol antes de ejecutar consultas SQL.

- **DynamoDB:** Usado para gestionar metadatos din√°micos y documentos JSON no estructurados.
  - Informaci√≥n adjunta, Historial de verificaci√≥n, Pruebas de vida o firmas electr√≥nicas.
  - En cada tabla DynamoDB, los accesos se segmentan con pol√≠ticas AWS IAM condicionales seg√∫n el rol (Condition: "bio:role" == "approver").

Ejemplo flujo autenticaci√≥n:

```json
{
  "sub": "uuid",
  "email": "usuario@dominio.com",
  "custom:role": "bio:editor"
}
```

**AWS Identity and Access Management (IAM):** Permite definir de manera segura qui√©n puede acceder a qu√© recursos y con qu√© nivel de permisos dentro del entorno en la nube. La implementaci√≥n se har√° con con pol√≠ticas por rol, usando etiquetas.

| Caso de uso                            | Acci√≥n permitida                | Servicio AWS    | Rol asociado                 |
| -------------------------------------- | ------------------------------- | --------------- | ---------------------------- |
| **Consulta de certificados validados** | `Vdynamodb:GetItem`, `Query`    | DynamoDB        | `bio:viewer`, `bio:approver` |
| **Carga de archivos adjuntos**         | `s3:PutObject`, `GetObject`     | Bucket S3       | `bio:editor`, `bio:approver` |
| **Lectura de llaves privadas**         | `secretsmanager:GetSecretValue` | Secrets Manager | `bio:admin`                  |
| **Acceso a logs de auditor√≠a**         | `logs:FilterLogEvents`          | CloudWatch Logs | `bio:admin`, `bio:approver`  |

Ejemplo politica por rol:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": ["dynamodb:PutItem", "dynamodb:GetItem"],
      "Resource": "arn:aws:dynamodb:::table/bioregistro_certificados"
    },
    {
      "Effect": "Allow",
      "Action": ["s3:PutObject"],
      "Resource": "arn:aws:s3:::bio-adjuntos/*"
    }
  ]
}
```

**3. Validaci√≥n de entradas**

Para proteger el backend del Bioregistro, se implementar√°n validaciones estrictas de datos en todas las capas de entrada. Estas validaciones estar√°n directamente integradas en los endpoints de la API desarrollados con FastAPI, usando las capacidades de tipado y validaci√≥n de Pydantic, reforzadas con validadores personalizados.

La aplicaci√≥n de esto sucede en los siguientes eventos:

- En todos los endpoints RESTful del Bioregistro (registro, modificaci√≥n, eliminaci√≥n, consulta).
- En validaciones internas antes de realizar operaciones sobre la base de datos.

**Validaciones estructurales:**

- Uso de tipos estrictos: `str`, `int`, `EmailStr`, `UUID`, `datetime`.
- Validaciones de longitud y formato (Regex).

Ejemplo de validaci√≥n estructural:

```python
from pydantic import BaseModel, Field, EmailStr
class RegistroResidente(BaseModel):
    cedula: str = Field(..., regex=r'^\d{9}$')
    nombre: str = Field(..., max_length=60)
    correo: EmailStr
    telefono: str = Field(..., regex=r'^\d{8}$')
    fechaNacimiento: str
```

**Prevenci√≥n de Inyecciones:**

- **SQL Injection:** al usar ORMs o query builders con `SQLALCHEMY`, evitando la concatenaci√≥n de strings en queries. -**NoSQL Injection:** validaci√≥n de claves primarias/secundarias con tipos y formatos v√°lidos.

**Validadores personalizados:**
Se emplear√°n funciones decoradoras (@validator) para definir reglas de negocio complejas

ejemplo:

```python
from pydantic import validator
class Registro(BaseModel):
    fechaNacimiento: datetime
    fechaDefuncion: Optional[datetime]

    @validator("fechaDefuncion")
    def check_fechas(cls, v, values):
        if v and "fechaNacimiento" in values and v < values["fechaNacimiento"]:
            raise ValueError("La fecha de defunci√≥n no puede ser anterior a la fecha de nacimiento.")
        return v
```

**4. Auditor√≠a y Registro de Actividades**
Se har√° con el objetivo de monitorear en tiempo real y registrar de forma persistente todos los accesos, modificaciones y acciones cr√≠ticas sobre los datos del Bioregistro, tanto por parte de usuarios humanos como de servicios automatizados.

**Acciones que se auditar√°n**

- Inicios y cierres de sesi√≥n con Cognito
- Accesos exitosos y fallidos a endpoints sensibles del backend
- Cambios de configuraci√≥n y roles dentro del sistema
- Accesos o intentos de acceso a recursos restringidos
- Uso de claves KMS para cifrado/descifrado de datos sensibles
- Acciones administrativas sobre recursos de AWS vinculados al Bioregistro

**Implementaci√≥n T√©cnica**

#### Middleware de FastAPI

Se desarrollar√° un middleware de auditor√≠a personalizado que capture metadatos clave en cada interacci√≥n:

- IP de origen
- Usuario autenticado
- Timestamp
- Endpoint accedido
- M√©todo HTTP
- C√≥digo de respuesta (status code)
- Rol del usuario

Los registros se almacenar√°n en DynamoDB, aprovechando su alto rendimiento y consulta eficiente para logs estructurados.

Ejemplo:

```json
{
  "PK": "user#1234",
  "SK": "log#2025-06-05T17:42:13Z",
  "endpoint": "/residente/456",
  "action": "UPDATE",
  "statusCode": 403,
  "ip": "190.10.25.6",
  "role": "PersonalAutorizado"
}
```
#### AWS CloudWatch

Se tiene una visualizaci√≥n en tiempo real de los logs generados por el backend. Se realiza la creaci√≥n de alarmas automatizadas para eventos sospechosos o violaciones de pol√≠ticas como las siguientes:

- M√°s de 5 intentos fallidos de autenticaci√≥n en 60 segundos.
- Acceso masivo a datos de residentes por un mismo usuario.
- Actividades fuera del horario laboral.

#### Integraci√≥n con AWS CloudTrail

Para capturar eventos directamente desde el entorno AWS, se utilizar√° AWS CloudTrail como complemento de auditor√≠a. Este registra todas las llamadas a la API de AWS, incluyendo:

- Uso de AWS KMS
- Acceso a buckets S3 con documentos biom√©tricos
- Cambios a roles, pol√≠ticas y grupos de IAM

Beneficios:

- Trazabilidad completa de acciones en servicios cr√≠ticos del backend
- Integraci√≥n con AWS KMS para detectar uso indebido de claves
- Compatible con otros servicios de AWS para ejecutar consultas avanzadas sobre logs

**5. Cifrado de Datos**
El m√≥dulo Bioregistro maneja informaci√≥n sensible relacionada con la identidad de los residentes, como n√∫meros de identificaci√≥n, datos biom√©tricos y documentos oficiales. Por ello, se implementa lo siguiente:

| Tipo de Cifrado         | Descripci√≥n                                                                                                     | Aplicaci√≥n en el Bioregistro                                                                                                              | Herramientas/Protocolos Usados                         | Caso de Uso                                                                                                                                                                         |
| ----------------------- | --------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Cifrado en Tr√°nsito** | Protege los datos mientras se transmiten entre el cliente y el servidor. Evita intercepciones o manipulaciones. | Aplicado en todas las solicitudes HTTP entre frontend y backend, y entre backend y servicios como Amazon Cognito, PostgreSQL y RabbitMQ.. | TLS 1.3, HTTPS obligatorio con AWS Certificate Manager | La adopci√≥n de HTTPS obligatorio ser√° gestionada mediante certificados v√°lidos y renovables (por ejemplo, con AWS Certificate Manager).                                             |
| **Cifrado en Reposo**   | Protege los datos almacenados en bases de datos o archivos para evitar acceso no autorizado.                    | Aplica al almacenamiento de c√©dulas, datos biom√©tricos y documentos subidos a S3 o PostgreSQL.                                            | AES-256, PostgreSQL TDE, S3 + SSE-KMS                  | Para almacenamiento de documentos e im√°genes biom√©tricas en Amazon S3, se aplicar√° cifrado del lado del servidor (SSE) con claves gestionadas por AWS Key Management Service (KMS). |

**Uso de AWS KMS para Gesti√≥n de Claves**

AWS KMS permitir√° la centralizaci√≥n del manejo de claves de cifrado, incluyendo:

- Rotaci√≥n autom√°tica de claves
- Control de acceso por pol√≠tica granular
- Auditor√≠a completa mediante integraci√≥n con AWS CloudTrail
- Cada operaci√≥n de cifrado y descifrado queda registrada, permitiendo trazabilidad sobre qu√© usuario accedi√≥ a qu√© recurso, cu√°ndo y con qu√© clave.

**Protecci√≥n Extendida**

Se combinar√° cifrado del lado del cliente con el cifrado del lado del servidor, especialmente en flujos sensibles como subida de documentos biom√©tricos desde el frontend. Esto permite que los datos ya lleguen cifrados a S3, agregando una capa adicional de defensa en caso de vulneraci√≥n de acceso al bucket.

**Verificaci√≥n de Implementaci√≥n**

Se integran pruebas autom√°ticas en los pipelines de CI para asegurar que:

- Ninguna transmisi√≥n ocurra por HTTP.
- Los datos almacenados no est√©n en texto plano.
- Las operaciones de cifrado sean exitosas y rastreables en CloudWatch.

Estas medidas aseguran la confidencialidad de los datos personales y fortalecen la postura de cumplimiento del proyecto con respecto a normativas como la Ley 8968 de Protecci√≥n de la Persona frente al tratamiento de sus datos personales y est√°ndares como ISO/IEC 27001.

**6. Protecci√≥n contra Abuso y Ataques**

| Categor√≠a                         | Estrategia                                                               | Herramienta / Tecnolog√≠a              | Caso de uso                                                                                     |
| --------------------------------- | ------------------------------------------------------------------------ | ------------------------------------- | ----------------------------------------------------------------------------------------------- |
| **Limitaci√≥n de tr√°fico**         | Aplicar l√≠mites de solicitudes por IP por endpoint y m√©todo.             | AWS API Gateway + FastAPI Middleware  | Evitar que un usuario o bot consulte masivamente los datos de residentes en un corto periodo.   |
| **Bloqueo por patrones**          | Identificaci√≥n de IPs con comportamiento malicioso y bloqueo autom√°tico. | AWS WAF                               | Bloqueo de IPs que intenten manipular repetidamente URLs como `/residente/1234/edit`            |
| **Protecci√≥n contra bots**        | Detecci√≥n de bots mediante an√°lisis de headers y frecuencia.             | Middleware personalizado + WAF        | Evita scraping autom√°tico de datos personales o intentos de acceso automatizado al registro.    |
| **Protecci√≥n DoS/Brute Force**    | Prevenci√≥n de ataques de denegaci√≥n de servicio o fuerza bruta en login. | AWS Shield + CloudWatch               | Resguarda el endpoint de autenticaci√≥n Cognito usado por el personal autorizado.                |
| **Validaci√≥n profunda de inputs** | Inspecci√≥n de JSON y par√°metros de URL para detectar inyecciones         | Pydantic + validadores personalizados | Prevenir que usuarios maliciosos inserten comandos o scripts en campos como nombre o direcci√≥n. |

**7. Gesti√≥n de Secretos con AWS Secrets Manager**

Se usar√° AWS Secrets Manager como proveedor principal para almacenar, cifrar y rotar autom√°ticamente los secretos necesarios del backend. Este servicio permite:

- Cifrado autom√°tico con AWS KMS de los valores sensibles.
- Control de acceso detallado mediante pol√≠ticas IAM por recurso.
- Auditor√≠a completa con AWS CloudTrail.
- Integraci√≥n directa desde FastAPI usando AWS SDK (boto3).

| Nombre del Secreto                 | Contenido                                             | Servicio                | Rotaci√≥n Autom√°tica   |
| ---------------------------------- | ----------------------------------------------------- | ----------------------- | --------------------- |
| `bioregistro/db_credentials`       | Usuario y contrase√±a para acceder a PostgreSQL        | PostgreSQL              | Activada cada 30 d√≠as |
| `bioregistro/jwt_signing_key`      | Llave privada para firmar JWT                         | FastAPI auth middleware | Solo lectura          |
| `bioregistro/rabbitmq_credentials` | Usuario y contrase√±a para conectarse a RabbitMQ       | RabbitMQ (eventos)      |                       |
| `bioregistro/s3_upload_token`      | Token temporal para subida de archivos desde frontend | S3 + Cognito            | 12h de disponibilidad |

Ejemplo de acceso seguro desde FastAPI

```py
import boto3
import json

def get_secret(secret_name):
    client = boto3.client("secretsmanager", region_name="us-east-1")
    response = client.get_secret_value(SecretId=secret_name)
    return json.loads(response['SecretString'])

db_creds = get_secret("bioregistro/db_credentials")
DB_USER = db_creds["username"]
DB_PASS = db_creds["password"]
```

Ejemplo de politicas de secretos con AWS IAM

```json
{
  "Effect": "Allow",
  "Principal": {
    "AWS": "arn:aws:iam::123456789012:role/bioregistro-backend-role"
  },
  "Action": "secretsmanager:GetSecretValue",
  "Resource": "*"
}
```

**8. Procedimiento de Recuperaci√≥n ante Incidente**

1. Detecci√≥n del incidente mediante alertas de CloudWatch.
2. Validaci√≥n del √∫ltimo snapshot v√°lido en RDS o versi√≥n del objeto en S3.
3. Restauraci√≥n autom√°tica desde consola de AWS Backup, RDS o S3.
4. Notificaci√≥n y verificaci√≥n de consistencia posterior al recovery.
5. Registro de incidente en CloudWatch Logs.

### Elementos de alta disponibilidad

**1. Replicaci√≥n y Multi-AZ en Bases de Datos**

Para asegurar la continuidad operativa del sistema en caso de fallos, se configur√≥ una topolog√≠a Master-Slave en Amazon RDS con PostgreSQL, la cual opera en la regi√≥n us-east-1 y se distribuye autom√°ticamente entre m√∫ltiples zonas de disponibilidad. Esta configuraci√≥n est√° activa en todo momento y permite realizar un failover autom√°tico hacia una r√©plica en caso de que el nodo principal presente fallas.

**2. Almacenamiento Seguro y Distribuido**

El almacenamiento de documentos legales y biom√©tricos se realiza en Amazon S3, mientras que DynamoDB se configura con respaldo continuo mediante Point-in-Time Recovery. Estos mecanismos se activan cada vez que se cargan o modifican datos, y garantizan una recuperaci√≥n confiable en caso de p√©rdidas o errores.

| Recurso    | Tecnolog√≠a    | Implementaci√≥n                             | Activaci√≥n            | Ubicaci√≥n                 |
| ---------- | ------------- | ------------------------------------------ | --------------------- | ------------------------- |
| Documentos | **Amazon S3** | Versionado y replicaci√≥n cruzada semanal   | Al cargar o modificar | `us-east-1` / `us-west-1` |
| Metadatos  | **DynamoDB**  | Backup continuo con Point-in-Time Recovery | En cada escritura     | `us-east-1`               |

**3. Estrategias Avanzadas de Monitoreo y Alertas**

La supervisi√≥n del backend se lleva a cabo en tiempo real gracias a **AWS CloudWatch** y **Prometheus**, que operan dentro del cl√∫ster `AWS EKS` donde residen los microservicios. Estas herramientas recogen m√©tricas de uso, disponibilidad e integridad del sistema y emiten alertas inmediatas ante comportamientos inusuales. Grafana nos permite visualizar esta informaci√≥n mediante dashboards.

| Tecnolog√≠a     | Rol                                                                             | Donde se ejecuta                    | Momento de ejecuci√≥n                |
| -------------- | ------------------------------------------------------------------------------- | ----------------------------------- | ----------------------------------- |
| **CloudWatch** | Captura m√©tricas y logs de servicios AWS                                        | Servicios AWS                       | En tiempo real y continuo           |
| **Prometheus** | Recoge m√©tricas internas de microservicios a trav√©s de endpoints personalizados | Dentro del cl√∫ster EKS              | Cada vez que se actualizan m√©tricas |
| **EKS**        | Aloja los microservicios del backend y los componentes de monitoreo             | AWS (regi√≥n `us-east-1`)            | Siempre activo durante operaci√≥n    |
| Grafana        | Visualiza datos recolectados para an√°lisis y diagn√≥stico                        | Conectado a CloudWatch y Prometheus | Monitoreo continuo                  |

**4. Sistema Automatizado de Backups**

Para afrontar p√©rdidas de informaci√≥n se implementaron respaldos autom√°ticos diarios en **RDS** y **DynamoDB** a las 2:00 a.m. y respaldos cruzados semanales en buckets de **S3** en la regi√≥n `us-west-1`. Esto se encuenta en ejecuci√≥n continua desde el backend de AWS y se activa sin intervenci√≥n manual.

**5. Balanceo de Carga y Escalabilidad**

El tr√°fico al backend es manejado por un Application Load Balancer de AWS, configurado dentro del cl√∫ster de EKS, que distribuye solicitudes entrantes de forma equitativa entre las instancias disponibles. Esta capa de balanceo est√° activa permanentemente y escala autom√°ticamente cuando se detecta una sobrecarga, garantizando alta disponibilidad incluso cuando hay picos de trafico.

**6. Cifrado y Protecci√≥n de Accesos**

Todo el backend se envuelve en pol√≠ticas de seguridad implementadas con AWS KMS. Como fue mencionado en secciones anteriores, se cifra la informaci√≥n en tr√°nsito y en reposo, y mediante roles IAM que limitan el acceso a los servicios seg√∫n el contexto del microservicio. Estas medidas se aplican durante cualquier operaci√≥n de lectura o escritura sobre S3, RDS o DynamoDB, y est√°n definidas para ejecutarse en todos los componentes alojados en la regi√≥n `us-east-1`.

**7. Recuperaci√≥n R√°pida ante Desastres**

En caso de incidentes que afecten la disponibilidad del sistema, la recuperaci√≥n se realiza autom√°ticamente con el uso de snapshots y versionado en RDS, S3 y DynamoDB. Se activan inmediatamente tras la detecci√≥n de fallas, que seran alertadas por CloudWatch. Esto permite devolverse al estado original usando los backups de otras regiones.

### Diagrama del backend

A continuaci√≥n se presenta el diagrama del backend del Bioregistro. En √©l se evidencia c√≥mo todo el ecosistema de AWS interact√∫a con los distintos microservicios desplegados en el cl√∫ster de Kubernetes provisto por EKS. Tambi√©n se describen los microservicios internos junto a sus distintas clases, los patrones de dise√±o utilizados, y c√≥mo interact√∫an entre s√≠.

Se muestra c√≥mo la contenerizaci√≥n de cada microservicio se realizar√° utilizando Docker, y c√≥mo el monitoreo interno ser√° gestionado por Prometheus. Adem√°s, se destaca que en la capa externa a AWS se encuentra SumSub, utilizado como sistema de terceros.

![image](img/DiagramaBackendBioregistro.svg)

## Dise√±o de los Datos

### Topolog√≠a de Datos

- **Tipo:** Base de Datos Replicada tipo OLTP, Almacenamiento de Objetos, Base de datos documental
  - Vamos a utilizar RDS con PostgreSQL como almacenamiento OLTP de los usuarios y sus distintos tipos. Se usar√° un m√≥delo master-slave con 2 read replicas en us-east-1 . Adem√°s se activar√° el Multi-AZ failover para permitir pasar el rol de master a una de replica lista para failover, esto nos dar√° alta disponibilidad. Estos respaldos se har√°n todos los d√≠as a las 2 de la ma√±ana de costa rica y se guaradar√°n en un S3 bucket de respaldos.
  - Utilizaremos un S3 Bucket como almacenamiento de objetos para guardar PDFs y documentos legales sobre las organizaciones.
  - Usaremos DynamoDB como base de datos documental, en ella se almacenar√° la metadata correspondiente a los documentos en el S3, y tambi√©n los distintos datos no estructurados que tienen los distintos colectivos. No utilizaremos los servicios de Global Tables ya que el acceso al sistema es principalmente desde Costa Rica. Por lo que solo usaremos 1 region: us-east-1.
  - Cabe aclarar que el Id para las personas f√≠sicas ser√° el mismo en Cognito y RDS, mientras que el Id de los colectivos ser√° el mismo tanto en RDS como en Dynamo.
  - Tambi√©n se implementar√° el uso de Redis por medio de Amazon Elastic Cach√©. Se usar√° el modo Clustered para garantizar mayor escalamiento, y se configurar√° dentro de la misma VPC de los microservicios del Bioregistro, para que as√≠ seolo pueda ser accedida desde ah√≠.
- **Tecnolog√≠a Cloud**:

  - RDS
  - DynamoDB
  - CloudWatch: Para el monitoreo de dichos servicios de AWS

- **Pol√≠tcias y Reglas**:

  - Single-region: Solo se usar√° una regi√≥n para RDS y DynamoDB, us-east-1
  - Backups autom√°ticos: Tanto RDS como Dynamo har√°n backups autom√°ticos a las 2 de la ma√±ana y lo subir√°n a un S3.
  - Backups cruzados: Para proteger los respaldos en caso de que la regi√≥n de aws caiga (poco probable), se cargaran adicionalmente en un S3 Bucket en us-west-1. Esto se har√° cada semana los viernes a las 2 de la ma√±ana, ya que su prioridad es menor.
  - Failover Autom√°tico: Dynamo tiene nativamente integrado la opci√≥n de hace failover. Con RDS se lograr√° por medio del uso de Multi-AZ.

- **Beneficios**:
  - Mantenernos en una sola regi√≥n, reduce la latencia entre replicas de las bases de datos y baja el costo de la arquitectura.
  - Tener backups autom√°ticos, con regiones cruzadas permite tener el sistema listo para enfrentar cualquier ca√≠da en aws.
  - Postgres es una Base de Datos open source por lo que no hace falta pagar licensias.
  - DynamoDB es de las opciones de BD documental m√°s veloces, adem√°s est√° completamente integrada con el ecosistema de aws, por lo que hacer respaldos o sacarle m√©tricas es muy sencillo.
  - DynamoDB est√° respaldado por AWS, por lo que ofrece un SLA del 99.999% y es 100% compatible con el resto de nuestros servicios en AWS.

### Tenency, Seguridad y Privacidad

- **Modelo**: Singe-Access-Point

  - En este caso la base de datos para los usarios y organizaciones no ser√° multi-tenant ya que al ser un almacenamiento de registros no es necesario.

  - Para acceder a la base de datos, implementaremos un Singe-Access-Point mediante una clase llamada RDSRepository.

  - No se implementar√° validaci√≥n mediante tokens JWT ni se utilizar√° Multi-Schema en esta base de datos, ya que est√° l√≥gicamente aislada del resto del sistema, eliminando as√≠ cualquier posibilidad de intrusi√≥n o acceso no autorizado a datos sensibles o datasets.

  - Tambi√©n usaremos IAM de AWS para que los servicios solo puedan acceder a lo que deben. Por ejemplo solo el microservicio que guarda en la BD puede usar el kms:Encrypt en RDS y Dynamo.

  - La base de datos en RDS no almacenar√° contrase√±as, ya que su prop√≥sito no es gestionar el inicio de sesi√≥n, sino mantener un registro estructurado de las entidades registradas en la plataforma.

  - El manejo de la encripci√≥n de las DEKs est√° a cargo del key-management-service, sin embargo, a dicha encripci√≥n tambi√©n se le aplicar√° el encryption at rest.

- **Encripci√≥n**:

  - Metadata de las Organizaciones (detallada al inicio del cap√≠tulo de Bioregistro).
  - Emails de los usuarios.
  - Informaci√≥n de contacto de usuarios y organizaciones.
  - Configuraciones de pago.

- **Cloud**:

  - Amazon Cognito para el registro de personas f√≠sicas.
  - Amazon RDS para PostgreSQL con RLS.
  - Encryption at rest en DynamoDB gracias a AWS KMS
  - Encryption at rest en RDS gracias a AWS KMS
  - Encryption at rest en el S3 Buckets con SSE-S3, para que AWS KMS gestione las claves y cifrado
  - AWS IAM.

- **Beneficios**:
  - Gracias a que solo abr√° un single point of acces regulado con AWS IAM, la intrusi√≥n de un tercero a la base de datos de usuarios es muy poco probable.
  - En caso de que algui√©n tenga acceso a la base de datos no podr√° hacer nada ya que todo est√° encriptado.
  - Cognito permite manejar de formar eficiente y agil el registro de personas f√≠sicas.

### Conexi√≥n a Base de datos

- **Modelo**: Transaccional v√≠a Statements / Store Procedures y ORM

Usaremos SQLAlchemy como ORM para interactuar con PostgreSQL dentro de la aplicaci√≥n. Adem√°s se usar√°n Store Procedures para operaciones m√°s complejas como registrar a una organizaci√≥n y hacer las relaciones pertinentes con personas f√≠sicas.

- **Patrones de POO**:

  - Factory: Usamos el patr√≥n Factory para la creaci√≥n de las clases RDSFactory, RDSRepository, DynamoFactory, DynamoRepository.

- **Beneficios**:

  - El c√≥digo es independiente del motor de base de datos relacional, lo que permite cambiarlo f√°cilmente si es necesario.
  - El desarrollo es m√°s √°gil que escribir SQL puro.
  - Se protege contra vulnerabilidades como SQL Injection.
  - Se puede garantizar el cumplimiento de las propiedades ACID.

- **Pool de Conexiones**
  Usaremos el pool integrado en SQLAlchemy (QueuePool), el cual es din√°mico. El tama√±o base del pool ser√° de 10 conexiones, y podr√° escalar hasta 15 conexiones simult√°neas.

  - **Beneficios**:
    - La escalabilidad se ajusta bajo demanda.
    - Proporciona mayor estabilidad en ambientes productivos.

- **Drivers**
  Para PostgreSQL utilizaremos el driver nativo psycopg2, integrado con SQLAlchemy, lo cual ofrece mejor rendimiento. Para DynamoDB y S3 emplearemos boto3, un cliente interpretado ampliamente soportado en el ecosistema AWS.

  - **Beneficios**:
    - Aprovechamos lo mejor de cada entorno: para PostgreSQL un driver nativo r√°pido, y para DynamoDB/S3 un driver interpretado m√°s port√°til y flexible.

### Diagrama de Base de Datos

A continuaci√≥n se presenta el diagrama de base de datos correspondiente al m√≥dulo de bioregistro. En √©l se muestra c√≥mo se gestionan tanto las personas f√≠sicas como los colectivos, incluyendo una relaci√≥n muchos a muchos que permite registrar qu√© personas representan a cada colectivo. Para clasificar los tipos de colectivo, se utiliza una tabla cat√°logo.

Aunque en RDS los colectivos comparten una estructura general para mantener la base simple y normalizada, es importante destacar que estos pueden tener campos espec√≠ficos seg√∫n su tipo. Por ejemplo, los ministerios no poseen c√©dula jur√≠dica, a diferencia de otros colectivos. Por esta raz√≥n, se decidi√≥ almacenar la metadata variable de cada colectivo en DynamoDB, utilizando el mismo id que en RDS. Esto permite extender la informaci√≥n sin preocuparse por rigidez en el schema, manteniendo flexibilidad sin perder trazabilidad entre sistemas.

Adem√°s, un aspecto clave es el manejo de las llaves en el esquema tripartito. La KEK del usuario se almacena en una tabla que guarda las claves correspondientes a los representantes de la empresa, junto con un indicador que marca si ya fueron validadas mediante el proceso de aprobaci√≥n tripartita. Esto permite que, una vez aprobadas, los usuarios no necesiten autenticarse cada vez que acceden a la cuenta empresarial.

Por su parte, la empresa almacena su propia KEK directamente en su tabla correspondiente, mientras que existe una tabla espec√≠fica que asocia las KEKs de Data Pura Vida con cada empresa registrada.

![image](img/DiagramaBDBioregistro.png)

# 4.2 La B√≥veda

El componente de La B√≥veda constituye el n√∫cleo de la plataforma, ya que es el responsable del almacenamiento y gesti√≥n de todos los datasets. Este m√≥dulo permite alojar datasets con distintos niveles de acceso y monetizaci√≥n, incluyendo opciones de descarga gratuita, pago √∫nico, suscripciones por cuotas, y configuraciones de visibilidad p√∫blica o privada.

Uno de los aspectos m√°s cr√≠ticos de La B√≥veda es la seguridad. Solo los usuarios que hayan adquirido un dataset o que hayan sido aprobados expl√≠citamente pueden acceder a su contenido. El nivel de resguardo es tan alto que ni siquiera los administradores del sistema tienen acceso directo a los datos almacenados, garantizando as√≠ una separaci√≥n estricta de responsabilidades y m√°xima confidencialidad.

Adicionalmente, este componente debe ser altamente resiliente y escalable, ya que se espera una carga intensiva tanto en volumen de datos como en frecuencia de acceso. La infraestructura debe soportar cargas en batch de datos, as√≠ como consultas anal√≠ticas complejas, sin comprometer la integridad ni el rendimiento del sistema.

Por estas razones, en esta secci√≥n se detalla el dise√±o de La B√≥veda con √©nfasis en los aspectos de seguridad, escalabilidad, resiliencia y eficiencia, pilares fundamentales para asegurar su correcto funcionamiento dentro del ecosistema de la plataforma.

En cuanto a como se planean guardar los datasets a continuaci√≥≈Ñ se muestra un diagrama con la vista a alto nivel:

![image](img/BovedaAltoNivel.png)

En la imagen se puede ver c√≥mo se debe tener un almacenamiento central, el cual debe estar dividido l√≥gicamente para los colectivos. Ya despu√©s cada colectivo tendr√° sus datasets guardados junto con sus tablas, que en ocasiones pueden ser usadas en datasets distintos. M√°s adelante se ver√° c√≥mo es que este enfoque ser√° logrado.

## Dise√±o del Backend

El backend de este componente es m√°s simple, ya que la mayor parte del peso cae en la estructura de la base de datos y su modelo de seguridad. En esta secci√≥n solo se comentar√°n dos funcionalidades: la funcionalidad de trazabilidad y c√≥mo el API interact√∫a con Redshift.

**Trazabilidad:**

En este componente es sumamente importante que se tenga trazabilidad de qui√©n ejecuta qu√© tarea. Por lo que, al hacer un query a Redshift, siempre se adjuntar√° como variable de sesi√≥n el ID de Cognito de la persona que pidi√≥ el query.

Luego, gracias a la funcionalidad de Amazon Redshift logs, se podr√° ver detalladamente la informaci√≥n del query de la siguiente forma:

| Nombre de columna | Descripci√≥n                                         |
| ----------------- | --------------------------------------------------- |
| recordtime        | Hora en la que ocurri√≥ el evento.                   |
| db                | Nombre de la base de datos.                         |
| user              | Nombre de usuario.                                  |
| pid               | ID del proceso asociado con la sentencia.           |
| userid            | ID del usuario.                                     |
| xid               | ID de la transacci√≥n.                               |
| query             | Un prefijo `LOG:` seguido del texto de la consulta. |

Ahora bien, estos logs sirven tanto para tener un registro de qu√© consultas se han hecho (para m√°s adelante dar contexto Agentes sobre como consultar un dataset), como para llevar cuotas de uso de datasets que se usan por cuotas definidas.

Por ello, cuando estos logs lleguen a AWS CloudWatch, se disparar√° una Lambda Function que se encargue de obtener el query y tambi√©n el ID del usuario que se sete√≥ en la consulta. Luego, se revisar√° si el dataset al que se le realiz√≥ la consulta es de tipo cuota, y en dado caso se va a la tabla de cuota en RDS para hacer la deducci√≥n.

Para el resto de los casos (incluyendo cuando el dataset es de tipo cuota), se insertar√° el query en un √≠ndice de OpenSearch, el servicio gestionado de Elasticsearch que ofrece AWS. Gracias a la naturaleza de time series de Elastic Search, podremos almacenar estos logs de manera eficiente, organiz√°ndolos en √≠ndices mensuales por dataset con el formato de nombre `DATASETNAME-YYYY-MM`.
Esto evitar√° almacenamiento masivo y poco escalable t√≠pico de motores SQL, y adem√°s funcionar√° como una fuente sencilla para que los agentes de IA puedan alimentarse con los queries asociados a cada dataset.

A continuaci√≥n un ejemplo de c√≥digo de como se puede realizar dicha lambda function:

```python
import json
import re
import base64
import gzip
from sqlalchemy import create_engine, Column, Integer, ForeignKey
from sqlalchemy.orm import sessionmaker, declarative_base

DATABASE_URL = "postgresql+psycopg2://usuario:password@host:puerto/dbname"

engine = create_engine(DATABASE_URL)
Session = sessionmaker(bind=engine)
Base = declarative_base()

class Cuotas(Base):
    __tablename__ = "Cuotas"
    Id = Column(Integer, primary_key=True)
    IdDataset = Column(Integer, ForeignKey("Dataset.Id"), nullable=False)
    CuotasRestantes = Column(Integer, nullable=False)

def lambda_handler(event, context):
    mensaje = event['awslogs']['data']

    #Es un zip, entonces viene por json en base64, por lo que se decodifica y luego extrae el contenido
    compressed_payload = base64.b64decode(mensaje)
    uncompressed_payload = gzip.decompress(compressed_payload)
    payload_json = json.loads(uncompressed_payload)

    session = Session()
    try:
        for log_event in payload_json["logEvents"]:
            message_text = log_event["message"]

            #Se busca datasetId en la query con regex
            match = re.search(r'/\*dataset:(\d+)\*/', message_text)
            if not match:
                continue
            dataset_id = int(match.group(1))

            ######
            # Aqu√≠ se revisa RDS para saber si es un dataset de pago por cuotas, y luego se hace
            # la inserci√≥n en el √≠ndice correspondiente de Opensearch
            ######

            cuota = session.query(Cuotas).filter(Cuotas.IdDataset == dataset_id).first()
            if cuota and cuota.CuotasRestantes > 0:
                cuota.CuotasRestantes -= 1
                session.commit()
                print(f"Cuotas restantes para dataset {dataset_id}: {cuota.CuotasRestantes}")
            else:
                print(f"No hay cuotas restantes o dataset {dataset_id} no encontrado")
    except Exception as e:
        session.rollback()
        print(f"Error procesando logs: {e}")
    finally:
        session.close()
```

**Interacci√≥n con API:**

M√°s adelante se ver√° c√≥mo se implementa el RBAC en el sistema, pero el API tambi√©n proveer√° un tipo de RBAC por l√≥gica. Se har√° de la siguiente forma:

1. Llega consulta desde el frontend

```json
{
  "jwt": "el token de sesi√≥n del usuario",
  "dataset": "nombre del dataset"
}
```

- Con el JWT se obtiene el ID de Cognito de la persona.

2. Proceso de autorizaci√≥n

- Primero se revisa que el usuario sea parte del colectivo propietario del dataset:

  - En caso de que s√≠, entonces se le asigna el IAM Role correspondiente al colectivo y se pasa el query a la creaci√≥n de queries.

- Ahora bien, si el usuario no es propietario, entonces se revisa la tabla de AccesoDataset para ver si tiene acceso al dataset.

  - Si no, entonces se rechaza la solicitud.

- Ahora bien, si no se rechaza, entonces se revisa si el dataset es por cuotas en la tabla de DatasetDePago; en caso de que s√≠, se revisan las cuotas restantes en la tabla de Cuotas.

  - Si no quedan, entonces se rechaza la conexi√≥n.

- Ya luego, si se pas√≥ todo el proceso de autorizaci√≥n, se le asigna el rol de IAM correspondiente al dataset por medio de un STS que sirva solo para esa consulta.

### Servicios en AWS

Se mencionar√°n solo los servicios de AWS que a√∫n no han sido descritos en alg√∫n punto de la arquitectura. En caso de querer ver todos los servicios de AWS que se usar√°n y como se desplegar√°n por medio de terraform ir a dicha secci√≥n despu√©s de la explicaci√≥n de los microservicios.

**AWS Lambda:**
Para las funciones serverless que obtienen informaci√≥n de los datasets y la loguean.

**Configuraci√≥n de Hardware:** Aunque no gestionamos hardware directamente, s√≠ configuraremos los recursos, como:

- **Memoria:** 1024 MB
- **Arquitectura:** arm64
- **Tiempo de ejecuci√≥n:** Node.js 22.x
- **Almacenamiento ef√≠mero:** 512MB
- **Tiempo de espera:** 5s
- **Retry attempts:** 1

**AWS OpenSearch:**

Para el sistema de logs para posterior entrenamiento de Agentes de IA.

**Configuraci√≥n del Dominio OpenSearch:**

- **Versi√≥n del motor**: OpenSearch 2.3
- **Tipo de instancia**: t3.small.search
- **Cantidad de instancias**: 2 (con zone awareness activado para alta disponibilidad)
- **Almacenamiento EBS**: 20 GB, tipo gp3
- **Encriptaci√≥n**:
  - En tr√°nsito (TLS 1.2 m√≠nimo)
  - En reposo (AES-256)
- **Seguridad**:
  - Autenticaci√≥n interna con usuario admin
  - Acceso v√≠a HTTPS obligatorio
  - Index Lifecycle Management (ILM):
- **Pol√≠tica de rollover**:
  - M√°ximo 5GB por √≠ndice
  - M√°ximo 1 d√≠a por √≠ndice
- **Retenci√≥n**:
  - Borrado autom√°tico de √≠ndices despu√©s de 30 d√≠as

## Dise√±o de los Datos

### Topolog√≠a de Datos

- **Tipo:** OLAP + OLTP

  - Para La B√≥veda se emplear√° un enfoque h√≠brido, utilizando una base de datos OLAP para el almacenamiento de los distintos datasets y una base de datos OLTP para toda la parte administrativa relacionada con personas, colectivos, cuotas y gesti√≥n de acceso a los datasets.
  - En cuanto al OLTP, como se describi√≥ previamente en la secci√≥n del Bioregistro, se utilizar√° una base de datos en RDS con PostgreSQL para almacenar la informaci√≥n de usuarios y colectivos. En esta secci√≥n se utilizar√° la misma base de datos, pero se agregar√°n nuevas tablas para registrar accesos a datasets, gestionar los registros de datasets y asociar tablas a los distintos datasets. Estas nuevas tablas ser√°n detalladas m√°s adelante en el diagrama correspondiente.
  - Para el almacenamiento OLAP se utilizar√° Amazon Redshift, un OLAP orientado a columnas, dise√±ado espec√≠ficamente para manejar grandes vol√∫menes de datos y consultas complejas a escala. Se optar√° por la versi√≥n Redshift Serverless, que permite el uso bajo demanda sin necesidad de configurar nodos, escalando autom√°ticamente seg√∫n la carga de trabajo. Esta versi√≥n tambi√©n replica autom√°ticamente los datos en tres zonas dentro del mismo Availability Zone y ofrece failover autom√°tico mediante snapshots. Adem√°s, se configurar√°n respaldos autom√°ticos incrementales los martes y viernes a la 1 a.m.
  - Se aprovechar√°n dos funcionalidades clave de Redshift: las Federated Queries, que permitir√°n consultar directamente las tablas administrativas almacenadas en RDS; y el almacenamiento interno de Redshift, que ofrece un modelo columnar altamente eficiente para los datos anal√≠ticos.
  - Cabe aclarar que no se detallar√°n las especificaciones t√©cnicas para RDS ya que fueron mencionadas previamente en el Bioregistro.

- **Tecnolog√≠a Cloud**:

  - RDS para PostgreSQL.
  - Amazon Redshift.

- **Pol√≠tcias y Reglas**:

  - Single-region: Solo se usar√° una regi√≥n para RDS y Redshift, us-east-1
  - Backups autom√°ticos: Tanto RDS como Redshift har√°n backups autom√°ticos a las 1 de la ma√±ana y lo subir√°n a un S3.
  - Backups cruzados: Para proteger los respaldos en caso de que la regi√≥n de aws caiga (poco probable), se cargaran adicionalmente en un S3 Bucket en us-west-1. Esto se har√° cada semana los viernes a las 2 de la ma√±ana, ya que su prioridad es menor.
  - Failover Autom√°tico: Redshift tiene nativamente integrado la opci√≥n de hace failover. Con RDS se lograr√° por medio del uso de Multi-AZ.

- **Beneficios**:
  - Usar RDS con PostgreSQL y Redshift dentro de la misma regi√≥n reduce la latencia entre los servicios y simplifica la arquitectura, lo que tambi√©n ayuda a controlar costos.
  - PostgreSQL es una base de datos open source, por lo que no se incurre en costos adicionales por licencias.
  - Amazon Redshift est√° optimizado para cargas masivas y consultas anal√≠ticas complejas, con escalabilidad autom√°tica en su versi√≥n serverless, lo que mejora el rendimiento sin necesidad de gestionar infraestructura.
  - Tanto RDS como Redshift est√°n plenamente integrados en el ecosistema AWS, facilitando la gesti√≥n de seguridad, monitoreo y respaldo con servicios nativos.
  - AWS garantiza altos niveles de disponibilidad y cumplimiento de SLA para ambos servicios, lo que aporta estabilidad y confiabilidad a la plataforma.
  - Redshift tiene integraci√≥n nativa con servicios como AWS Glue.
  - Redshift puede adherirse a RDS para interactuar con el facilmente:
    ```sql
    CREATE EXTERNAL SCHEMA rds_schema
    FROM POSTGRES
    DATABASE 'admin_db'
    URI 'my-db-instance.abc123xyz.us-east-1.rds.amazonaws.com'
    PORT 5432
    IAM_ROLE 'arn:aws:iam::cuenta-dpv:role/admin-dpv'
    SECRET_ARN 'arn:aws:secretsmanager:us-east-1:123456789012:secret:MySecret'
    ```
  - Redshift permite copiar en Batch archivos de Parquet desde un S3 y mapearlos a tablas en su almacenamiento interno:
    ```sql
      COPY esquema.tabla_destino
      FROM 's3://tu-bucket/ruta/a/parquets/'
      IAM_ROLE 'arn:aws:iam::cuenta-dpv:role/admin-dpv'
      FORMAT AS PARQUET;
    ```

### RLS

No se usar√° RLS ya que el acceso a datasets se hace por tablas, entonces una vez un usuario tenga acceso a un dataset, podr√° ver todo el contenido que este tenga; no habr√°n filas a las que est√© restringido. Nuestro dise√±o es seguro porque en una misma tabla solo se guarda informaci√≥n correspondiente a un solo colectivo. Puede ser que esa tabla se comparta entre datasets del colectivo, pero igual no pasa nada, dado que el acceso sigue siendo por tabla. En la siguiente secci√≥n se dir√° c√≥mo se gestiona el acceso por tabla.

### Tenency, Seguridad y Privacidad

- **Modelo**: Singe-Access-Point, RBAC, Multi-Tenant

  - El Single-Acess-Point ser√° obligatorio, ya que solo las clases de RDSRepository y RedshiftRepository podr√°n interactuar con los sistemas de bases de datos.
  - Se usar√° la opci√≥n de Multi-Tenant que ofrece Redshift, para de ese modo asignar un Schema distinto para que cada colectivo guarde sus datasets.
  - Para hacer el manejo de RBAC se usar√°n LakeFormation, IAM de AWS, y el API (Especificado en el Backend).
  - Para implementar el Lakeformation y IAM se usar√°n 2 tipos de roles generales que luego ser√°n divididos seg√∫n corresponda:
    - Rol IAM de Colectivo: Siempre que se crea un nuevo dataset se crean tags de LakeFormation para que den permiso de acceso a las tablas de dicho dataset, Luego esas tags se asignan al rol IAM. Lo tendr√°n los admin del colectivo y sus representantes.
    - Rol de Dataset: Se crear√° un rol de IAM espec√≠fico para cada dataset. Se crearan las tags del mismo modo que con el rol anterior, solo que en este caso el Rol de IAM ser√° especifico al acceso a las tablas del dataset. De esta manera cuando una persona acceda a un dataset solo podr√° si previamente se le asigno este rol.
  - Cuando se vayan a ejecutar las consultas en Redshfit, se agregar√° como variable de sesi√≥n a la consulta el Id del usuario en Cognito.
  - Cabe aclarar que hay un Rol de IAM por defecto asociado al tag de lakeformation "dataset=public-free", esto permite que cualquier usuario pueda ver un Dataset. Ser√° asignado a los datasets que son gratis y p√∫blicos.

- **Ejemplos**

  - A continuaci√≥n como es que se crean las tags con LakeFormation para acceder a tablas en Redshift:

    ```python
    import boto3

    client = boto3.client('lakeformation')

    # Se crea el nombre y key del tag
    client.create_lf_tag(
        TagKey='dataset',
        TagValues=['Colectivo1234']
    )

    #Se le asignan recursos de redshift
    client.assign_lf_tags_to_resource(
        Resource={
            'Table': {
                'CatalogId': 'AWS_ACCOUNT_ID',
                'DatabaseName': 'datalake',
                'Name': 'Tabla1'
            }
        },
        LFTags=[
            {
                'TagKey': 'dataset',
                'TagValues': ['Colectivo1234']
            }
        ]
    )
    ```

  - Ya en la secci√≥n de registration-service del bioregistro se espec√≠fico como crear un rol de IAM, ahora a continuaci√≥n se muestra como asignarle tags de LakeFormation:
    ```python
    client.grant_permissions(
        Principal={ # A que rol de IAM se asigna
            'DataLakePrincipalIdentifier': 'arn:aws:iam::YOUR_ACCOUNT_ID:group/DPV_DataAccess_Empresas2024'
        },
        Resource={ # Que tags de Lakeformation se le asiginan
            'LFTagPolicy': {
                'ResourceType': 'TABLE',
                'Expression': [
                    {
                        'TagKey': 'dataset',
                        'TagValues': ['empresas2024']
                    }
                ],
                'ResourceType': 'TABLE'
            }
        },
        Permissions=['SELECT', 'DESCRIBE'], #que pueden realizar sobre los datos
        PermissionsWithGrantOption=[]
    )
    ```
  - Ahora bien, un ejemplo de como se puede adjuntar como elemento de la Sesi√≥n el ID de cognito:

    ```python
    from sqlalchemy import create_engine, text

    #Crear el engine con los par√°metros de Redshift
    engine = create_engine("postgresql+psycopg2://usuario:contrase√±a@host:puerto/base_de_datos")

    cognito_id = 'abc-123-cognito-sub-id'

    with engine.connect() as conn:
        conn.executeu(text("SET session.cognito_user_id = :cognito_id"), {"cognito_id": cognito_id})

        result = conn.execute(text("SELECT * FROM public.algo LIMIT 10;"))
        for row in result:
            print(row)
    ```

- **Encripci√≥n**:

  - Toda la informaci√≥n estar√° encriptada gracias al Encryption at Rest

- **Cloud**:

  - Amazon RDS para PostgreSQL con RLS.
  - LakeFormation, para manejar los permisos modulares a tablas
  - AWS IAM, para dar permisos espec√≠ficos asociados con pol√≠ticas de LakeFormation
  - Encryption at rest en Redshift gracias a AWS KMS
  - Encryption at rest en RDS gracias a AWS KMS

- **Beneficios**:
  - Gracias a que solo abr√° un single point of acces, la intrusi√≥n de un tercero a la base de datos de usuarios es muy poco probable.
  - Gracias a que en la sesi√≥n de la consulta se adjunta el Id del usuario en Cognito, se tiene trazabilidad de quien ejecute que consulta.
  - Tener multi-tenant agrega una capa de separaci√≥n l√≥gica que evita que entre colectivos puedan acceder a espacios que no les corresponde.
  - En caso de que algui√©n tenga acceso a la base de datos no podr√° hacer nada ya que todo est√° encriptado.
  - Ya que para poder acceder a una tabla se ocupa tener el rol ef√≠mero de IAM que a su vez tenga los tags necesarios de LakeFormation, inclusive los administradores de la base de datos no van a poder ver que contenido tienen las tablas. Solo aquellos con los permisos podr√°n.
  - Gracias a que Redshfit internamente es una base de datos columnar las consultas pueden ser realizadas de manera m√°s eficiente ya que solo trae a memoria la informaci√≥n necesario y no todos los datos de un registro.

### Conexi√≥n a Base de datos

- **Modelo**: Transaccional v√≠a Statements / Store Procedures y ORM

Usaremos SQLAlchemy como ORM para interactuar con PostgreSQL y Redshift dentro de la aplicaci√≥n. Adem√°s se usar√°n Store Procedures para operaciones m√°s complejas como las consultas a datasets desde el m√≥dulo de Centro de visualizaci√≥n y Consumo.

- **Patrones de POO**:

  - Factory: Usamos el patr√≥n Factory para la creaci√≥n de las clases RDSFactory, RDSRepository, RedshiftFactory,RedshiftRepository.

- **Beneficios**:

  - El c√≥digo es independiente del motor de base de datos relacional, lo que permite cambiarlo f√°cilmente si es necesario. Se puede usar el mismo tanto para RDS como Redshift.
  - El desarrollo es m√°s √°gil que escribir SQL puro.
  - Se protege contra vulnerabilidades como SQL Injection.
  - Se puede garantizar el cumplimiento de las propiedades ACID.

- **Pool de Conexiones**
  Usaremos el pool integrado en SQLAlchemy (QueuePool), el cual es din√°mico. El tama√±o base del pool ser√° de 10 conexiones, y podr√° escalar hasta 15 conexiones simult√°neas.

  - **Beneficios**:
    - La escalabilidad se ajusta bajo demanda.
    - Proporciona mayor estabilidad en ambientes productivos.

- **Drivers**
  Para PostgreSQL y Redshift utilizaremos el driver nativo psycopg2, integrado con SQLAlchemy, lo cual ofrece mejor rendimiento.

  - **Beneficios**:
    - Para PostgreSQL un driver nativo r√°pido, se aprovecha lo mejor.

### Dise√±o para IA

**Implementaciones comunes a todas las tablas**

Dado que es imposible predecir con exactitud el contenido espec√≠fico de cada dataset, se aplicar√°n ciertas implementaciones transversales a todas las tablas de Redshift para facilitar su interpretaci√≥n por parte de agentes de AI:

- Cada registro deber√° incluir una columna llamada **CategoriaSemantica**, que permitir√° dar contexto sobre el tipo de informaci√≥n contenida en la tabla.
- Se a√±adir√° tambi√©n una columna de **Descripci√≥n**, destinada a ofrecer una breve explicaci√≥n sobre el contenido de cada fila, brindando a√∫n m√°s contexto sem√°ntico.
- Estas columnas ser√°n incorporadas autom√°ticamente durante el proceso del motor de transformaci√≥n.
- Se registrar√°n todos los logs hechos a Opensearch, donde los m√≥delos de inteligencia artificial podr√°n ver que consultas se le hacen a un dataset espec√≠fico, y de esta forma puedan dar resultados de mayor calidad.

**Justificaci√≥n**

La orientaci√≥n de **La B√≥veda** hacia un dise√±o habilitado a agentes de AI responde a las siguientes necesidades:

- Las consultas provenientes del centro de visualizaci√≥n y consumo podr√°n realizarse mediante lenguaje natural. Para que los agentes puedan generar consultas SQL adecuadas, necesitan contar con metadatos descriptivos y sem√°nticos que les permitan comprender la estructura y el contenido del dataset.
- En casos donde los datasets deban actualizarse tras una nueva carga o un cambio de esquema en la fuente original, los agentes deben tener suficiente contexto para redise√±ar el modelo o adaptar la carga posterior de forma correcta y aut√≥noma.
- Mantener un registro de las consultas en una base de datos de time series permitir√° proporcionar contexto actualizado y frecuente a los agentes de IA para futuras operaciones sobre los datasets.
- Al utilizar una base de datos time series, se garantiza que la informaci√≥n registrada sea siempre reciente y relevante, facilitando an√°lisis y respuestas m√°s precisas por parte de los agentes.

### Diagrama de Base de Datos

A continuaci√≥n se presenta el diagrama de base de datos correspondiente al m√≥dulo de La B√≥veda. En √©l se muestra c√≥mo se utiliza la misma base de datos de RDS que en el bioregistro, ya que su rol es meramente administrativo.

Se puede ver c√≥mo existe una tabla que almacena la informaci√≥n principal de los datasets, y que se asocia con PersonaFisica para poder revisar qui√©nes tienen acceso a dichos datasets. Adem√°s, para mantener el dise√±o normalizado, se tiene una tabla extra que guarda los datasets de pago y dice si son por cuota o pago √∫nico, adem√°s de su costo. La tabla de Cuotas que guarda cu√°ntas consultas le restan a un usuario con respecto a un dataset, y finalmente una tabla llamada tablas, ya que almacena a que tablas pertenece un dataset.

Con respecto a la estructura de Redshift, esta es imprescindible, por ello no se muestra en el diagrama; depender√° completamente de lo que suban los usuarios. Eso s√≠, definitivamente estar√° separada por schema para cada colectivo.

![image](img/DiagramaBDBoveda.png)

# 4.3 Centro de Carga

Este componente representa la primera etapa en el proceso de carga de datasets hacia La B√≥veda. Su funci√≥n principal es extraer datos desde m√∫ltiples fuentes, incluyendo:

- Archivos en formato:
  - CSV
  - Excel
  - JSON
- Conexiones a bases de datos:
  - SQL (MySQL, PostgreSQL, SQL Server y MariaDB)
  - MongoDB
- REST APIs externas

Todos los datos obtenidos se almacenan en estado crudo dentro de un bucket S3, sirviendo como punto de entrada para que el Motor de Transformaci√≥n los procese e inserte posteriormente en Redshift, el n√∫cleo anal√≠tico de La B√≥veda.

## Dise√±o del Frontend

### Arquitectura del Cliente

**Client-Side Rendering con Renderizado Est√°tico**

La arquitectura implementa **CSR** con contenido est√°tico servido desde **S3** y **CloudFront** como CDN. Los bundles de React generados durante el build se almacenan en buckets S3 y se distribuyen globalmente atrav√©s de CloudFront para optimizar latencia y disponibilidad.

**API √∫nica** desarrollada en **FastAPI** para toda la comunicaci√≥n backend, centralizando autenticaci√≥n, validaci√≥n y procesamiento de datos.

**Gesti√≥n de Estado Durante Uploads Largos**

- **Persistencia autom√°tica** en `localStorage` para mantener progreso de uploads pausables/resumibles entre sesiones del navegador
- **Recovery autom√°tico** tras desconexiones de red mediante detecci√≥n de sesiones interrumpidas y restauraci√≥n del estado exacto
- **Optimizaci√≥n de memoria** para archivos grandes procesando muestras de 10KB usando FileReader API

### Patrones de Dise√±o de Objetos

#### Chain of Responsibility - Procesamiento de Fuentes de Datos

El sistema debe manejar m√∫ltiples tipos de fuentes de datos (archivos, APIs, bases de datos) de manera flexible y extensible. Cada fuente requiere validaciones y procesamientos espec√≠ficos. Se activa al momento de seleccionar una fuente de datos en la interfaz de usuario.

**Implementaci√≥n en Frontend:**

- **FileHandler**: Procesa archivos locales (CSV, Excel, JSON) con validaci√≥n de formato, tama√±o y estructura mediante drag-drop o input
- **APIHandler**: Gestiona conexiones con APIs externas, incluyendo testing de conectividad y autenticaci√≥n OAuth2/API Keys
- **DatabaseHandler**: Maneja extracci√≥n directa de bases de datos con discovery de esquemas y selecci√≥n de tablas

#### Strategy Pattern - Configuraci√≥n por Tipo de Acceso

Los datasets requieren configuraciones completamente diferentes seg√∫n su nivel de acceso (p√∫blico, privado, pagado), con campos y validaciones espec√≠ficas para cada tipo. Se activa durante el evento `onChange` del selector de tipo de dataset en el wizard de configuraci√≥n.

**Implementaci√≥n en Frontend:**

- **PublicStrategy**: Muestra campos de descripci√≥n extendida y metadatos de catalogaci√≥n para datasets abiertos
- **PrivateStrategy**: Despliega controles granulares de permisos con listas de usuarios autorizados y configuraci√≥n institucional
- **PaidStrategy**: Presenta formularios de pricing, t√©rminos de uso y m√©todos de pago con validaci√≥n regulatoria

#### Builder Pattern - Configuraci√≥n Paso a Paso

La configuraci√≥n de datasets es compleja y requiere m√∫ltiples pasos con validaciones interdependientes. El patr√≥n Builder permite construir la configuraci√≥n gradualmente. Se activa durante la navegaci√≥n del wizard de configuraci√≥n, persistiendo autom√°ticamente cada paso.

#### Singleton/Facade - Gateway Centralizado

Se necesita un punto √∫nico de comunicaci√≥n con el backend para mantener consistencia en autenticaci√≥n, manejo de errores y gesti√≥n de conexiones. Se activa durante toda interacci√≥n con el backend, desde validaciones hasta uploads finales.

**Implementaci√≥n en Frontend:**

- **UploadGateway**: Instancia √∫nica que centraliza `sendChunk()`, `trackProgress()`, `validateFile()` y `saveConfiguration()`
- **Gesti√≥n de conexiones**: Pools HTTP reutilizables, manejo de tokens JWT y retry logic centralizado
- **Abstracci√≥n de complejidad**: Oculta m√∫ltiples endpoints backend detr√°s de m√©todos simples

#### Observer Pattern - Monitoreo Distribuido de Progreso

El progreso de upload debe actualizarse simult√°neamente en m√∫ltiples componentes de la interfaz sin acoplarlos directamente. Se activa durante todo el proceso de carga con actualizaciones en tiempo real.

**Implementaci√≥n en Frontend:**

- **UploadProgressObserver**: Actualiza barras de progreso con porcentajes y tiempo estimado
- **UIProgressObserver**: Actualiza componentes espec√≠ficos mediante referencias React
- **NotificationObserver**: Env√≠a alertas al sistema de messaging del usuario

#### Diagrama de Clases 
El diagrama muestra la integraci√≥n de todos los patrones de dise√±o implementados en el frontend. La arquitectura se organiza en **5 capas** claramente diferenciadas:

![alt text](image-1.png)

1. **Capa Singleton/Facade**: UploadGateway mantiene una instancia √∫nica para toda comunicaci√≥n con el backend, mientras ChunkUploadManager gestiona la fragmentaci√≥n de archivos grandes

2. **Capa Chain of Responsibility**: AbstractSourceHandler coordina el procesamiento secuencial de diferentes fuentes de datos (archivos, APIs, bases de datos)

3. **Capa Builder**: DatasetConfigBuilder construye configuraciones paso a paso mediante el wizard interactivo

4. **Capa Strategy**: ConfigurationStrategy y sus implementaciones (Public, Private, Paid) manejan las diferentes configuraciones de acceso

5. **Capa Observer**: ProgressTracker notifica a m√∫ltiples observers para actualizar la interfaz de usuario en tiempo real

### Componentes Visuales

#### Flujo de Interacci√≥n del Usuario

```
Selecci√≥n de fuente ‚Üí validaci√≥n formato/conectividad ‚Üí preview con an√°lisis IA ‚Üí
configuraci√≥n metadatos ‚Üí configuraci√≥n permisos ‚Üí procesamiento ETDL ‚Üí
monitoreo transformaci√≥n ‚Üí activaci√≥n dataset
```

Cabe aclarar que el flujo de procesamiento ETDL se realiza de forma as√≠ncrona, por lo que el usuario podr√° salirse del portal web y esperar a que le llegue una notificaci√≥n por correo, y las notificaciones propias de la aplicaci√≥n.

#### Componentes Principales

- **FileDropZone**: Capacidades drag-drop para archivos hasta 500MB con validaci√≥n autom√°tica y feedback visual inmediato
- **ConfigurationPanel**: Adaptativo que muestra opciones relevantes seg√∫n el tipo de fuente seleccionada
- **PreviewComponent**: Visualizaci√≥n tabular de primeras 1000 filas con an√°lisis autom√°tico de tipos de columnas
- **ProgressDisplay**: Monitoreo espec√≠fico del proceso ETDL con estados detallados

#### Validaci√≥n en Tiempo Real

- **Formato de archivo**: Validaci√≥n `onChange` con `mimeTypeValidator` ejecut√°ndose en menos de 100ms
- **Estructura de datos**: An√°lisis autom√°tico de headers CSV/Excel detectando columnas malformadas
- **Preview inteligente**: Integraci√≥n con endpoint `/ai/suggest-metadata` para sugerencias autom√°ticas de IA
- **Smart defaults**: Sugerencias autom√°ticas basadas en tipo de archivo y pol√≠ticas de seguridad

### Estructura de carpetas

```
centro-carga-frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/                  # Comunicaci√≥n con FastAPI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ uploadAPI.js      # Funciones para carga de archivos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validationAPI.js  # Validaciones server-side
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadataAPI.js    # Gesti√≥n de metadatos IA
‚îÇ   ‚îú‚îÄ‚îÄ models/               # Entidades de dominio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dataset.js        # Modelo principal de dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DatasetConfig.js  # Configuraci√≥n completa
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ UploadProgress.js # Estado de progreso detallado
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ValidationResult.js # Resultados de validaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ components/           # Atomic Design Pattern
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ atoms/            # Componentes b√°sicos reutilizables
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ UploadButton.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FileInput.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ProgressBar.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ValidationMessage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ molecules/        # Combinaciones de atoms
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ConfigItem.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ColumnPreview.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PricingPanel.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ FileMetadata.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ organisms/        # Componentes complejos
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FileDropZone.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DatasetConfigForm.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PreviewPanel.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ValidationSummary.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates/        # Layouts de p√°gina
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ UploadLayoutTemplate.jsx
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ WizardTemplate.jsx
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ConfigurationTemplate.jsx
‚îÇ   ‚îú‚îÄ‚îÄ hooks/                # Custom hooks (ViewModel)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useFileUpload.js  # Manejo completo de carga
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useDatasetConfig.js # Configuraci√≥n de datasets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useDataPreview.js # Preview y an√°lisis de datos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useMetadataAI.js  # Generaci√≥n autom√°tica IA
‚îÇ   ‚îú‚îÄ‚îÄ services/             # L√≥gica de negocio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ UploadManager.js  # Gesti√≥n de cargas (Singleton/Facade)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ValidationService.js # Validaciones (Chain of Responsibility)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MetadataExtractor.js # Extracci√≥n de metadatos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ProgressTracker.js # Seguimiento progreso (Observer)
‚îÇ   ‚îî‚îÄ‚îÄ utils/                # Utilidades compartidas
‚îÇ       ‚îú‚îÄ‚îÄ fileValidators.js # Validadores espec√≠ficos por tipo
‚îÇ       ‚îú‚îÄ‚îÄ formatters.js     # Formateadores de datos
‚îÇ       ‚îî‚îÄ‚îÄ constants.js      # Constantes del m√≥dulo
```

### Tecnolog√≠as Integradas

#### Stack Principal

- **React 18**: Base del frontend con hooks y context API
- **Tailwind CSS**: Styling siguiendo Atomic Design principles
- **Formik + Yup**: Formularios robustos con validaci√≥n completa
- **Plotly**: Visualizaci√≥n de datos durante preview
- **Axios**: HTTP client con interceptors configurados

#### Servicios AWS

- **S3**: Almacenamiento de bundles est√°ticos
- **CloudFront**: CDN global para servir el web app

### Diagrama del Frontend

![alt text](image.png)

## Dise√±o del Backend

### Microservicios del Componente

**1. dataset-upload-service**

Este servicio administra la carga inicial de datasets por parte de usuarios desde m√∫ltiples fuentes como archivos Excel, CSV, JSON, APIs o conexiones directas a bases de datos SQL y NoSQL.

Los componentes internos incluyen:

- **UploadController:** Expone los endpoints RESTful para gestionar las solicitudes de carga y configuraci√≥n inicial de datasets.

  - `/upload/dataset:` Permite subir archivos directamente.

  - `/upload/dataset/api:` Configura y prueba conexiones con APIs externas.

- **ValidationManager:** Realiza la validaci√≥n inicial de estructura, formato y tipo de archivos recibidos. Se utiliza un patr√≥n Strategy para validar los distintos tipos de datos. Esto Permitir√° que si en un futuro se quieren agregar nuevos tipos de fuentes sea posible de manera f√°cil

- **TemporaryStorageHandler:** Almacena temporalmente y de forma cifrada datasets en AWS S3 hasta su validaci√≥n y transformaci√≥n.

- **UploadFlowCoordinator:** Coordina el flujo completo desde la carga hasta la validaci√≥n y notificaci√≥n. Funciona como un patr√≥n Observer.

El flujo principal para cargar un dataset desde un archivo es el siguiente:

1. Inicio del proceso de carga:

- El frontend llama a `POST /upload/dataset`

```json
{
  "userId": "uuid-del-usuario",
  "datasetName": "Nombre del dataset",
  "fileType": "CSV",
  "fileContent": "base64-string"
}
```

- Antes de continuar cabe aclarar como es que se extraer√°n los datos:
  - Archivos CSV, JSON, Excel se pasaran a base64 para poder ser enviados por https y luego se almacenar√°n en el S3.
  - Para el caso de APIs es muy similar, porque es equivalente a pasar archivos en formato JSON.

- Para Mongo y las bases de Datos SQL es un tanto distinta la situaci√≥n, ya que se ocupan distintos par√°metros que con los otros archivos, que son m√°s sencillos.

- Primero que todo el JSON del POST debe verse algo as√≠:

  ```json
  {
    "userId": "uuid-del-usuario",
    "datasetName": "Nombre del dataset",
    "fileType": "TipoDeSQl|Mongo",
    "connString": "https://host:puerto@usuario:contrasena",
    "Datasets": ["Tabla1", "Tabla2", "tabla3"]
    }
  ```
  - Hace falta especificar cual es la fuente, cual es su Connection String, y cuales son las tablas/colleciones que deben ser traidas.


2. Recepci√≥n y almacenamiento temporal:

- El UploadController recibe la solicitud y extrae la informaci√≥n del archivo.

- Invoca a `TemporaryStorageHandler.storeTemporary()` pasando el archivo decodificado, el nombre del dataset y el userId.

`El TemporaryStorageHandler` ejecuta un flujo como el siguiente:

```python
import base64
import uuid
import boto3
from cryptography.fernet import Fernet

class TemporaryStorageHandler:
    def __init__(self, s3_client, encryption_key):
        self.s3 = s3_client
        self.fernet = Fernet(encryption_key)

    def storeTemporary(self, base64_file, dataset_name, user_id):
        decoded = base64.b64decode(base64_file)
        encrypted = self.fernet.encrypt(decoded)
        file_id = str(uuid.uuid4()) + ".csv"
        s3_key = f"temp/{user_id}/{file_id}"
        self.s3.put_object(Bucket="data-temp-storage", Key=s3_key, Body=encrypted)
        return {
            "s3_url": f"s3://data-temp-storage/{s3_key}",
            "file_id": file_id
        }
```

- El resultado se guarda en una tabla en DynamoDB llamada `DatasetUploadTemp` con estado "uploaded".

- Cabe aclarar que ese ejemplo tan solo ser√° para los archivos de tipo CSV, JSON, y excel, para los de SQL y Mongo se har√° lo siguiente:
  - **SQL/Mongo**: Se enviar√° un archivo de pyspark que mapeee las tablas/ seleccionadas al S3 de data-temp-storage con formato parquet.



3. Validaci√≥n inicial:

- ValidationManager toma la URL del archivo en S3 desde DatasetUploadTemp.

- Descarga el archivo cifrado, lo descifra, y realiza validaciones b√°sicas:

  - Revisi√≥n de encabezados.
  - Coherencia de tipos de datos por columna.
  - Detecci√≥n de campos vac√≠os y estructura tabular.
  - Nombre unico de Dataset.
  - Revisa si todos los registros vienen con un timestamp (este no es un criterio de rechazo, es de contexto).

```python
import pandas as pd
from cryptography.fernet import Fernet
import boto3

class ValidationManager:
    def __init__(self, s3_client, encryption_key):
        self.s3 = s3_client
        self.fernet = Fernet(encryption_key)

    def validate(self, s3_key):
        try:
            obj = self.s3.get_object(Bucket="data-temp-storage", Key=s3_key)
            decrypted = self.fernet.decrypt(obj['Body'].read())
            df = pd.read_csv(pd.compat.StringIO(decrypted.decode('utf-8')))
            assert df.columns is not None
            assert not df.isnull().all(axis=1).any()
            return True
        except Exception as e:
            logger.error(f"Error en validaci√≥n de dataset: {str(e)}")
            raise
```

4. Notificaci√≥n y confirmaci√≥n:

- `UploadFlowCoordinator` utiliza RabbitMQ para enviar mensajes al notification-service, el cual notifica al usuario por correo electr√≥nico o bien por notificaci√≥n del sistema en caso de estar online, sobre el √©xito de la carga inicial y los siguientes pasos para configurar detalladamente el dataset.

```py
import pika
import json

class UploadFlowCoordinator:
    def __init__(self):
        connection = pika.BlockingConnection(pika.ConnectionParameters('rabbitmq-host'))
        self.channel = connection.channel()
        self.channel.queue_declare(queue='notification-queue')

    def notify_success(self, user_id, dataset_id):
        message = {
            "type": "upload_success",
            "user_id": user_id,
            "dataset_id": dataset_id,
            "message": "El dataset fue cargado y validado exitosamente."
        }
        self.channel.basic_publish(
            exchange='',
            routing_key='notification-queue',
            body=json.dumps(message)
        )
```

5. Respuesta al frontend:

```json
{
  "datasetId": "uuid-del-dataset",
  "status": "initial-validation-passed"
}
```

**2. dataset-configuration-service**

Una vez el dataset haya sido cargado en el microservicio anterior, sigue el este que permite configurar el comportamiento, incluyendo privacidad, acceso, monetizaci√≥n y periodicidad de actualizaci√≥n. A continuaci√≥n los componentes internos:

- **ConfigurationController:** Expone los endpoints para definir pol√≠ticas de configuraci√≥n por dataset.

  - `/config/dataset/access`
  - `/config/dataset/payment`
  - `/config/dataset/delta`

- **PermissionHandler:** Valida si el usuario tiene permisos administrativos sobre el dataset.

- **PaymentModelService:** Aplica l√≥gica de monetizaci√≥n basada en reglas.

- **DeltaUploadManager:** Define y activa las configuraciones de carga peri√≥dica.

**Flujo completo de configuraci√≥n de un dataset:**

1. Definici√≥n de acceso

- El frontend realiza una solicitud `POST /config/dataset/access` con el datasetId y el tipo de acceso:

```json
{
  "datasetId": "uuid-del-dataset",
  "access": "public|private|restricted",
  "allowedUsers": ["uuid-user1", "uuid-user2"]
}
```

- `ConfigurationController` llama a `PermissionHandler.validateOwnership()` para validar que el usuario tenga permisos sobre ese dataset.

- Si pasa la validaci√≥n, se actualiza `AccesoDataset` en RDS con el nuevo tipo de acceso.

2. Configuraci√≥n de monetizaci√≥n

- Se llama a `POST /config/dataset/payment`:

```json
{
  "datasetId": "uuid-del-dataset",
  "model": "subscription|per_request|free",
  "price": 9.99,
  "period": "monthly"
}
```

- `ConfigurationController` delega a `PaymentModelService`, que valida el modelo y registra las condiciones en la tabla `DatasetDePago`.

3. Configuraci√≥n de cargas incrementales

- Solicitud `POST /config/dataset/delta`:

```json
{
  "datasetId": "uuid-del-dataset",
  "cron": "0 0 * * *",
  "connectionId": "secreto-en-secrets-manager",
  "mode": "delta",
  "triggerMethod": "timed_pull|callback",
  "IgnoreColumns": "true|false"
}
```

- `DeltaUploadManager` invoca a `SecurityController.retrieve()` del security-service para obtener credenciales.

- Para el par√°metro del Cron se definir√°n en la UI como posibles tiempos:

  - A una hora espec√≠fica del d√≠a: 1:00, 7:00, 13:00, 22:00, etc.
  - Opci√≥n para ejecutar cada 12, 6, 3 horas.

- Para el par√°metro de mode est√°n las siguientes opciones.

  - Delta: Permite hacer cargas diferenciales. **Esta opci√≥n solo se permitir√° si el dataset de la fuente tiene: timestamps en cada registro, garantiza que las PKs (o equivalente) no cambian y son incrementales**.
  - Complete: Solicita que se cargue todo el dataset desde 0 y se deseche el que hay en Redshift.

- Para triggerMethod existen dos opciones:

  - Callback: no se registra el dataset como timed_pull y se asumir√° que solo se puede actualizar on demand.
  - Si triggerMethod es timed_pull entonces se registrar√° en `DatasetCrons` de RDS cada cuanto se hace el pull de los datos, cual es la fuente de datos (el connection string o URL), que tipo es (SQL, MongoDB o API), y el modo en el que opera (Complete o Delta).

- Para IgnoreColumns existen dos opciones:
  - Si se desea que se ignoren columnas nuevas que vengan en los datasets posteriores a la primera carga.
  - Si se desea que cuando venga una nueva columna en una tabla se le a√±ada a toda la tabla destino en redshift.

Respuesta al frontend:

```json
{
  "status": "configured",
  "datasetId": "uuid-del-dataset"
}
```

4. Pol√≠ticas de acceso y restricci√≥n

El sistema de configuraci√≥n permite definir restricciones adicionales sobre el acceso a datasets privados o pagos. Estas pol√≠ticas se aplican autom√°ticamente en los microservicios de consulta y son definidas por el usuario administrador del dataset a trav√©s de `ConfigurationController`.

- El sistema de permisos evita accesos no autorizados mediante `RBAC` gestionadas por `PermissionHandler`. El sistema de ingresos a los datasets ya fue explicado previamente en el microservicio de la B√≥veda, aqu√≠ aplica el mismo

Respuesta al frontend:

```json
{
  "status": "configured",
  "datasetId": "uuid-del-dataset"
}
```

**3. security-service**

Este servicio centraliza el manejo seguro de credenciales y par√°metros sensibles relacionados con fuentes de datos externas utilizadas por otros microservicios como dataset-upload-service y dataset-configuration-service. Contiene los siguientes componentes:

- **SecurityController:** Expone endpoints REST para almacenar y recuperar secrets que se encuentran cifrados.

  - `/security/store`: Almacena secretos de conexi√≥n (usuario, contrase√±a, API key).
  - `/security/retrieve`: Devuelve los secretos descifrados para uso interno de microservicios.

- **EncryptionManager:** Se encarga del cifrado y descifrado utilizando AES-256, e integra pol√≠ticas de rotaci√≥n autom√°tica de llaves.

- **SecretsManagerHandler:** Utiliza AWS Secrets Manager para persistencia de secretos de forma segura y auditable.

Flujos principales del microservicio:

1. Almacenamiento de credenciales

- Cuando un dataset se configura para carga por conexi√≥n externa, el frontend env√≠a:

```json
{
  "connectionName": "prod-db",
  "username": "usuario",
  "password": "secreto123",
  "type": "postgres"
}
```

- `SecurityController` recibe la solicitud y llama a `SecretsManagerHandler`, para que llame a EncryptionManager para cifrar los datos.

`SecretsManagerHandler` almacena el secreto bajo una clave.

```py
class EncryptionManager:
    def encrypt(self, data):
        return self.fernet.encrypt(data.encode()).decode()

class SecretsManagerHandler:
    def store_secret(self, key, secret_data):
        client.put_secret_value(SecretId=key, SecretString=secret_data)
```

- Un punto muy importante a aclarar es que para poder identificar el secreto en aws secret manager, se le pondr√° el mismo id que el utilizado en la tabla de datasets de la b√≥veda. De esta forma se puede extraer con facilidad el secret cuando se ocupe hacer pull de datos.

2. Recuperaci√≥n de credenciales

Otro microservicio solicita el secreto con un secretId:

```json
{
  "secretId": "id123"
}
```

- `SecurityController` consulta a `SecretsManagerHandler` y luego llama a `EncryptionManager.decrypt()` para descifrar antes de devolverlo.

```py
class EncryptionManager:
    def decrypt(self, token):
        return self.fernet.decrypt(token.encode()).decode()
```

Respuesta:

```json
{
  "username": "usuario",
  "password": "secreto123",
  "type": "postgres",
  "dataset": "dataset4"
}
```

4. Notificaci√≥n

Se env√≠a una notificaci√≥n al usuario sobre el resultado de la validaci√≥n utilizando el notification-service.

```json
{
  "status": "validated_with_warnings",
  "reportUrl": "https://s3.amazonaws.com/reports/datasetId_validation.pdf"
}
```

**4. notification-service**

Este servicio permite comunicar eventos relevantes del sistema a los usuarios finales y a sistemas administrativos mediante colas de mensajes, correo electr√≥nico o notificaciones en la aplicaci√≥n. Tiene los siguientes componentes:

- **NotificationListener:** Escucha los mensajes que llegan a la cola `notification-queue` de RabbitMQ y lo procesa con los handlers segun el tipo de evento.

- **EmailSender:** Envia emails a los usuarios utilizando Amazon SES.

- **WebhookNotificationHandler:** Env√≠a notificaciones a servicios externos v√≠a HTTP POST (por ejemplo, para sistemas de auditor√≠a externos).

- **AdminAuditHandler:** Registra eventos cr√≠ticos como fallos de validaci√≥n o problemas de pago en un log especial para revisi√≥n administrativa.

Tabla de rutas posibles:

| Tipo de evento      | Handlers                                             |
| ------------------- | ---------------------------------------------------- |
| `upload_success`    | `EmailNotificationHandler`, `AppNotificationHandler` |
| `validation_failed` | `EmailNotificationHandler, AdminAuditHandler`        |
| `external_alert`    | `WebhookNotificationHandler`                         |
| `quota_exceeded`    | `AppNotificationHandler`, `EmailNotificationHandler` |
| `admin_warning`     | `AdminAuditHandler`, `EmailNotificationHandler`      |

**Flujo t√≠pico de notificaci√≥n por evento exitoso:**

1. Otro microservicio (por ejemplo `upload-service` o `validation-service`) publica un mensaje a RabbitMQ con estructura:

```json
{
  "type": "upload_success",
  "user_id": "uuid-user",
  "dataset_id": "uuid-dataset",
  "message": "Tu dataset fue cargado y validado exitosamente."
}
```

2. NotificationListener consume el mensaje y delega la tarea al handler correspondiente:

```py
import pika
import json

class NotificationListener:
    def __init__(self):
        connection = pika.BlockingConnection(pika.ConnectionParameters('rabbitmq-host'))
        self.channel = connection.channel()
        self.channel.queue_declare(queue='notification-queue')

    def start_listening(self):
        def callback(ch, method, properties, body):
            data = json.loads(body)
            if data['type'] == 'upload_success':
                EmailNotificationHandler.send_success_email(data['user_id'], data['dataset_id'])
                AppNotificationHandler.add_to_feed(data['user_id'], data['message'])

        self.channel.basic_consume(queue='notification-queue', on_message_callback=callback, auto_ack=True)
        self.channel.start_consuming()
```

3. El correo es enviado mediante EmailNotificationHandler.`send_success_email()` y la notificaci√≥n se agrega al feed del usuario.

Respuesta esperada:

```json
{
  "status": "notified",
  "user_id": "uuid-user",
  "dataset_id": "uuid-dataset"
}
```

### Diagramas de Clases

**1. dataset-upload-service**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Amarillo: Representa un observer.
- Naranja: Representa un dependency injection.
- Verde: Representa un strategy.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el UploadController, que act√∫a como facade para que el API general del backend se comunique con este microservicio. Este controlador delega las llamadas a un observer mediante el EventManager, encargado de notificar a la l√≥gica de negocio correspondiente seg√∫n el tipo de llamada realizada al UploadController.

Dentro de esa l√≥gica se encuentran el TemporaryStorageHandler, ValidationManager (Usa un patr√≥n Strategy ya que el tipo de archivo condiciona como se procede en la validaci√≥n) y UploadFlowCoordinator, que reciben como dependencias los servicios de la segunda capa de facade.

En esta segunda capa se encuentran:

- RabbitMQMessager: abstrae el env√≠o de mensajes al exchange del bioregistro.
- FileHandler: Se encarga de guardar y traer los archivos en S3, y guardar su path en la tabla `DatasetUploadTemp` de DynamoDB.
- DBDumper: Se encarga de hacer los dumps que el FileHandler ocupe, usa un strategy ya que los de SQL y Mongo se hacen de formas distintas.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesCentroCarga1.png)

**2. dataset-configuration-service**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Amarillo: Representa un observer.
- Naranja: Representa un dependency injection.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el ConfigurationController, que act√∫a como facade para que el API general del backend se comunique con este microservicio. Este controlador delega las llamadas a un observer mediante el EventManager, encargado de notificar a la l√≥gica de negocio correspondiente seg√∫n el tipo de llamada realizada al ConfigurationController.

Dentro de esa l√≥gica se encuentran el PermissionHandler, PaymentModelService y DeltaUploadManager, que reciben como dependencias los servicios de la segunda capa de facade.

En esta segunda capa se encuentran:

- DatasetManager: Se encarga de crear configuraciones para los datasets como tipos de pago, tipos de carga, autorizaciones.
- FileHandler: Se encarga traer los archivos en S3.
- SecurityRequester: se comunica con el security-service para autorizar a cambios en datasets.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesCentroCarga2.png)

**3. security-service**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Amarillo: Representa un observer.
- Naranja: Representa un dependency injection.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el SecurityController, que act√∫a como facade para que el API general del backend se comunique con este microservicio. Este controlador delega las llamadas a un observer mediante el EventManager, encargado de notificar a la l√≥gica de negocio correspondiente seg√∫n el tipo de llamada realizada al SecurityController.

Dentro de esa l√≥gica se encuentran el SecretsManagerHandler, que recibe con inyecci√≥n dependencias los servicios de la segunda capa de facade.

En esta segunda capa se encuentran:

- AWSSecretHandler: Se encarga de cargar y obtener secretos de AWS Secret Manager .
- EncryptionManager: Se encarga del proceso de encripci√≥n y desencripci√≥n.

![identity clases](img/ClasesCentroCarga3.png)

**4. notification-service**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Celeste: Muestra un factory.
- Verde: Representa un strategy.
- Naranaja: Dependency Injection.
- Caf√©: Representa un Singleton Pattern.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

Se cuenta con una clase abstracta NotificationListener que provee la l√≥gica y conexi√≥n a RabbitMQ. Esta clase es reutilizada por dos componentes principales:

- EmailSender: escucha mensajes destinados a ser reenviados por correo electr√≥nico a trav√©s de AWS SES utilizando el SESService.
- NotificationConsumer: detecta la llegada de nuevas notificaciones que deben mostrarse dentro de la aplicaci√≥n.

Adem√°s, existe una capa encargada de escuchar conexiones de usuarios al frontend para enviar notificaciones en tiempo real mediante el WebSocketController. En segundo plano, el NotificationConsumer verifica si llegan nuevas notificaciones. Si el usuario est√° conectado, se le muestran inmediatamente; de lo contrario, se almacenan en DynamoDB a trav√©s del NotificationManager, para que en la pr√≥xima conexi√≥n el WebhookNotificationHandler se las muestre.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Cada conexi√≥n es manejada utilizando el patr√≥n Singleton.

![identity clases](img/ClasesCentroCarga4.png)

### Servicios de AWS

**Amazon S3**
El servicio de **AWS S3** ser√° el almac√©n principal para la carga de datos en crudo de los datasets este ser√° utilizado por **TemporaryStorageHandler** en **dataset-upload-service**. Tambi√©n es importante mencionar que este servicio servir√° para el acceso a los datasets por **validation-service** para el an√°lisis de los datos.

**Configuraci√≥n de Hardware:** Servicio de almacenamiento de objetos, ilimitadamente escalable. No requiere configuraci√≥n de hardware directa.

**AWS KMS:**
Los datasets necesitan ser protegidos para ello utilizamos **AWS KMS** ya que este servicio nos ser√° de utilidad para proporcionar las claves criptogr√°ficas para el cifrado y descifrado de los datos.
Se utilizar√° para proteger los datasets almacenados temporalmente en **S3** por **TemporaryStorageHandler** en **dataset-upload-service** y para el cifrado/descifrado de secretos gestionados por el **security-service**.

**Configuraci√≥n de Hardware:** Servicio gestionado, serverless. No requiere configuraci√≥n de hardware. Sin embargo, se pueden configurar la creaci√≥n de claves.

- **Tipo de clave:** Sim√©trico
- **Uso de claves:** Cifrado y descifrado
- **Origen del material de claves:** KMS
- **Regionalidad:** Clave de una sola regi√≥n

**AWS SES**
Ser√° el servicio para el env√≠o de correos electr√≥nicos transaccionales a los usuarios finales (ej., notificaci√≥n de carga exitosa, fallo de validaci√≥n), gestionado por el **EmailNotificationHandler** dentro del **notification-service**.

**Configuraci√≥n de Hardware:**
Para configurar un SES simplemente necesitamos dirigirnos a crear una identidad. En tipo de identidad utilizaremos **Direcci√≥n de correo electr√≥nico**, luego en **Direcci√≥n de correo electr√≥nico** colocamos el correo electr√≥nico que utilizaremos (ej. notificacionesDatos@gmail), luego nos llega una notificaci√≥n al correo donde tendremos que verificar la direcci√≥n de correo electr√≥nico.

### Sistema de Monitoreo

El monitoreo del Componente del Centro de Carga de Datos se implementar√° siguiendo una estrategia de observabilidad integral que permita supervisar en tiempo real el comportamiento, rendimiento y seguridad de todo el proceso de ingesta inicial de datasets.

**M√©tricas y Rendimiento**

**AWS CloudWatch** ser√° el servicio para monitoreo m√°s importante se encargara de recopilar y almacenar m√©tricas operacionales del Componente del Centro de Carga de Datos. Se monitorizar√°n aspectos cr√≠ticos como:

**M√©tricas de Negocio:**

- Cantidad de datasets cargados exitosamente por formato (CSV, Excel, JSON).
- Tasa de √©xito en la validaci√≥n inicial de esquema y estructura del archivo.
- Tiempo promedio del proceso completo de carga (desde la recepci√≥n hasta el almacenamiento temporal en S3).
- Volumen de datos (en GB) ingesados diariamente.
- Cantidad de notificaciones de carga enviadas (√©xito/fracaso).

**M√©tricas de Infraestructura:**

- **S3 (data-temp-storage):** Latencia de operaciones PutObject, GetObject, ListObjects; cantidad de PutRequests, GetRequests; tasa de errores (4xx, 5xx).
- **RDS/DynamoDB:** Latencia de conexiones, ReadIOPS, WriteIOPS, utilizaci√≥n de recursos para las tablas DatasetUploadTemp y DatasetMetadata.
- **AWS RabbitMQ:** Tama√±o de la cola de notificaciones de notification-queue, mensajes entrantes/salientes, latencia de conexi√≥n al broker.
- **AWS KMS:** Tasa de solicitudes y errores en las operaciones de cifrado/descifrado de las claves usadas por dataset-upload-service.

**Prometheus** complementar√° a CloudWatch recopilando m√©tricas espec√≠ficas como las del microservicio de **dataset-upload-service** a trav√©s de un **endpoint** dedicado. Esto permitir√° obtener m√©tricas m√°s granulares sobre el comportamiento interno de la aplicaci√≥n, como:

- Los contadores de operaciones espec√≠ficas (ej., validation_attempts_total, encryption_calls_total).
- Los histogramas de distribuci√≥n de tiempos (ej., file_parsing_duration_seconds, db_write_duration_seconds).

**Visualizaci√≥n y Dashboards**
**Grafana** se utilizar√° como plataforma principal de visualizaci√≥n, integr√°ndose tanto con CloudWatch como con Prometheus para crear dashboards interactivos que permitan:

- **Dashboard Operacional de Carga:** Vista en tiempo real del estado general del proceso de carga de datos. Mostrar√° el volumen de cargas activas, la distribuci√≥n de archivos por formato, la tasa de √©xito/fracaso de las cargas, y el estado de salud de los pods de dataset-upload-service y sus dependencias (S3, DBs, MQ).

- **Dashboard de Rendimiento de Carga:** Monitoreo espec√≠fico de las latencias. Incluir√° el tiempo promedio del proceso de carga, la latencia de escritura en S3, la latencia de registro de metadatos en DBs, y el consumo de recursos (CPU/memoria) del **dataset-upload-service**.

- **Dashboard de Calidad y Seguridad de Carga:** Seguimiento de eventos relacionados con la calidad inicial y la seguridad del proceso de carga. Mostrar√° la tasa de errores en la validaci√≥n inicial de esquema, intentos de acceso no autorizado a recursos de carga v√≠a security-service, y monitoreo de las operaciones de cifrado.

**Logs y Trazabilidad**
El sistema de logging aprovechar√° **CloudWatch Logs** para centralizar todos los registros generados por los componentes del Centro de Carga de Datos. Se implementar√° un esquema de logging estructurado que facilite:

- **Trazabilidad Completa con AWS X-Ray:** Cada transacci√≥n de carga tendr√° un identificador √∫nico de correlaci√≥n (ID de traza X-Ray) que permitir√° seguir su flujo desde la recepci√≥n del archivo, pasando por la interacci√≥n con S3, KMS, el registro de metadatos en RDS/DynamoDB, y la interacci√≥n con security-service o validation-service, hasta la notificaci√≥n final.

- **Auditor√≠a y Diagn√≥stico**
  - **CloudWatch Logs Insights:** Permite la consulta interactiva de logs para identificar r√°pidamente la causa ra√≠z de cualquier incidencia (ej., errores en el procesamiento de un tipo de archivo espec√≠fico).
  - **AWS CloudTrail:** Registra todas las llamadas a la API de AWS realizadas por el dataset-upload-service y sus roles asociados (ej., s3:PutObject, kms:Encrypt, secretsmanager:GetSecretValue), esencial para auditor√≠a y seguridad.

**Sistema de Alertas y Notificaciones**
Se configurar√° un sistema proactivo de alertas utilizando **CloudWatch Alarms** que notificar√° al equipo de operaciones cuando se detecten condiciones an√≥malas:

- **Alertas Cr√≠ticas (respuesta inmediata requerida):**

  - Fallo total del **dataset-upload-service** o indisponibilidad de su **endpoint** de **health check**.
  - Tasa de error (HTTP 5xx en el UploadController o en S3) superior al 5% en una ventana de 5 minutos.
  - Fallo en la conexi√≥n con servicios cr√≠ticos (S3, RDS/DynamoDB, KMS, Amazon MQ).
  - Detecci√≥n de un incremento s√∫bito de errores en operaciones de cifrado/descifrado (KMS).
  - Errores cr√≠ticos registrados en CloudWatch Logs por el dataset-upload-service (ej., Unhandled Exception).

- **Alertas de Advertencia (revisi√≥n prioritaria):**

  - Degradaci√≥n del rendimiento con latencias de carga de datasets superiores a 30 segundos.
  - Uso de recursos (CPU, memoria) del pod de **dataset-upload-service** por encima del 80% de capacidad.
  - Incremento inusual en las validaciones iniciales de datasets fallidas.
  - Acumulaci√≥n de objetos sin procesar en el bucket data-temp-storage por m√°s de un umbral de tiempo.

- **Alertas Informativas (seguimiento regular):**
  - Resumen diario de m√©tricas operacionales de carga (ej., total de cargas exitosas del d√≠a).
  - Reporte semanal de tendencias de volumen de datos ingesados.

**Monitoreo de Cumplimiento y Seguridad**
Dado el manejo de datos sensibles en la carga, se implementar√°n controles espec√≠ficos de monitoreo para garantizar el cumplimiento normativo y la seguridad:

- **Auditor√≠a de Accesos a Datos Cargados:** Registro detallado usando CloudTrail y CloudWatch Logs de todos los accesos PutObject, GetObject al bucket data-temp-storage, identificando qui√©n accedi√≥, cu√°ndo y con qu√© prop√≥sito.
- **Verificaci√≥n de Cifrado:** Monitoreo continuo del estado de cifrado de datos en reposo en S3 mediante pol√≠ticas de bucket y eventos de KMS, asegurando que todos los archivos cargados est√©n cifrados correctamente.
- **Monitoreo de Acceso a Secretos:** Seguimiento de los intentos de acceso y las rotaciones de credenciales en AWS Secrets Manager utilizadas por el dataset-upload-service para conectarse a fuentes externas o bases de datos.

**Health Checks y Disponibilidad**
Los microservicios del Centro de Carga implementar√°n m√∫ltiples niveles de verificaci√≥n de salud que ser√°n monitoreados continuamente por Kubernetes y los sistemas de monitoreo:

- **Liveness Probe:** Verificaci√≥n b√°sica de que el dataset-upload-service est√° activo y respondiendo, ejecutada cada 10 segundos por Kubernetes.
- **Readiness Probe:** Verificaci√≥n comprehensiva de que el dataset-upload-service puede procesar solicitudes de carga, incluyendo conectividad con S3, KMS, bases de datos (RDS/DynamoDB) y Amazon RabbitMQ.
- **Deep Health Checks:** Verificaciones peri√≥dicas m√°s exhaustivas que validan la integridad de configuraciones cr√≠ticas (ej., validaci√≥n de esquemas de carga), la disponibilidad de claves de cifrado, y la correcta operaci√≥n del flujo completo de carga de un archivo de prueba simulado.

**An√°lisis y Mejora Continua**
El sistema de monitoreo no solo detectar√° problemas, sino que proporcionar√° insights para la mejora continua del proceso de carga:

- **An√°lisis de Tendencias:** Identificaci√≥n de patrones en el volumen y tipo de cargas (ej., picos horarios, aumento de un formato espec√≠fico) para optimizar recursos y predecir necesidades futuras.
- **Detecci√≥n de Anomal√≠as:** Uso de las capacidades de CloudWatch para identificar comportamientos inusuales (ej., ca√≠da repentina en el n√∫mero de cargas exitosas) que podr√≠an indicar problemas emergentes.
- **Reportes de Capacidad:** Proyecciones basadas en datos hist√≥ricos de volumen de carga y uso de recursos para planificar el crecimiento de la infraestructura de almacenamiento (S3) y c√≥mputo (EKS).
- **Optimizaci√≥n de Costos:** An√°lisis del uso de recursos de S3, EKS y DBs para identificar oportunidades de optimizaci√≥n de costos sin comprometer el rendimiento.

### Modelo de seguridad detallado

El backend del componente Centro de Carga gestiona informaci√≥n cr√≠tica relacionada con datasets, incluyendo la carga, validaci√≥n y categorizaci√≥n. Dado su rol esencial, se implementar√° un modelo de seguridad robusto y granular orientado a prevenir accesos no autorizados, asegurar integridad, confidencialidad, trazabilidad y disponibilidad continua de los datos.

1. Control de Acceso Granular

| Rol del usuario  | Descripci√≥n                                         | Permisos sobre recursos del Centro de Carga                                                                |
| ---------------- | --------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
| `Carga:viewer`   | Usuario con acceso de solo lectura                  | Visualizar historial de cargas, detalles de configuraci√≥n, esquema de columnas y metadatos                 |
| `Carga:editor`   | Usuario autorizado a dise√±ar y actualizar cargas    | Crear cargas nuevas, definir esquema de columnas, asociar metadata, configurar delta                       |
| `Carga:approver` | Validador de configuraciones previas a la ejecuci√≥n | Aprobar configuraciones antes de ser activadas, validar transformaciones, confirmar integridad estructural |
| `Carga:admin`    | Administrador completo del m√≥dulo de carga          | Modificar permisos de carga, eliminar configuraciones, visualizar trazabilidad completa, forzar cargas     |

Ejemplo de flujo de autorizaci√≥n en RDS (PostgreSQL):

- **AccesoADataset**: registra qu√© personas tienen acceso a cada dataset.
- **DatasetDePago y TipoDePago**: gestionan la configuraci√≥n de pagos asociados a datasets, incluyendo su categorizaci√≥n.
- **DatasetCrons**: define el modo de carga que tendr√°n los datasets recurrentes (Delta, Complete), y el tipo de fuente del que pueden venir (Base de datos o API). Esta tabla ser√° consultada por Airflow para aplicar los DAGs correspondientes de forma automatizada.
- **Tablas**: Esta tabla almacena a que Dataset pertenecen las distintas tablas de Redshift .
1. Un usuario autenticado realiza una solicitud para modificar una configuraci√≥n de carga.

2. El backend identifica que el usuario tiene rol `Carga:editor` y autentica su token internamente.

3. Se ejecuta una funci√≥n almacenada en PostgreSQL:


```SQL
SELECT actualizar_configuracion_carga(:id_config, :nueva_metadata, :usuario);
```


4. Dentro de la funci√≥n `actualizar_configuracion_carga`, se valida que el usuario tenga permisos equivalentes al rol editor en la tabla `roles_usuario`:

```SQL
IF NOT EXISTS (SELECT 1 FROM roles_usuario WHERE usuario = $3 AND rol = 'editor') THEN
  RAISE EXCEPTION 'Acceso no autorizado';
END IF;
```

5. Si pasa la validaci√≥n, se actualizan los campos correspondientes; en caso contrario, se bloquea la operaci√≥n y se registra un intento fallido en la bit√°cora de auditor√≠a.

**2. Cifrado de Informaci√≥n**

#### Cifrado en tr√°nsito

Todas las comunicaciones entre el frontend del centro de carga, los microservicios y los servicios de almacenamiento (Amazon S3 y RDS), se ejecutan mediante HTTPS con TLS 1.3. Igualmente, EKS fuerza el uso de TLS con certificados actualizados gestionados mediante AWS Certificate Manager.

#### Cifrado en reposo

Cada tipo de dato gestionado por el Centro de Carga est√° protegido mediante mecanismos nativos de cifrado proporcionados por los servicios utilizados:

| Tecnolog√≠a      | Elementos cifrados                                | Mecanismo de cifrado                       | Particularidades de seguridad                                                        |
| --------------- | ------------------------------------------------- | ------------------------------------------ | ------------------------------------------------------------------------------------ |
| Amazon S3       | Archivos de datasets cargados + metadata asociada | AWS KMS                                    | Bucket con pol√≠ticas que rechazan cargas no cifradas y control de acceso restringido |
| Amazon RDS      | Configuraciones estructurales                     | Cifrado de disco autom√°tico con claves KMS | Acceso restringido a trav√©s de funciones y roles internos                            |
| Amazon DynamoDB | Estados de carga, historial de ejecuci√≥n          | Cifrado nativo activado autom√°ticamente    | Acceso limitado por pol√≠tica IAM de microservicio                                    |

**Adicionalmente**

- Los archivos cargados son escaneados y validados antes de ser almacenados. Solo si cumplen con los requisitos del esquema de columnas aprobado y no presentan fallos estructurales o sem√°nticos, se escriben en el bucket correspondiente, con nombre aleatorio y metadata cifrada.

- En caso de rechazo en el proceso de validaci√≥n, el archivo se descarta y se registra el evento en la bit√°cora para trazabilidad.

**3. Auditor√≠a y Trazabilidad**

#### Elementos auditados

- Solicitudes de creaci√≥n, modificaci√≥n y eliminaci√≥n de configuraciones de carga.
- Procesos de validaci√≥n estructural y sem√°ntica.
- Aprobaciones manuales o autom√°ticas.
- Errores detectados en archivos cargados.
- Consultas sobre configuraciones y ejecuciones pasadas.

#### Origen y estructura del registro

1. Identificador de usuario y rol.
2. Timestamp exacto de la operaci√≥n.
3. Tipo de operaci√≥n realizada.
4. IP de origen o microservicio emisor.
5. Resultado de la acci√≥n (√©xito, error, rechazo por validaci√≥n).

#### Tecnolog√≠as utilizadas

- **Amazon CloudWatch Logs:** Registro estructurado de eventos en tiempo real.

- **Amazon DynamoDB Streams:** Replicaci√≥n de eventos sensibles a una tabla de auditor√≠a hist√≥rica.

#### Acceso y resguardo

El acceso a los registros est√° restringido a roles con privilegios de auditor√≠a mediante pol√≠ticas IAM. Se implementan estrategias de rotaci√≥n, almacenamiento cifrado y retenci√≥n m√≠nima de 12 meses.

**4. Monitoreo y Gesti√≥n de Incidentes**

#### 4.1 Monitoreo en Tiempo Real

- **Prometheus:** Se utiliza para recolectar m√©tricas personalizadas relacionadas con la carga de datasets, como el tiempo promedio de validaci√≥n de un archivo o la tasa de √©xito en las cargas de datos.

- **AWS CloudWatch:** Monitorea los logs generados por los microservicios del Centro de Carga, enviando alertas cuando se detectan patrones an√≥malos, como fallos recurrentes o tiempos de espera demasiado largos en el procesamiento de datos.

#### 4.2 Gesti√≥n de Incidentes

Esta secci√≥n est√° dise√±ada para identificar y responder r√°pidamente ante cualquier tipo de evento que pueda comprometer la integridad del sistema o la seguridad de los datos.

1. **Detecci√≥n de Incidentes:** Se utilizan reglas de alerta configuradas en CloudWatch para detectar incidentes como errores en la carga de datos, archivos rechazados por validaciones o ca√≠das de servicios externos (como bases de datos o APIs).

2. **Clasificaci√≥n:** Se eval√∫a la gravedad del incidente y se clasifica como cr√≠tico, medio o bajo.

| **Clasificaci√≥n de Incidente** | **Descripci√≥n**                                                                                                                                                                         | **Ejemplo**                                                                                                                                                                              | **Acci√≥n Requerida**                                                                                                                            |
| ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| **Cr√≠tico**                    | Incidentes que afectan directamente la operaci√≥n del sistema y pueden comprometer la seguridad o la integridad de los datos. Requieren una acci√≥n inmediata para restaurar el servicio. | - Ca√≠da de base de datos (Amazon RDS) durante una carga de datos. <br> - Exposici√≥n accidental de datos sensibles.                                                                       | - Restauraci√≥n inmediata desde backups. <br> - Notificaci√≥n a los administradores a trav√©s de AWS SNS. <br> - Reinicio autom√°tico de servicios. |
| **Medio**                      | Incidentes que afectan el rendimiento o la funcionalidad del sistema. Se pueden retrasar procesos o requerir una intervenci√≥n manual.                                                   | - Error en la validaci√≥n de un archivo de carga que retrasa la operaci√≥n pero no detiene el flujo. <br> - Lento procesamiento de un dataset debido a un error de configuraci√≥n temporal. | - Notificaci√≥n al equipo de soporte. <br> - Revalidaci√≥n y reintento de carga.                                                                  |
| **Bajo**                       | Incidentes menores que no afectan el funcionamiento principal del sistema, pero que requieren atenci√≥n para evitar que se conviertan en problemas mayores.                              | - Un archivo rechazado por un error de formato menor. <br> - Una solicitud de visualizaci√≥n de metadatos que no devuelve resultados por una peque√±a falla en el frontend.                | - Registro del incidente en el sistema de auditor√≠a. <br> - Resoluci√≥n del error en la pr√≥xima actualizaci√≥n.                                   |

3. **Respuesta Autom√°tica:** En caso de un incidente, se utiliza AWS Lambda para ejecutar funciones que puedan mitigar el impacto.

```py
import json
import boto3
import logging

# Configurar el cliente de S3
s3_client = boto3.client('s3')
lambda_client = boto3.client('lambda')

# Configurar el cliente de SNS para notificaciones
sns_client = boto3.client('sns')

# Nombre del bucket y archivo que estamos tratando de cargar
bucket_name = 'nombre-del-bucket'
file_key = 'ruta/al/archivo/dataset.csv'

# Tema de SNS para notificaci√≥n de incidentes
sns_topic_arn = 'arn:aws:sns:region:account-id:topic-name'

# Funci√≥n Lambda para manejar el incidente
def lambda_handler(event, context):
    try:
        # Intentamos cargar el archivo desde S3 nuevamente (simulando la carga de datos)
        response = s3_client.upload_file(file_key, bucket_name, file_key)
        logging.info(f"Archivo cargado exitosamente: {file_key}")

        # Si la carga fue exitosa, enviar una notificaci√≥n de √©xito
        send_notification("Carga de datos exitosa", "El archivo se carg√≥ correctamente.")

        return {
            'statusCode': 200,
            'body': json.dumps('Carga exitosa')
        }

    except Exception as e:
        logging.error(f"Error en la carga de datos: {str(e)}")

        # Enviar una notificaci√≥n de error
        send_notification("Error en la carga de datos", f"Ocurri√≥ un error: {str(e)}")

        # Reintentar la carga, si la operaci√≥n fall√≥
        logging.info(f"Reintentando carga para el archivo: {file_key}")
        return retry_load()

# Funci√≥n para reintentar la carga del archivo
def retry_load():
    try:
        # Intentar subir el archivo a S3 nuevamente
        s3_client.upload_file(file_key, bucket_name, file_key)
        logging.info("Carga reintentada exitosa.")
        send_notification("Carga reintentada exitosa", f"El archivo {file_key} se ha cargado exitosamente despu√©s del reintento.")

        return {
            'statusCode': 200,
            'body': json.dumps('Carga reintentada exitosa')
        }

    except Exception as e:
        logging.error(f"Error en el reintento de carga: {str(e)}")
        send_notification("Error en el reintento de carga", f"Ocurri√≥ un error en el reintento: {str(e)}")

        return {
            'statusCode': 500,
            'body': json.dumps('Fallo en la carga despu√©s del reintento')
        }

# Funci√≥n para enviar notificaciones a SNS
def send_notification(subject, message):
    response = sns_client.publish(
        TopicArn=sns_topic_arn,
        Message=message,
        Subject=subject
    )
    logging.info(f"Notificaci√≥n enviada: {response}")

```

4. **Notificaci√≥n de Incidentes:** Cuando un incidente es clasificado como cr√≠tico AWS SNS env√≠a notificaciones a los administradores y responsables de la seguridad.

### Elementos de Alta Disponibilidad

**1. Almacenamiento Distribuido**

Se utiliza Amazon S3 para el almacenamiento seguro de los datos de carga, incluyendo archivos de configuraci√≥n, logs y otros datos asociados con el proceso. Con una pol√≠tica de replicaci√≥n cruzada de objetos y versionado, cualquier archivo cargado se replica autom√°ticamente a otra zona de disponibilidad. Con esto se cumple la disponibilidad de los datos en caso de fallo en una zona. Para los datos como logs de operaciones, se usa Amazon DynamoDB con activaci√≥n de Point-in-Time Recovery para asegurar la disponibilidad continua de los metadatos asociados con las cargas.

| Recurso        | Tecnolog√≠a    | Implementaci√≥n                                | Activaci√≥n                     | Ubicaci√≥n   |
| -------------- | ------------- | --------------------------------------------- | ------------------------------ | ----------- |
| Datos de Carga | **Amazon S3** | Replicaci√≥n cruzada entre zonas y versionado  | Cada vez que se carga/modifica | `us-east-1` |
| Metadatos      | **DynamoDB**  | Backup continuo con recuperaci√≥n en el tiempo | En cada operaci√≥n de escritura | `us-east-1` |

**2. Monitoreo y Alertas**

Se utiliza AWS CloudWatch para obtener m√©tricas de disponibilidad de los recursos del backend del Centro de Carga como el uso de CPU, memoria y latencia. Por otro lado, se configura Prometheus para la recolecci√≥n de m√©tricas personalizadas sobre los microservicios que gestionan las cargas de trabajo y las interacciones del sistema, con visualizaci√≥n de las m√©tricas en Grafana.

| Tecnolog√≠a     | Funci√≥n                                         | Ubicaci√≥n                           | Ejecuci√≥n                           |
| -------------- | ----------------------------------------------- | ----------------------------------- | ----------------------------------- |
| **CloudWatch** | Monitoreo de m√©tricas de infraestructura AWS    | Servicios de AWS                    | En tiempo real y continuo           |
| **Prometheus** | Recolecci√≥n de m√©tricas espec√≠ficas del sistema | Dentro del cl√∫ster EKS              | Cada vez que se actualizan m√©tricas |
| **Grafana**    | Visualizaci√≥n de datos para diagn√≥stico         | Conectado a CloudWatch y Prometheus | Monitoreo constante                 |

**3. Balanceo de carga**

#### 3.1 Distribuci√≥n de Solicitudes a Microservicios

Las solicitudes entrantes, como las que requieren la carga de datos o la consulta de estado, son dirigidas al Application Load Balancer de AWS. Este ALB distribuye las solicitudes entre las diferentes instancias de los microservicios encargados de procesar los datos.

- Si el Centro de Carga recibe varias solicitudes simult√°neas para cargar grandes vol√∫menes de datos desde Amazon S3, el ALB distribuye estas solicitudes entre las instancias disponibles que gestionan el procesamiento de estos archivos.

#### 3.2 Auto Scaling para Manejo de Picos de Tr√°fico

El Centro de Carga est√° configurado con Auto Scaling Groups (ASG) para ajustarse autom√°ticamente a los picos de tr√°fico. Cuando el volumen de solicitudes sube, el Auto Scaling agrega nuevas instancias para manejar la mayor carga.

- Si se detecta un aumento en el tr√°fico durante un periodo de alta demanda, el Auto Scaling aumenta autom√°ticamente el n√∫mero de instancias disponibles para manejar las nuevas solicitudes de carga sin que se experimenten fallos en el sistema.

#### 3.3 Integraci√≥n con Kubernetes

El Centro de Carga tambi√©n se beneficia del uso de Amazon EKS para gestionar microservicios. El ALB trabaja en dirigir el trafico a contenedores espec√≠ficos dentro del cl√∫ster de Kubernetes, mejorando la distribuci√≥n de solicitudes.

- En un escenario donde se requiere escalar din√°micamente los microservicios, el Ingress Controller en combinaci√≥n con el ALB asegura que el tr√°fico se distribuya equitativamente entre los contenedores de Kubernetes.

### Diagrama del Backend

## Dise√±o de los Datos

La influencia de este componente sobre la base de datos es m√≠nima, ya que reutiliza la misma instancia de RDS compartida con La B√≥veda y el Bioregistro, cuyas especificaciones ya fueron detalladas previamente. Del mismo modo, las configuraciones para DynamoDB y S3 se mantienen id√©nticas a las de esos componentes.
El √∫nico aporte nuevo se encuentra reflejado en el diagrama de base de datos que se presenta a continuaci√≥n.

### Diagrama de Base de Datos

A continuaci√≥n se presenta el diagrama de base de datos correspondiente al m√≥dulo del Centro de Carga. Este diagrama incluye las tablas clave que componen el componente, entre ellas:

- **AccesoADataset**: registra qu√© personas tienen acceso a cada dataset.
- **DatasetDePago y TipoDePago**: gestionan la configuraci√≥n de pagos asociados a datasets, incluyendo su categorizaci√≥n.
- **DatasetCrons**: define el modo de carga que tendr√°n los datasets recurrentes (Delta, Complete), y el tipo de fuente del que pueden venir (Base de datos o API). Esta tabla ser√° consultada por Airflow para aplicar los DAGs correspondientes de forma automatizada.

![image](img/DiagramaBDCentroCarga.png)


# 4.4 Motor De Transformaci√≥n

Este componente del sistema representa el punto de conexi√≥n entre los datasets inteligentes cargados en un sistema altamente eficiente y la data cruda e ineficiente.

**Procesos involucrados**
Los procesos que ejecuta este componente se dividen en dos partes:

- El primero ocurre justo despu√©s de que el Centro de carga notifica que la carga de un dataset en data-temp-storage ha sido finalizada y est√° lista para transformarse.
- El segundo es responsable de ejecutar las transformaciones necesarias para insertar nueva informaci√≥n en los casos en que haya carga recurrente.

Ambos procesos est√°n fuertemente relacionados, ya que deben mantenerse sincronizados por dataset. La transformaci√≥n que se gener√≥ inicialmente debe ser replicable durante futuras cargas. Para lograrlo, se llevar√° un registro trazable de cada paso de la primera transformaci√≥n (esto se detalla m√°s adelante).

Las transformaciones a aplicar no tienen par√°metros fijos en la mayor√≠a de los casos. Van a depender completamente de la naturaleza del dataset recibido y de su distribuci√≥n de registros. Por eso se adopta un enfoque basado en Agentes de IA.

A continuaci√≥n se muestra un diagrama de clases que presenta una visi√≥n general de este enfoque:
![image](img/AgentesAltoNivel.png)

En el diagrama se puede ver c√≥mo un orquestador define una cadena de operaciones que ser√°n ejecutadas por distintos agentes. Estos agentes se encargan de analizar los datasets para aplicarles transformaciones como:
- Eliminaci√≥n de registros duplicados
- Renombramiento de columnas o tablas
- Merging con datasets internos
- Generaci√≥n de SortKeys y DistKeys para Redshift

Este diagrama no representa la implementaci√≥n final, ya que nuestro enfoque est√° basado en tres pilares fundamentales:

- **El coordinador**: El coordinador no ser√° una clase, sino que estar√° implementado directamente usando Airflow.
- **El ejecutor**:En lugar de tener clases de Python ejecutando cambios directamente, contaremos con un cluster de Spark autogestionado al que se le enviar√°n los jobs con las instrucciones de transformaci√≥n.
- **Los analistas**: Aqu√≠ s√≠ hablamos de microservicios concretos, cada uno con la responsabilidad de analizar un dataset y sugerir transformaciones. Usaremos LangChain para orquestar el flujo interno. El resultado de cada analista ser√° un archivo o bien de PySpark o SQLAlchemy,los cuales contendr√°n todas las instrucciones necesarias para llevar a cabo la transformaci√≥n, y los procedimientos para copiar los datos en Redshift respectivamente. Dichos analistas seguir√°n un collaborative pattern orienta a IA agents ya que unen fuerzas para lograr transformar un Dataset y subirlo en limpio a Redshift.

A continuaci√≥n se presenta un grafo que resume este flujo:

![image](img/DiagramaMotor1.png)

Adem√°s, se puede evidenciar que en el proceso de Transformaci√≥n de Datos (TD) dentro de nuestro flujo ETDL, optamos por un enfoque de medallones, donde organizamos los datos en tres categor√≠as:
- **Bronce**: Datos en crudo, sin ninguna transformaci√≥n. Pueden estar en distintos formatos, desde CSV hasta Parquet.
- **Plata**: : Datos parcialmente transformados y estandarizados, todos en un mismo formato (en nuestro caso, Parquet).
- **Oro**: Datos completamente limpios, modelados y cargados en Redshift, listos para ser consultados con BI.


Por otro lado el segundo proceso, encargado de replicar transformaciones sobre datasets recurrentes, es m√°s sencillo. Usaremos la funcionalidad de Airflow con Crons para coordinar la ejecuci√≥n peri√≥dica.

El job aplicar√° directamente los mismos archivos de PySpark que fueron generados en la creaci√≥n inicial del dataset.

A continuaci√≥n se presenta un diagrama que representa este flujo:

  ![image](img/DiagramaMotor2.png)



## Dise√±o del Backend
A continuaci√≥n, se presentar√° la secci√≥n de Dise√±o del Backend. En esta se detallan los microservicios correspondientes a cada uno de los cinco Analistas, junto con la configuraci√≥n de Airflow (incluyendo los DAGs definidos), la integraci√≥n con Spark y todos los componentes que intervienen en el proceso de llevar los datos limpios hasta Redshift.


### Microservicios del Componente

Un detalle importante a aclarar es que este proceso de transformaci√≥n ir√° dejando los resultados de cada etapa en un bucket S3 bajo la ruta /silver-data/nombre-del-dataset. Esto permite mantener trazabilidad sobre cada paso del flujo, posibilita realizar rollbacks si fuera necesario, y garantiza la persistencia tanto del schema esperado como de los archivos de transformaci√≥n de Spark y de Inserci√≥n con SQLAlchemy. De esta manera, si en el futuro se desea volver a actualizar el dataset, las transformaciones ya estar√°n disponibles est√°n listas para reutilizarse, funcionando como una especie de "cach√© l√≥gica".

**1. schema-extractor**

Este microservicio se encarga de, seg√∫n el tipo de archivo fuente, extraer su esquema y los top N registros por cada tabla o agrupaci√≥n. Para ello utiliza pandas y genera como resultado un archivo JSON con la estructura inferida.

Se invoca muchas veces a lo largo del proceso de transformaci√≥n, ya que antes de llamar a cualquier analista, es necesario entender la estructura del dataset. Fue aislado en un servicio independiente para desacoplar completamente esta l√≥gica del procesamiento espec√≠fico de cada analista.

 - **SchemaExtractorController**: Expone el endpoint RESTful del microservicio.
  - `/extract-schema/`: Al llamarlo, se genera un JSON con la estructura del dataset solicitado.
- **SchemaExtractor**: Se encarga de extraer el esquema desde los datos crudos. Utiliza distintos strategies seg√∫n el formato de entrada (CSV, JSON, Parquet, etc.).

El flujo principal ser√≠a el siguiente:

1. La consulta llega al SchemaExtractorController:
- El nodo de airflow correspondiente llama a `POST /extract-schema`
```json
{
  "s3path": "Path del s3 donde est√° el dataset",
  "step": "dice a que paso del proceso de transformaci√≥n pertence",
  "datasetName": "Nombre del dataset",
  "fileType": "CSV|JSON|Parquet|Excel",
  "NRegisters": "Declara cuantos registros por tabla hay que extraer"
}
```
- Dicha informaci√≥n se enruta al SchemaExtractor para que por medio de Pandas extraiga la informaci√≥n del schema de los archivos del dataset. Adem√°s se obtienen los top N elementos sobre el mismo.

- Se generar√° un json con la informaci√≥n mencionada, llamado schema-json. Dicho archivo ser√° s√∫bido al S3 bajo el nombre de "schema-[Numero de step].json".

**2. spark-builder**

Este microservicio tambi√©n es transversal a todo el motor de transformaci√≥n. Se encarga de generar un archivo de spark con las instrucciones para realizar transformaciones, con base en la estructura generada en el schema-extractor y las recomendaciones de los analistas.

Funcionar√° gracias a un agent de IA con base en el modelo de hugging face: Qwen/Qwen2.5-Coder-32B-Instruct.

 - **SparkBuilderController**: Expone el endpoint RESTful del microservicio.
  - `/gen-spark/`: Punto de entrada que provee la informaci√≥n con la que crear el archivo de pyspark.
- **SparkBuilder**: Se encarga de coordinar el enrequicimiento de contexte con el Agente, para que genere un archivo de spark funcional.

El flujo principal ser√≠a el siguiente:

1. La consulta llega al SparkBuilderController:
- El nodo de airflow correspondiente llama a `POST /gen-spark`
```json
{
  "s3path": "Path del s3 donde est√° el schema.json",
  "step": "dice a que paso del proceso de transformaci√≥n pertence",
  "datasetName": "Nombre del dataset",
  "recomendations": "recomendations.json",
  "fileType": "CSV|JSON|Excel|Parquet"
}
```
- Dicha informaci√≥n es enrutada al SparkBuilder, el cu√°l podr√° dar el contexto de cual es la fuente de los datos, que recomendaciones tiene, que transformaciones debe realizar el archivo de pyspark y donde debe guardarlo.

- Para asegurar que todos los scripts generados en Spark sigan una estructura consistente, se utilizar√° un archivo .txt que contiene el prompt base. Este prompt especificar√° las pr√°cticas que deben seguirse durante las transformaciones, como evitar el uso de funciones UDF, para borrar duplicados siempre usar la funci√≥n de DropDuplicates() de pyspark, etc. Adem√°s incluir√° el espacio para agregar las recomendaciones de cada caso.

- Cabe aclarar que las transformaciones independientemente del formato de origen ser√°n pasadas a formato parquet.

- Se obtendra el archivo de pyspark con las transformaciones necesarias. Dicho archivo ser√° s√∫bido al S3 bajo el nombre de "transformation-[Numero de step].py".


**3. sql-builder**

Este microservicio tambi√©n es transversal a todo el motor de transformaci√≥n. Se encarga de generar un archivo de python con SQLAlchemy, seg√∫n un input de recomendaciones. Su desarrollo est√° orientado a la integraci√≥n con Redshfit, ya que sus resultados ser√°n aplicados en dicho OLAP.

Funcionar√° gracias a un agent de IA con base en el modelo de hugging face: defog/sqlcoder-7b-2

 - **SqlBuilderController**: Expone el endpoint RESTful del microservicio.
  - `/gen-pysql/`: Punto de entrada que provee la informaci√≥n con la que crear el archivo de python con SQLAlchemy.
- **SqlBuilder**: Se encarga de coordinar el enrequicimiento de contexto con el Agente, para que genere un archivo de python SQLAlchemy.

El flujo principal ser√≠a el siguiente:

1. La consulta llega al SqlBuilderController:
- El nodo de airflow correspondiente llama a `POST /gen-pysql`
```json
{
  "s3path": "Path del s3 donde est√° el schema.json",
  "step": "dice a que paso del proceso de transformaci√≥n pertence",
  "datasetName": "Nombre del dataset",
  "recomendations": "recomendations.json"
}
```
- Dicha informaci√≥n es enrutada al SqlBuilder, el cu√°l podr√° saber el contexto de cual es el schema de la fuente, que recomendaciones tiene y donde debe guardarlo.

- Para asegurar que todos los scripts generados en SQLAlchemy sigan una estructura consistente, se utilizar√° un archivo .txt que contiene el prompt base. Este prompt especificar√° las pr√°cticas que deben seguirse durante el dise√±o de las consultas, como que cuando tenga que insertar un dataser completo use el comando COPY.

- El producto final del archivo generado por defog/sqlcoder-7b-2 es un script de python SQLAlchemy apto para Redshift.

- Se obtendra el archivo de python y ser√° s√∫bido al S3 bajo el nombre de "SQLAlchemy-[Numero de step].py".


**4. schema-architect**

Este microservicio representa el analista encargado de leer las tablas, columnas y nombres (extraidas por el schema-extractor), aplicar normalizaci√≥n b√°sica y sugerir mejoras estructurales. El resultado final ser√° un json con todas las recomendaciones para que el spark-builder tenga en cuenta.

Funcionar√° gracias a un agent de IA con base en el modelo de hugging face: Phind/Phind-CodeLlama-34B-v2

Los componentes internos son los siguientes:

- **SchemaArchitectController**: Expone el endpoint RESTful del microservicio.
  - `/suggest-transformations/`: Al llamarlo, se espera obtener las recomendaciones.

- **SchemaConsultor**: Consulta al agente LLM con el esquema extra√≠do para obtener sugerencias de mejoras, normalizaci√≥n o reestructuraci√≥n.


El flujo principal ser√≠a el siguiente:

1. La consulta llega al SchemaArchitectController:
- El nodo de airflow correspondiente llama a `POST /suggest-transformations`
```json
{
  "s3path": "Path del s3 donde est√° el schema.json",
  "datasetName": "Nombre del dataset",
  "fileType": "CSV|JSON|Excel|Parquet"
}
```
- Dicha informaci√≥n se enruta al SchemaConsultor para que sepa de donde sacar el schema y cual es el archivo de origen.


2. Proceso de consulta:
- Una vez recibido el schema.json mediante Langchain, se realiza una cadena de consultas a Phind/Phind-CodeLlama-34B-v2 de Hugging Face sobre:
  - Diagn√≥stico del schema: Que tablas/Columnas duplicadas hay, Columnas "mergeables", Relaciones mal establecidas, Mejoras de dise√±o de base de datos.
  - Normalizado: Se le pregunta que recomienda normalizar llegando hasta 4NF.
 -  Renombrador: Se le pregunta que tablas/columnas pueden tener nombres m√°s explicitos.

- Para cada prompt se tendr√° un .txt base al cual simplemente se le a√±adir√° el contexto obtenido por medio del SchemaExtractor para realizar consultas completas:
``` txt
{Contexto sobre que consideramos sobre buen dise√±o de base de datos, junto con ejemplos}

Dado el siguiente esquema de un archivo {tipo de archivo origen}:

{input del schemaExtractor}

Identifica cualquier problema de modelado evidente:
- Que tablas/Columnas duplicadas hay
- Que columnas y tablas se pueden unir o separar
- Que relaciones est√°n mal establecidas
- Consejos sobre dise√±o de base de datos con base en el contexto que se le di√≥

Devuelve un JSON con una lista de observaciones t√©cnicas con este formato:
{
  "mejoras": [
    {"tipo": "renombrar_columna", "tabla": "clientes", "de": "nombre", "a": "nombre_completo"},
    {"tipo": "normalizar_tabla", "tabla": "ordenes", "sugerencia": "mover direcci√≥n a tabla direcciones"}
  ]
}
```

- El flujo orquestrado para los prompts es hecho por LangChain de una forma similar a la siguiente:
``` python
from langchain.llms import HuggingFaceHub
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.schema.runnable import RunnableMap
import os

# Setup del modelo HuggingFace
llm = HuggingFaceHub(
    repo_id="Phind/Phind-CodeLlama-34B-v2",
    model_kwargs={
        "temperature": 0.2,
        "max_new_tokens": 800
    },
    huggingfacehub_api_token=os.environ["HUGGINGFACEHUB_API_TOKEN"]
)

# Cargar prompts base desde archivos de texto
def load_prompt(file):
    with open(f"prompts/{file}", "r") as f:
        return f.read()

diagnose_prompt = PromptTemplate.from_template(load_prompt("diagnose_prompt.txt"))
normalize_prompt = PromptTemplate.from_template(load_prompt("normalize_prompt.txt"))
rename_prompt    = PromptTemplate.from_template(load_prompt("rename_prompt.txt"))

# Cada paso como cadena
diagnose_chain = LLMChain(llm=llm, prompt=diagnose_prompt)
normalize_chain = LLMChain(llm=llm, prompt=normalize_prompt)
rename_chain = LLMChain(llm=llm, prompt=rename_prompt)

# Ejecutar todo en paralelo y devolver los 3 JSONs
schema_consultor = RunnableMap({
    "diagn√≥stico": diagnose_chain,
    "normalizaci√≥n": normalize_chain,
    "renombramiento": rename_chain
})
```

3. Se obtendr√° un reccomendations.json que ser√° devuelto al nodo de airflow que hizo el POST.

**5. cleaner**

Este microservicio act√∫a como un analista que revisa las tablas del dataset para decidir en cu√°les tiene sentido eliminar duplicados. Por ejemplo, si en la tabla usuarios un mismo registro aparece tres veces por un error en el centro de carga, ah√≠ conviene limpiarlos.

En cambio, en una tabla como visitas_web, donde un mismo usuario puede tener m√∫ltiples visitas id√©nticas en apariencia, eliminar duplicados ser√≠a incorrecto porque cada fila representa un evento v√°lido.

Funcionar√° gracias a un agent de IA con base en el modelo de hugging face: Phind/Phind-CodeLlama-34B-v2

Los componentes internos son los siguientes:

- **CleanerController**: Expone el endpoint RESTful del microservicio.
  - `/clean-dataset/`: Al llamarlo, se espera obtener las recomendaciones.
- **CleanerConsultor**: Consulta al agente LLM con el esquema extra√≠do para obtener sugerencias sobre en que tablas hay que eliminar duplicados.

El flujo principal ser√≠a el siguiente:

1. La consulta llega al CleanertController:
- El nodo de airflow correspondiente llama a `POST /clean-dataset`
```json
{
  "s3path": "Path del s3 donde est√° el schema.json",
  "datasetName": "Nombre del dataset",
  "fileType": "CSV|JSON|Excel|Parquet"
}
```
- Dicha informaci√≥n se enruta al CleanerConsultor para que sepa de donde sacar el schema y tenga contexto sobre que hay en cada tabla.

2. Proceso de consulta:
- Una vez recibido el schema.json, mediante Langchain se le consultar√° al m√≥delo de hugging face, para que analice en que tablas es recomendado eliminar data.
- Se le dar√° un prompt con la suficiente informaci√≥n para que genere inferencias con sentido.

3. Se obtendr√° un reccomendations.json que ser√° devuelto al nodo de airflow que hizo el POST.

**6. formatter**

Este microservicio act√∫a como un analista que revisa los tipos de datos para dinero, fechas, decimales, y en general aspectos de formato de un dataset. Gracias a √©l, posteriormente en la base de datos en Redshift se mantendr√° un formato unificado para los tipos de dato.

Funcionar√° gracias a un agent de IA con base en el modelo de hugging face: Phind/Phind-CodeLlama-34B-v2.

Los componentes internos son los siguientes:

- **FormatterController**: Expone el endpoint RESTful del microservicio.
  - `/format-dataset/`: Al llamarlo, se espera obtener las recomendaciones.
- **FormatterConsultor**: Consulta al agente LLM con el esquema extra√≠do para obtener sugerencias sobre que tipos de datos hay que cambiar.

El flujo principal ser√≠a el siguiente:

1. La consulta llega al FormatterController:
- El nodo de airflow correspondiente llama a `POST /clean-dataset`
```json
{
  "s3path": "Path del s3 donde est√° el schema.json",
  "datasetName": "Nombre del dataset",
  "fileType": "CSV|JSON|Excel|Parquet"
}
```
- Dicha informaci√≥n se enruta al FormatterConsultor para que sepa de donde sacar el schema y tenga contexto sobre que hay en cada tabla.

2. Proceso de consulta:
- Una vez recibido el schema.json,  mediante Langchain se le consultar√° al m√≥delo de hugging face, para que analice en que tablas es modificar tipos de datos.
- Se le dar√° un prompt con la suficiente informaci√≥n para que genere inferencias con sentido.
- En el prompt se le dir√° la lista completa de tipos de datos de uso interno en Redshift, por ejemplo:
  - INTEGER: para enteros.
  - REAL: Para n√∫meros punto flotante.
  - DATE: Con DD-MM-YYYY, para las fechas.
  - TEXT: Para los campos de strings/texto.
  - ...

3. Se obtendr√° un reccomendations.json que ser√° devuelto al nodo de airflow que hizo el POST.

**7. ai-stager**

Este microservicio es el analista m√°s importante desde el punto de vista de posterior an√°lis RAG dentro de Redshift. Se encargar√° de decir que contenido debe ser agregado a las columnas de CategoriaSemantica y Descripcion sobre todas las tablas del dataset.

Funcionar√° gracias a un agent de IA con base en el modelo de hugging face: Phind/Phind-CodeLlama-34B-v2. Sin embargo tambi√©n se usar√° como ayuda la biblioteca de NLP de spaCy.

Los componentes internos son los siguientes:

- **AIStagerController**: Expone el endpoint RESTful del microservicio.
  - `/enrich-dataset/`: Al llamarlo, se espera obtener las recomendaciones.
- **AIStagerConsultor**: Consulta al agente LLM con el esquema extra√≠do para obtener sugerencias sobre que metadata ponerle a los registros, adem√°s de usar spaCy para enriquecer el contexto.

El flujo principal ser√≠a el siguiente:

1. La consulta llega al AIStagerController:
- El nodo de airflow correspondiente llama a `POST /enrich-dataset`
```json
{
  "s3path": "Path del s3 donde est√° el schema.json",
  "datasetName": "Nombre del dataset",
  "fileType": "CSV|JSON|Excel|Parquet"
}
```
- Dicha informaci√≥n se enruta al AIStagerConsultor para que sepa de donde sacar el schema y tenga contexto sobre que hay en cada tabla.

2. Enriquecimiento de consulta:
  - Antes de enviar la informaci√≥n al agente para generar las descripciones y categor√≠as sem√°nticas, se utilizar√° spaCy para analizar los nombres de tablas, columnas, tipos de datos y los primeros N registros. Este an√°lisis extraer√° entidades que enriquecer√°n el contexto disponible. Aunque spaCy por s√≠ sola podr√≠a proporcionar la funci√≥n de categorizaci√≥n sem√°ntica, se emplear√° solo como una etapa previa al an√°lisis del agente para obtener mejores resultados.

3. Proceso de consulta:
- Una vez recibido el schema.json y el enriquecimiento de Spacy, mediante Langchain se realiza una cadena de consultas a Phind/Phind-CodeLlama-34B-v2 de Hugging Face sobre:
  - Descripci√≥n: Con base en los primeros N registros, nombres de tablas, columnas y tipos de datos, el sistema debe identificar patrones y generar descripciones espec√≠ficas para cada patr√≥n detectado.
  - Categorizaci√≥n Sem√°ntica: Utilizando la misma informaci√≥n de la descripci√≥n m√°s las entidades extra√≠das por spaCy, el sistema generar√° categor√≠as sem√°nticas para todos los registros bas√°ndose en los patrones identificados.

- Para cada prompt se tendr√° un .txt base al cual simplemente se le a√±adir√° el contexto obtenido por medio del Schema.json y las entidades de spaCy para realizar consultas completas.

3. Se obtendr√° un recomendations.json que ser√° devuelto al nodo de airflow que hizo el POST.

**8. merger**

Este microservicio se encarga de obtener informaci√≥n general sobre los datasets que el colectivo ya tiene almacenados, para posteriormente enviar esta informaci√≥n al agente de IA. El agente evaluar√° si es conveniente remover alguna tabla del proceso de transformaci√≥n actual y fusionarla con alguna tabla existente en Redshift. A diferencia de los analistas previos, este lo que busca es encontrar sugerencias sobre como debe ser el proceso de inserci√≥n a Redshift, no es tanto sobre transformaci√≥n de datos, es m√°s orientado al dise√±o.

Funcionar√° gracias a un agent de IA con base en el modelo de hugging face: Phind/Phind-CodeLlama-34B-v2.

Los componentes internos son los siguientes:

- **MergerController**: Expone el endpoint RESTful del microservicio.
  - `/merge-dataset/`: Al llamarlo, se espera obtener las recomendaciones.
- **RedshiftExtractor**: Extrae informaci√≥n sobre las tablas y sus respectivas columnas, de los datasets del colectivo
- **MergeConsultor**: Consulta al agente LLM con el esquema extra√≠do y la informaci√≥n de Redshift si vale la pena fusionar alguno de los datasets

El flujo principal ser√≠a el siguiente:

1. Recepci√≥n de Consulta:
- El nodo de airflow correspondiente llama a `POST /enrich-dataset`
```json
{
  "s3path": "Path del s3 donde est√° el schema.json",
  "datasetName": "Nombre del dataset",
  "fileType": "CSV|JSON|Excel|Parquet"
}
```
- Dicha informaci√≥n se enruta al MergeConsultor para que sepa de donde sacar el schema y tenga contexto sobre que hay en cada tabla.

2. Obtenci√≥n de contenido de Redshift:
  - El RedshiftExtractor se encargar√° de traer todos los nombres de Columnas y Tablas de los datasets que tenga el Colectivo en la plataforma:
    - Revisar√° en RDS la tabla que marca que Datasets pertencen al colectivo, y adem√°s la tabla que detalla que tablas de Redshift pertenecen a cada Dataset.
    - Mediante un Script de SQLAlchemy obtendr√° todos los resultados.
    - Se genera un archivo json con los schemas obtenidos.
    - Si el colectivo nunca ha subido un dataset, entonces este paso se omite.

3. Proceso de consulta:
- Una vez recibido el schema.json y el schema-redshift.json de los datasets ya subidos, mediante Langchain se le realizara consulta a Phind/Phind-CodeLlama-34B-v2 de Hugging Face sobre:
  - Merge: Se le dar√° un prompt con un contexto sobre que consideramos un Merge en nuestro sistema, junto con ambos Json con schemas, para que infiera que tablas pueden ser unidas, cuales deben insertarse directamente, cuales deben ser actualizadad.

3. Se obtendr√° un recomendations.json que ser√° devuelto al nodo de airflow que hizo el POST.


**9. optimizer**

Este microservicio es el agente final del proceso de Transformaci√≥n y Dise√±o. Tendr√° que inferir con base en el Schema.json de la √∫ltima transformaci√≥n, si vale la pena definir SortKeys y DistKeys a la hora de insertar en Redshift. Al igual que el merger, este analista lo que busca es encontrar sugerencias sobre como debe ser el proceso de inserci√≥n a Redshift, no es tanto sobre transformaci√≥n de datos, es m√°s orientado al dise√±o.

Funcionar√° gracias a un agent de IA con base en el modelo de hugging face: Phind/Phind-CodeLlama-34B-v2.

Los componentes internos son los siguientes:

- **OptimizerController**: Expone el endpoint RESTful del microservicio.
  - `/optimize-dataset/`: Al llamarlo, se espera obtener las recomendaciones.
- **OptimizerConsultor**: Consulta al agente LLM con el esquema extra√≠do si vale la penas definir DistKeys, SortKeys, y en caso de que s√≠, en donde.

El flujo principal ser√≠a el siguiente:

1. Recepci√≥n de Consulta:
- El nodo de airflow correspondiente llama a `POST /optimize-dataset`
```json
{
  "s3path": "Path del s3 donde est√° el schema.json",
  "datasetName": "Nombre del dataset",
  "fileType": "CSV|JSON|Excel|Parquet"
}
```
- Dicha informaci√≥n se enruta al OptimizerConsultor para que sepa de donde sacar el schema y tenga contexto sobre que hay en cada tabla.

2. Proceso de consulta:
- Una vez recibido el schema.json, mediante Langchain se le consultar√° al m√≥delo de hugging face, para que analice en que tablas es crear optimizaciones de b√∫squeda.
- Se le dar√° un prompt con la suficiente informaci√≥n para que genere inferencias con sentido.
- Adem√°s se le proveeran que criterios de aceptaci√≥n debe cumplir una columna para poder ser apta para SortKeys y DistKeys.

3. Se obtendr√° un recomendations.json que ser√° devuelto al nodo de airflow que hizo el POST.


**10. uploader**

Este microservicio representa el agente final del flujo de Airflow. Su funci√≥n es desarrollar una estrategia de inserci√≥n del dataset a Redshift, utilizando como base el √∫ltimo archivo parquet generado por los procesos de transformaci√≥n y los archivos de SQLAlchemy creados por el optimizer y merger.

Funcionar√° gracias a un agent de IA con base en el modelo de hugging face: Phind/Phind-CodeLlama-34B-v2.

Los componentes internos son los siguientes:

- **UploaderController**: Expone el endpoint RESTful del microservicio.
  - `/optimize-dataset/`: Al llamarlo, se espera obtener las recomendaciones.
- **UploaderConsultor**: Consulta al agente LLM con el schema final y los archivos de SQLAlchemy para que sugiera como debe ser el archivo de inserci√≥n definitivo.

El flujo principal ser√≠a el siguiente:

1. Recepci√≥n de Consulta:
- El nodo de airflow correspondiente llama a `POST /optimize-dataset`
```json
{
  "s3path": "Path del s3 donde est√° el schema.json",
  "datasetName": "Nombre del dataset",
  "fileType": "CSV|JSON|Excel|Parquet"
}
```
- Dicha informaci√≥n se enruta al UploaderConsultor para que sepa de donde sacar el schema y los archivos de SQLAlchemy y tenga contexto sobre que hay en cada tabla.

2. Proceso de consulta:
- Una vez recibido el schema.json y los archivos de SQLAlchemy. mediante Langchain se realiza una cadena de consultas a Phind/Phind-CodeLlama-34B-v2 de Hugging Face sobre:
- Inserci√≥n: Define cuales son las tablas que pueden ser insertadas directamente con el comando de COPY.
- Update: Ubica cuales son las tablas que se deben mergear y sugiere como hacerle update a la existente en Redshift. Debe tomar en cuenta t√©cnicas como aprovechar Ids Incrementales para saber solo cuales insertar, y timestamps para saber a cuales registros hacer update.
- Optimizador: Verifica que los SortKeys y DistKeys solo se est√©n aplicando a tablas nuevas.

3. Se obtendr√° un recomendations.json que ser√° devuelto al nodo de airflow que hizo el POST.

**11. schema-enforcer**

Este microservicio es exclusivo a los procesos de carga recurrente, ya que se encarga de revisar que la data traida de la fuente tenga el mismo schema que se uso durante le proceso de transformaci√≥n. En caso de no coincidir, da las recomendaciones para que m√°s adelante se pueda crear la transformaci√≥n para que el dataset qued√© con el formato necesitado.

Funcionar√° gracias a un agent de IA con base en el modelo de hugging face: Phind/Phind-CodeLlama-34B-v2.

Los componentes internos son los siguientes:

- **SchemaEnforcerController**: Expone el endpoint RESTful del microservicio.
  - `/enforce-schema/`: Al llamarlo, se espera obtener las recomendaciones.
- **SchemaEnforcerConsultor**: Revisa con base en el schema del schema-extractor viejo y el nuevo, que diferencias hay, y da recomendaciones de como unificar los formatos

El flujo principal ser√≠a el siguiente:

1. Recepci√≥n de Consulta:
- El nodo de airflow correspondiente llama a `POST /optimize-dataset`
```json
{
  "s3path": "Path del s3 donde est√° el schema.json",
  "datasetName": "Nombre del dataset",
  "fileType": "CSV|JSON|Excel|Parquet"
}
```
- Dicha informaci√≥n se enruta al SchemaEnforcerConsultor para que sepa de donde sacar el schema viejo y el nuevo.

2. Proceso de consulta:
- Una vez recibido los schema.json mediante Langchain se realizan consultas a Phind/Phind-CodeLlama-34B-v2 de Hugging Face sobre que diferencias nota en ambos Schemas, y en caso de que las haya resaltar cuales son y donde.

3. Se obtendr√° un recomendations.json que ser√° devuelto al nodo de airflow que hizo el POST.


### Diagramas de Clases

A continuaci√≥n se presenta la estructura de clases utilizada por cada uno de los microservicios. Es importante aclarar que, aunque algunas clases se repiten entre diagramas, en realidad son las mismas. Sin embargo, dado que cada microservicio funciona como una unidad aislada, se ha generado un diagrama independiente para cada uno.

Otro punto a destacar es que, aunque en la capa de "consultores" dichas clases realizan una funci√≥n similar con distintas l√≥gicas (lo que podr√≠a interpretarse como un Strategy pattern), esta relaci√≥n no se refleja expl√≠citamente. Esto se debe a que, dentro de cada microservicio, solo existe un √∫nico consultor activo, por lo que no es necesario mostrar dicho dise√±o.


**1. schema-extractor**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Verde: Representa un strategy.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el SchemaExtractorController, que act√∫a como facade para que el Airflow se comunique con este microservicio. Este controlador delega las llamadas directamente a un SchemaExtractor dependiendo del tipo de archivo que sea el dataset, se usa strategy para que dentro de esa clase Pandas pueda adaptarse.

Luego para poder acceder a los datasets se usa el DatasetHandler que se encarga de traer los archivos en S3.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesMT1.png)


**2. spark-builder**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el SparkBuilderController, que act√∫a como facade para que el Airflow se comunique con este microservicio. Este controlador delega las llamadas directamente al SparkBuilder que se encarga de orquestrar la adquisi√≥n del schema.json, para poder mediante el Contexter usandol√≥ y las reccomendations crear el prompt para el QwenCoder que har√° el script final de PySpark.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesMT2.png)


**3. sql-builder**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el SqlBuilderController, que act√∫a como facade para que el Airflow se comunique con este microservicio. Este controlador delega las llamadas directamente al SqlBuilder que se encarga de orquestrar la adquisi√≥n del schema.json y el reccomendations para con el contexter crear el prompt para el SqlCoder que har√° el script final de SQLAlchemy.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesMT3.png)


**4. schema-architect**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el SchemaArchitectController, que act√∫a como facade para que el Airflow se comunique con este microservicio. Este controlador delega las llamadas directamente al SchemaConsultor que se encarga de orquestrar la adquisi√≥n del schema.json para con el contexter crear el prompt para el SchemaAnalyst logre realizar las recomendaciones sobre el dataset sobre modelado, normalizaci√≥n, y nombres significativos.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesMT4.png)

**5. cleaner**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el CleanerController, que act√∫a como facade para que el Airflow se comunique con este microservicio. Este controlador delega las llamadas directamente al CleanerConsultor que se encarga de orquestrar la adquisi√≥n del schema.json para con el contexter crear el prompt para el GarbageAnalyst logre realizar las recomendaciones sobre el dataset con respecto a que en que tablas eliminar registros.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesMT5.png)

**6. formatter**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el FormatterController, que act√∫a como facade para que el Airflow se comunique con este microservicio. Este controlador delega las llamadas directamente al FormatterConsultor que se encarga de orquestrar la adquisi√≥n del schema.json para con el contexter crear el prompt para el FormatAnalyst logre realizar las recomendaciones sobre el dataset con respecto a que tipos de datos o formatos no coinciden con los de Redshift internamente.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesMT6.png)

**7. ai-stager**
Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el AIStagerController, que act√∫a como facade para que el Airflow se comunique con este microservicio. Este controlador delega las llamadas directamente al AIStagerConsultor que se encarga de orquestrar la adquisi√≥n del schema.json para con el contexter, obtener del SpacyEntityExtractor contexto adicional sobre los registros del sistema. Posteriormente, crear el prompt para el AIStagerAnalyst logre realizar las recomendaciones sobre el dataset con respecto a que campos de Descriptions y Categor√≠asSemanticas se le puede asignar a los registros del dataset.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesMT7.png)


**8. merger**
Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el MergerController, que act√∫a como facade para que el Airflow se comunique con este microservicio. Este controlador delega las llamadas directamente al MergeConsultor que se encarga de orquestrar la adquisi√≥n del schema.json para con el contexter, y obtener los Schemas de los datasets del Colectivo. Posteriormente, crear el prompt para que el MergeAnalyst logre realizar las recomendaciones sobre que tablas pueden ser mergeables entre las del dataset y las de Redshift

Finalmente, existe una capa de repositorios gestionado mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesMT8.png)

**9. optimizer**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el OptimizerController, que act√∫a como facade para que el Airflow se comunique con este microservicio. Este controlador delega las llamadas directamente al OptimizerConsultor que se encarga de orquestrar la adquisi√≥n del schema.json para con el contexter crear el prompt para que el OptimizerAnalyst logre realizar las recomendaciones sobre que DistKeys y SortKeys usar en redshift.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesMT9.png)

**10. uploader**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el UploaderController, que act√∫a como facade para que el Airflow se comunique con este microservicio. Este controlador delega las llamadas directamente al UploaderConsultor que se encarga de orquestrar la adquisi√≥n del schema.json y los archivos de SQLAlchemy generados por el optimizer y merger, para con el contexter crear el prompt para que el UploaderAnalyst logre desarrollar la estrategia de subida del dataset a Redshift.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesMT10.png)

**11. schema-enforcer**

Primeramente, los patrones de dise√±o orientados a objetos utilizados son los siguientes:

- Morado: Representa un facade.
- Celeste: Muestra un factory.
- Caf√©: Representa un singleton.

Ahora bien, las clases est√°n organizadas de la siguiente manera:

El punto de entrada es el SchemaEnforcerController, que act√∫a como facade para que el Airflow se comunique con este microservicio. Este controlador delega las llamadas directamente al SchemaEnforcerConsultor que se encarga de orquestrar la adquisi√≥n de los schema.json, para con el contexter crear el prompt para que el SchemaAnalyst logre ubicar cuales son las diferencias entre ambos.

Finalmente, existe una capa de repositorios gestionada mediante el patr√≥n Factory. Adem√°s, cada conexi√≥n se maneja utilizando el patr√≥n Singleton.

![identity clases](img/ClasesMT11.png)

### Arquitectura de Airflow

Apache Airflow ser√° el coordinador de este componente, por lo que tendr√° dos responsabilidades principales:
- Disparar el DAG de transformaci√≥n inicial.
- Programar los Crons asociados a los datasets con carga recurrente.

Para iniciar el proceso de transformaci√≥n, habr√° un DAG en Airflow que se ejecuta cada 30 segundos y escucha a RabbitMQ en busca de mensajes del centro de carga. Estos mensajes indican cu√°ndo hay un nuevo dataset listo para procesar e incluyen la ruta en S3, el nombre del dataset y su tipo.

Con esta informaci√≥n, se activa el DAG principal de transformaci√≥n, que est√° dise√±ado con par√°metros din√°micos para adaptarse a cualquier dataset. El grafo de este DAG sigue la siguiente estructura:

``` txt
(schema-extractor -> schema-architect -> spark-builder) -> execute-spark -> (schema-extractor -> cleaner -> spark-builder) -> execute-spark -> (schema-extractor -> formatter -> spark-builder) -> execute-spark -> (schema-extractor -> ai-stager -> spark-builder) ->  execute-spark -> (merger -> sql-builder) -> (optimizer -> sql-builder) -> (uploader -> sql-builder) -> execute-sql.
```

En la representaci√≥n del DAG se pueden identificar cuatro etapas de transformaci√≥n seguidas por tres etapas de c√°lculo e inserci√≥n en Redshift. Cada una de estas fases se ejecuta de forma secuencial, asegurando un flujo ordenado y trazable de los datos.

Por otro lado, el sistema de carga recurrente se apoya en el Scheduler de Airflow. Se implementar√°n 24 DAGs programados (uno por cada hora del d√≠a), adem√°s de tres DAG adicionales que corren cada 12, 6 y 3 horas. Todos estos DAGs reutilizan una misma plantilla din√°mica, que realiza lo siguiente:

- Consulta la tabla DatasetCrons en RDS.
- Filtra los datasets cuya expresi√≥n CRON coincida con la hora de ejecuci√≥n actual.
- Itera sobre los resultados y lanza, para cada uno, una ejecuci√≥n espec√≠fica del DAG de transformaci√≥n parametrizado.

``` python
TriggerDagRunOperator(
    task_id="trigger_etl",
    trigger_dag_id="etl_pipeline_dag",
    conf=dataset_pipeline("covid_datos", "s3://bronze_data/covid_datos"),
)
```

Este DAG se encargar√° de aplicar el siguiente Grafo:
``` txt
(schema-extractor -> schema-enforcer -> spark-builder) -> execute-spark -> execute-spark -> execute-spark ->  execute-spark -> execute-sql.
```
Este DAG tiene una estructura m√°s simple, ya que introduce √∫nicamente una fase adicional al inicio: la verificaci√≥n del esquema del dataset. En esta etapa se compara el dataset de origen con el schema.json previamente almacenado. Si se detectan cambios, el esquema se ajusta.

Una vez validado o actualizado el esquema, el proceso contin√∫a aplicando los scripts de PySpark generados previamente por el motor de transformaci√≥n, los cuales se encuentran guardados en su ruta correspondiente en S3. Finalmente, se realiza la inserci√≥n o actualizaci√≥n de los datos en Redshift con base en lo que se defini√≥ tambi√©n en el uploader.

Un punto adicional a aclarar es que la base de datos interna de Airflow ser√° una pequ√±e√±a instancia de Postgres.

Adem√°s, para la comunicaci√≥n de errores en los procesos de un DAG Airflow permite conectar con AWS SES, por lo que en caso de que un paso falle se le enviar√° un correo a los administradores de la plataforma para que resuelvan manualmente el error.

Finalmente, ya que se usar√° Airflow autogestionado se optar√° por la opci√≥n de CeleryExecutor, el cual coordina una cola de rabbitMQ entre los nodos workers, para que estos puedan seleccionar DAGs a ejecutar seg√∫n su disponibilidad. Esta opci√≥n permite escalar los workers horizontalmente en caso de ser necesario.



### Arquitectura de Spark

Para utilizar Spark, se adoptar√° un enfoque autogestionado desplegando el Helm Chart oficial de Bitnami sobre Kubernetes. Inicialmente, se configurar√°n 9 nodos worker, aunque antes del despliegue definitivo se realizar√°n pruebas de carga para validar si esta cantidad resulta suficiente o si es necesario escalar horizontalmente.

La ejecuci√≥n de scripts PySpark se realizar√° mediante el comando spark-submit, el cual ser√° lanzado desde los nodos de Airflow llamados execute-spark. Estos nodos enviar√°n las tareas al nodo master de Spark, que se encargar√° de coordinar la ejecuci√≥n distribuida, dividiendo autom√°ticamente la carga de trabajo entre los workers del cl√∫ster.



### Servicios de AWS

**Amazon EKS (Elastic Kubernetes Service)**

El cluster de Kubernetes opera como el n√∫cleo computacional donde residen todos los microservicios del Motor de Transformaci√≥n, activ√°ndose autom√°ticamente cuando el Centro de Carga notifica la disponibilidad de nuevos datasets para procesar.

**Configuraci√≥n de Hardware:**

- **Versi√≥n de Kubernetes**: 1.29 (alineada con el resto del ecosistema)
- **Tipo de nodos**: Amazon EC2 t3.large (2 vCPU, 8 GB RAM)
- **Escalado**: 3-15 nodos que se expanden durante picos de carga nocturna cuando los datasets programados se procesan en batch
- **Almacenamiento**: EBS gp3 con 100 GB por nodo para checkpoints temporales de Spark
- **Red**: VPC privada que facilita comunicaci√≥n segura con RDS y Redshift durante las transformaciones


**AWS SES**
Servicio para env√≠o de correos electr√≥nicos confiables y escalables.

- **Configuraci√≥n:**
  - **Regi√≥n:** us-east-1.
  - **Identidad verificada:** Dominios y correos electr√≥nicos verificados.
  - **Pol√≠ticas de env√≠o:** Limitaciones y tasas configuradas para evitar bloqueos.
  - **Autenticaci√≥n:** SPF, DKIM y DMARC configurados para mejorar entregabilidad.


### Monitoreo

**Prometheus en EKS - Recolecci√≥n Contextual**

Prometheus opera continuamente dentro del cluster, pero intensifica la recolecci√≥n de m√©tricas durante ventanas de procesamiento activo, adaptando la frecuencia de scraping seg√∫n la carga operacional.

**Momentos de alta frecuencia:**
Durante ejecuci√≥n de jobs Spark masivos, el scrape interval se reduce a 15 segundos para capturar m√©tricas granulares de memory spill, shuffle operations y task failures. Fuera de estas ventanas, vuelve a 60 segundos para optimizar recursos.

**ServiceMonitors adaptativos:**

- **etl-orchestrator**: Intensifica monitoreo cuando coordina m√∫ltiples jobs concurrentes, especialmente durante cargas batch nocturnas
- **airflow-scheduler**: Monitoreo continuo con alertas que se activan cuando la cola de tareas supera umbrales definidos din√°micamente seg√∫n patrones hist√≥ricos
- **spark-jobs**: M√©tricas se recolectan solo durante ejecuci√≥n activa, eliminando overhead cuando no hay procesamiento

**AWS CloudWatch - Monitoreo de Servicios Subyacentes**

CloudWatch captura autom√°ticamente m√©tricas de la infraestructura AWS que soporta las transformaciones, correlacionando performance de aplicaci√≥n con salud de servicios subyacentes.


**EKS Cluster Health:**

- **Node utilization**: CPU y memoria de nodos correlacionada con n√∫mero de executors Spark activos
- **Pod startup latency**: Tiempo que toman pods en alcanzar estado Ready durante scaling events
- **API server response time**: Latencia de Kubernetes API durante operaciones de scaling masivo

**AWS X-Ray - Tracing Distribuido**

X-Ray proporciona visibilidad completa del flujo de requests entre microservicios durante transformaciones, identificando bottlenecks espec√≠ficos en el pipeline ETL.

**Traces instrumentados:**

- **End-to-end ETL flow**: Desde notificaci√≥n de RabbitMQ hasta confirmaci√≥n de carga en Redshift, mostrando latencia de cada paso
- **Cross-service calls**: Llamadas entre etl-orchestrator y data-quality-service visualizadas con latencia detallada
- **AWS service interactions**: Latencia de llamadas a Secrets Manager, S3, y KMS durante operaciones cr√≠ticas
- **Database query performance**: Tiempo espec√≠fico de queries a RDS correlacionado con carga concurrente

Los traces permiten identificar r√°pidamente si lentitud proviene de network latency, database contention, o processing logic.

**AWS Config - Compliance Monitoring**

Config monitorea continuamente configuraciones de seguridad y compliance, alertando sobre desviaciones que podr√≠an violar requisitos de la Ley 8968.

**Rules configuradas:**

- **EKS security groups**: Valida que solo puertos necesarios est√©n abiertos y que tr√°fico sea restringido a subnets autorizadas
- **S3 bucket encryption**: Asegura que todos los buckets del Motor mantengan cifrado habilitado con keys apropiadas
- **RDS security configurations**: Monitorea que cifrado en tr√°nsito y en reposo permanezca habilitado

Las violaciones activan autom√°ticamente remediation workflows que revierten cambios no autorizados.

**AWS CloudTrail - Auditor√≠a Completa**

CloudTrail registra todas las API calls realizadas por microservicios del Motor, proporcionando trazabilidad completa para auditor√≠as de compliance e investigaci√≥n de incidentes.

**Eventos auditados:**

- **S3 data access**: Cada lectura/escritura de datos durante transformaciones, incluyendo IP source y timestamp exacto
- **RDS connections**: Establecimiento de conexiones desde pods EKS hacia RDS con identificaci√≥n precisa de workload

Los logs se integran con sistemas de SIEM gubernamentales cuando se procesan datasets de entidades p√∫blicas.

**Grafana - Visualizaci√≥n Contextual del Pipeline**

**Dashboard "ETL Pipeline Flow":**
Combina m√©tricas de Prometheus, CloudWatch y trazas de X-Ray en visualizaci√≥n unificada que muestra datasets fluyendo desde Centro de Carga hasta La B√≥veda, con drill-down capability hacia traces espec√≠ficos cuando hay problemas.

**Dashboard "Security & Compliance":**
Integra datos de Config, CloudTrail y CloudWatch para mostrar postura de seguridad en tiempo real, incluyendo encryption status, access patterns y compliance violations con alertas visuales inmediatas.

**Dashboard "Cost Optimization":**
Correlaciona m√©tricas de utilizaci√≥n de recursos con costos generados, mostrando cost-per-transformation y sugiriendo optimizaciones autom√°ticas basadas en patterns hist√≥ricos.

### Diagrama del backend

A continuaci√≥n se presenta el diagrama del backend del Motor de transforamci√≥n. En √©l se evidencia c√≥mo todo el ecosistema de AWS interact√∫a con los distintos microservicios desplegados en el cl√∫ster de Kubernetes provisto por EKS. Tambi√©n se describen los microservicios internos junto a sus distintas clases, los patrones de dise√±o utilizados, y c√≥mo interact√∫an con Airflow, para que posteriormente este se encarga de delegar los jobs a spark.

Se muestra c√≥mo la contenerizaci√≥n de cada microservicio se realizar√° utilizando Docker, y c√≥mo el monitoreo interno ser√° gestionado por Prometheus. Adem√°s, se destaca que en la capa externa a AWS donde se encuentra como Hugging Face funciona como fachada para interactuar con los 3 M√≥delos de inteligencia artificial principales.

![image](img/DiagramaBackendMT.svg)



# 4.5 MarketPlace

## Dise√±o del Frontend

### Arquitectura del Cliente 

Nuestra arquitectura de cliente consistir√° en Client Side Rendering con rendering est√°tico, con una √∫nica capa dedicada a la web. Esta decisi√≥n se toma porque los bundles de React generados en el build de cada proyecto ser√°n almacenados en un bucket de S3, el cual ser√° servido a los clientes mediante el CDN provisto por CloudFront.

Adem√°s, para acceder al backend se utilizar√° una √∫nica API, desarrollada en FastAPI alojada en EKS.

### Patrones de Dise√±o de Objetos 

El dise√±o del frontend del componente Marketplace de Data Pura Vida sigue principios de dise√±o orientado a objetos que buscan flexibilidad, mantenibilidad y escalabilidad. Los principales patrones aplicados son los siguientes:

#### 1 **Patr√≥n de Strategy**

- Ubicaci√≥n: En los filtros de b√∫squeda de datasets.
- Descripci√≥n: El frontend permite al usuario aplicar distintos tipos de filtros (por precio, categor√≠a, tipo de dataset, popularidad, etc). Cada filtro implementa una estrategia diferente de ordenamiento o filtrado, pero todos heredan de una interfaz com√∫n, lo que permite agregar nuevos filtros en el futuro sin modificar el flujo principal.
- Beneficio: Permite extender f√°cilmente nuevos criterios de b√∫squeda sin alterar el resto del sistema.

#### 2Ô∏è **Patr√≥n de Singleton**

- Ubicaci√≥n: Cliente HTTP centralizado (por ejemplo ApiConnector o MarketplaceApiClient).
- Descripci√≥n: Todo el frontend utiliza una √∫nica instancia para gestionar las conexiones al backend (requests HTTP a la API REST de Marketplace).
- Beneficio: Garantiza un √∫nico punto de configuraci√≥n de headers, manejo de tokens, interceptores de error, y manejo centralizado de respuestas.

#### 3Ô∏è **Patr√≥n de Observer (Pub-Sub)**

- Ubicaci√≥n: Sistema de notificaciones y actualizaci√≥n de componentes de UI.
- Descripci√≥n: Algunos componentes de la interfaz est√°n suscritos a eventos globales como la finalizaci√≥n de una compra, actualizaci√≥n de un dataset o expiraci√≥n de accesos.
- Beneficio: Desacopla los componentes visuales del flujo de negocio, permitiendo que reaccionen a eventos sin depender directamente unos de otros.

#### 4Ô∏è **Patr√≥n de Facade**

- Ubicaci√≥n: M√≥dulo de servicios de pago.
- Descripci√≥n: Las operaciones de compra, validaci√≥n de pagos, visualizaci√≥n de precios y confirmaci√≥n de compra son orquestadas desde un √∫nico m√≥dulo de servicios, el cual encapsula la comunicaci√≥n con Stripe y la l√≥gica de negocio asociada.
- Beneficio: Simplifica el uso de APIs externas, ocultando la complejidad de validaciones, formatos de respuesta y errores.

#### 5Ô∏è Patr√≥n MVVM (Model-View-ViewModel)

- Ubicaci√≥n: Arquitectura general del frontend.
- Descripci√≥n:
  - Model: Define los objetos de negocio como Dataset, Order, PaymentTransaction.
  - ViewModel: Implementado mediante custom hooks como useDatasetSearch(), useMarketplaceCart().
  - View: Los componentes visuales de React, organizados bajo Atomic Design.
- Beneficio: Separa de forma clara la l√≥gica de presentaci√≥n, la l√≥gica de negocio y el manejo de estado de UI.

### Estructura de Carpetas del Sistema 

El frontend del componente Marketplace sigue una estructura modular basada en el patr√≥n de dise√±o Atomic Design, el patr√≥n MVVM y principios de escalabilidad y mantenibilidad. La organizaci√≥n permite extender f√°cilmente nuevos m√≥dulos de negocio dentro del Marketplace.

```plaintext
frontend/
‚îú‚îÄ‚îÄ public/                     # Archivos est√°ticos
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/                    # L√≥gica de conexi√≥n con el backend (Axios + interceptores)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ marketplaceApi.ts   # Endpoints espec√≠ficos del Marketplace
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ authApi.ts          # Autenticaci√≥n general v√≠a Cognito
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                 # Definici√≥n de los modelos de negocio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dataset.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Order.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Payment.ts
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ hooks/                  # ViewModels (gesti√≥n de estado y l√≥gica de UI)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useDatasetSearch.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useCart.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ usePayment.ts
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ components/             # Componentes visuales seg√∫n Atomic Design
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ atoms/              # Botones, inputs, etiquetas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ molecules/          # Formularios de b√∫squeda, carritos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ organisms/          # Composici√≥n de vistas completas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates/          # Layouts reutilizables
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ pages/                  # Rutas principales del sistema
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MarketplaceHome.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DatasetDetails.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Cart.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Checkout.tsx
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ contexts/               # Contexto global de usuario y carrito
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ UserContext.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CartContext.tsx
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ services/               # L√≥gica externa: pagos, facturaci√≥n, etc.
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stripeService.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ invoiceService.ts
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/                  # Funciones utilitarias comunes
‚îÇ   ‚îî‚îÄ‚îÄ App.tsx                 # Punto de entrada de la aplicaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ amplify/                    # Configuraci√≥n de AWS Amplify y Cognito
‚îÇ   ‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îî‚îÄ‚îÄ aws-exports.js
‚îÇ
‚îî‚îÄ‚îÄ tests/                      # Pruebas unitarias e integraci√≥n
    ‚îú‚îÄ‚îÄ unit/
    ‚îî‚îÄ‚îÄ integration/
```


### Tecnolog√≠as utilizadas en el cliente

| Tecnolog√≠a    | Descripci√≥n                                |
| ------------- | ------------------------------------------ |
| React         | Framework principal para UI                |
| Tailwind CSS  | Framework de estilos responsivos           |
| Axios         | Cliente HTTP centralizado                  |
| Stripe        | Gesti√≥n de pagos y facturaci√≥n             |
| React Context | Manejo de estado global (usuario, carrito) |
| React Router  | Control de rutas y navegaci√≥n              |


### Componentes Visuales

#### Patrones y Principios:

- **Dise√±o Responsivo:** Aplicado desde el desarrollo inicial, permitiendo que el Marketplace sea visualizado correctamente en desktop, tablets y m√≥viles. Se utiliza Tailwind para web. Las clases CSS usan unidades relativas (rem, %, vw) y los breakpoints de Tailwind manejan la adaptaci√≥n autom√°tica.

- **SOLID:**
  - Single Responsibility: Cada componente de React cumple una √∫nica funci√≥n. Los componentes visuales est√°n completamente separados de los hooks de l√≥gica.
  - Open/Closed Principle: Los componentes son extensibles sin modificar su c√≥digo interno, como los botones (Button) o tarjetas de datasets (DatasetCard).
  - Liskov Substitution Principle: Las listas de datasets permiten diferentes tarjetas de visualizaci√≥n que pueden reemplazar a las generales seg√∫n el tipo de dataset.
  - Interface Segregation Principle: Gracias a Atomic Design, los componentes solo exponen las props necesarias.
  - Dependency Inversion Principle: La l√≥gica de negocio reside en los ViewModels (custom hooks), manteniendo los componentes visuales desacoplados.

- **DRY (Don't Repeat Yourself):** Los componentes son reutilizables (atoms, molecules). Adem√°s, las funciones utilitarias en `utils/` centralizan validaciones de pago, c√°lculos de carrito, formateo de precios, etc.

- **Separaci√≥n de Responsabilidades:** Los componentes visuales solo presentan la informaci√≥n. Toda la l√≥gica est√° en hooks como `useDatasetSearch()`, `useCart()`, `usePayment()`. Los modelos (Dataset, Order, Payment) manejan la conexi√≥n con la API.

- **Atomic Design:** Implementado en la carpeta `components/`:
  - Atoms: `Button`, `Input`, `Icon`, `Badge`.
  - Molecules: `SearchBar`, `DatasetCard`, `PriceFilter`.
  - Organisms: `DatasetList`, `ShoppingCartPanel`, `PaymentForm`.
  - Templates: `MarketplaceLayout`, `CartLayout`.
  - Pages: `MarketplaceHomePage`, `DatasetDetailsPage`, `CheckoutPage`.

- **MVVM:** 
  - Model: Clases de datos y funciones que manejan las llamadas a la API (ejemplo: `DatasetApi`).
  - View: Componentes visuales React organizados por Atomic Design.
  - ViewModel: Hooks como `useCart()`, `usePayment()`, `useDatasetSearch()` que gestionan la l√≥gica de negocio.

#### Herramientas y est√°ndares:

- Tailwind: Framework principal de estilos responsivos.

## Dise√±o del Backend

### Microservicios 

### marketplace-catalog-service

**Responsabilidad Principal**
Gesti√≥n del cat√°logo de datasets con capacidades avanzadas de b√∫squeda, indexaci√≥n y agregaci√≥n de m√©tricas de calidad.

**Componentes Internos**
- **catalog-metadata-sync-service**: Sincroniza metadatos entre La B√≥veda y marketplace cada 15 minutos
- **catalog-search-engine-service**: Gestiona indexaci√≥n en OpenSearch con soporte multilenguaje
- **catalog-quality-aggregator-service**: Agrega m√©tricas de calidad del Motor de Transformaci√≥n

**Tecnolog√≠as Utilizadas**
- FastAPI para endpoints REST
- OpenSearch para b√∫squeda y analytics
- Redis para cache de queries frecuentes
- PostgreSQL para metadatos de datasets

**APIs Expuestas**
```
GET /api/v1/catalog/search - B√∫squeda avanzada con filtros y faceting
GET /api/v1/catalog/datasets/{id} - Detalles completos de dataset
GET /api/v1/catalog/categories - Listado de categor√≠as con conteos
GET /api/v1/catalog/trending - Datasets trending basados en analytics
GET /api/v1/catalog/recommendations/{user_id} - Recomendaciones personalizadas
```

**Operaci√≥n**
Activo constantemente con picos durante b√∫squedas matutinas (8-10 AM) y sincronizaciones nocturnas (2-4 AM). Opera en EKS con pods distribuidos en availability zones para alta disponibilidad.



### marketplace-payment-service

**Responsabilidad Principal**
Procesamiento de pagos √∫nicos y recurrentes, gesti√≥n de suscripciones, detecci√≥n de fraude y generaci√≥n autom√°tica de facturas.

**Componentes Internos**
- **payment-processor-service**: Procesamiento con validaci√≥n y fraud detection
- **subscription-billing-service**: Gesti√≥n de suscripciones recurrentes y renovaciones
- **invoice-generator-service**: Generaci√≥n autom√°tica de facturas PDF
- **fraud-detection-service**: Risk scoring en tiempo real con ML
- **webhook-handler-service**: Manejo seguro de webhooks de providers

**Integraciones Externas**
- Stripe SDK para pagos internacionales
- Sistema de pagos local costarricense
- Redis para idempotencia de transacciones
- FastAPI para todos los endpoints

**APIs Expuestas**
```
POST /api/v1/payments/initiate - Inicio de proceso de pago
GET /api/v1/payments/{id}/status - Estado de transacci√≥n espec√≠fica
POST /api/v1/subscriptions - Creaci√≥n de nueva suscripci√≥n
PUT /api/v1/subscriptions/{id}/cancel - Cancelaci√≥n de suscripci√≥n
GET /api/v1/invoices - Listado de facturas del usuario
POST /api/v1/payments/webhooks/{provider} - Webhooks de providers
```

**Operaci√≥n**
Alta disponibilidad 24/7 con picos durante horarios de oficina y finales de mes. Distribuido en pods con affinity a nodos dedicados para cumplir PCI DSS compliance.

---

### marketplace-access-service

**Responsabilidad Principal**
Control granular de acceso a datasets, generaci√≥n de tokens JWT, tracking de uso en tiempo real y auditor√≠a completa de accesos.

**Componentes Internos**
- **access-provisioning-service**: Activaci√≥n autom√°tica post-compra (<30 segundos)
- **token-management-service**: Generaci√≥n y gesti√≥n de JWT tokens con TTL de 24 horas
- **usage-tracking-service**: Monitoreo en tiempo real para billing y rate limiting
- **permission-validator-service**: Validaci√≥n granular en <50ms por request
- **audit-logger-service**: Logging completo para compliance (retenci√≥n 7 a√±os)

**Integraciones**
- gRPC clients para La B√≥veda y Bioregistro
- PostgreSQL para access records
- RabbitMQ para coordinaci√≥n de eventos
- OpenSearch para logs de auditor√≠a

**APIs Expuestas**
```
POST /api/v1/access/provision - Provisioning de acceso (interno)
GET /api/v1/access/my-datasets - Datasets accesibles por usuario
POST /api/v1/access/tokens/generate - Generaci√≥n de access token
DELETE /api/v1/access/tokens/{id} - Revocaci√≥n de token
GET /api/v1/access/usage - Estad√≠sticas de uso del usuario
```

**Operaci√≥n**
Ejecuta continuamente con activaci√≥n intensa post-compra. Opera distribuido para manejar m√∫ltiples usuarios simult√°neos durante horarios peak.


### marketplace-user-service

**Responsabilidad Principal**
Gesti√≥n de perfiles comerciales, tracking de comportamiento de navegaci√≥n y preferencias de usuarios con sincronizaci√≥n cross-device.

**Componentes Internos**
- **user-profile-manager-service**: Gestiona perfiles complementando autenticaci√≥n del Bioregistro
- **user-behavior-tracker-service**: Rastrea navegaci√≥n para ML de recomendaciones
- **user-preference-engine-service**: Motor de preferencias con clustering de usuarios
- **user-session-manager-service**: Sesiones distribuidas con sincronizaci√≥n cross-device

**Tecnolog√≠as Especializadas**
- Redis para sesiones distribuidas
- PostgreSQL para storage de preferencias
- DynamoDB para storage de comportamiento
- FastAPI para APIs de usuario

**APIs Expuestas**
```
POST /api/v1/users/register - Registro de nuevo usuario
GET /api/v1/users/me - Perfil completo del usuario actual
PUT /api/v1/users/me/preferences - Actualizaci√≥n de preferencias
POST /api/v1/users/behavior - Tracking de eventos de comportamiento
GET /api/v1/users/me/recommendations - Recomendaciones personalizadas
```

**Operaci√≥n**
Activo 24/7 para gesti√≥n de sesiones globales, con mayor carga durante horarios laborales de Costa Rica (6 AM - 6 PM UTC-6).



### marketplace-recommendation-service

**Responsabilidad Principal**
Motor de recomendaciones personalizadas basado en machine learning, an√°lisis de comportamiento y similitud de contenido.

**Componentes Internos**
- **behavioral-ml-service**: An√°lisis con ML para recommendations personalizadas
- **content-similarity-service**: C√°lculo de similaridad entre datasets
- **recommendation-engine-service**: Motor principal que combina multiple enfoques
- **ab-testing-framework-service**: Framework para testing de algoritmos

**Tecnolog√≠as de ML**
- Amazon SageMaker para model training y deployment
- Hugging Face Transformers para embeddings sem√°nticos
- Redis para cache de recommendations
- FastAPI para endpoints de recomendaciones

**Estrategias Implementadas**
- **Collaborative Filtering**: Usuarios con preferencias similares
- **Content-Based Filtering**: Similitud de metadatos de datasets
- **Hybrid Ensemble**: Combinaci√≥n weighted de ambos enfoques

**APIs Expuestas**
```
GET /api/v1/recommendations/personalized - Recomendaciones personalizadas
GET /api/v1/recommendations/similar/{dataset_id} - Datasets similares
GET /api/v1/recommendations/trending - Trending recommendations
POST /api/v1/recommendations/feedback - Feedback de calidad
```

**Operaci√≥n**
Batch processing nocturno para entrenar modelos, inference en tiempo real <100ms durante navegaci√≥n activa. Pods optimizados para ML inference desplegados en EKS.



### Diagrama de clases

**marketplace-catalog-service**
Este microservicio se encarga de la gesti√≥n del cat√°logo de datasets, incluyendo la metadata, calidad, y la sincronizaci√≥n con el Datalake.

**Patrones de Dise√±o Utilizados:**
-	Morado: Facade
-	Amarillo: Observer
-	Celeste: Factory
-	Caf√©: Singleton
-	Verde: Strategy

**Organizaci√≥n de Clases:**
El punto de entrada principal es el CatalogController, que act√∫a como Facade para las APIs externas del Marketplace (ej., /api/v1/catalog/datasets, /api/v1/catalog/categories). Este controlador delega las operaciones a un Observer central, el CatalogEventManager, encargado de notificar a los m√≥dulos de l√≥gica de negocio relevantes.
Dentro de la l√≥gica de negocio, se encuentran:

-	**DatasetManager:** Responsable de la creaci√≥n, actualizaci√≥n y eliminaci√≥n de datasets y su metadata. Recibe FileHandler y MetadataValidator como dependencias inyectadas.
-	**QualityAggregator:** Escucha eventos de calidad (quality.metrics.updated) y calcula un score consolidado para cada dataset.
-	**SearchIndexer:** Escucha eventos de actualizaci√≥n de datasets (dataset.updated) y coordina la indexaci√≥n de los datos en OpenSearch.
Estos m√≥dulos de l√≥gica de negocio reciben como dependencias los servicios de la segunda capa de Facade:
-	**MetadataFileHandler:** Se encarga de interactuar con AWS S3 para almacenar y recuperar archivos de metadata asociados a los datasets.
-	**OpenSearchIndexer:** Abstrae la comunicaci√≥n con OpenSearch para indexar y actualizar documentos de datasets. Utiliza un patr√≥n Strategy para manejar diferentes tipos de indexaci√≥n (ej., delta vs. completa).
-	**DataQualityService:** Se comunica con el dataset-quality-service (asumido microservicio externo o interno) para obtener m√©tricas de calidad de los datos.

Finalmente, existe una capa de repositorios para la persistencia de datos (PostgreSQL, DynamoDB). Los repositorios son gestionados mediante un patr√≥n Factory, como RepositoryFactory, que provee instancias de DatasetRepository, MetadataRepository, QualityMetricsRepository, etc. Cada conexi√≥n a la base de datos es manejada utilizando el patr√≥n Singleton para optimizar los recursos.

![image](img/ClasesMarketplace1.png)

**marketplace-payment-service**
Este microservicio gestiona todo el flujo de pagos y suscripciones en el Marketplace.

**Patrones de Dise√±o Utilizados:**
-	Morado: Facade
-	Amarillo: Observer
-	Verde: Strategy
-	Celeste: Factory
-	Caf√©: Singleton

**Organizaci√≥n de Clases:**
El punto de entrada es el PaymentController, que act√∫a como Facade para las APIs de pago (/api/v1/payments/initiate, /api/v1/payments/webhook). Este controlador delega las llamadas a un Observer principal, el PaymentEventManager, encargado de notificar a la l√≥gica de negocio seg√∫n el evento de pago o webhook.
Dentro de la l√≥gica de negocio, se encuentran:

-	**PaymentProcessor:** Orquesta el proceso de pago, interactuando con pasarelas de pago externas. Recibe StripeGateway y BACGateway como dependencias.
-	**SubscriptionManager:** Gestiona la creaci√≥n, renovaci√≥n y cancelaci√≥n de suscripciones.
-	**FraudDetector:** Escucha eventos de pago (payment.initiated, payment.completed) y utiliza un patr√≥n Strategy para aplicar diferentes algoritmos de detecci√≥n de fraude.
-	**InvoiceGenerator:** Genera facturas a partir de transacciones de pago completadas.
Estos m√≥dulos de l√≥gica de negocio reciben como dependencias los servicios de la segunda capa de Facade:
-	**StripeGateway:** Abstrae la comunicaci√≥n con la API de Stripe.
-	**BACGateway:** Abstrae la comunicaci√≥n con la API del BAC Credomatic.
-	**NotificationSender:** Se comunica con el notification-service para enviar confirmaciones de pago o alertas.
-	**AccessProvisioner:** Se comunica con el marketplace-access-service para habilitar el acceso a los datasets tras un pago exitoso.

Finalmente, existe una capa de repositorios para la persistencia de datos (PostgreSQL, DynamoDB, Redis). Los repositorios son gestionados mediante un patr√≥n Factory, como RepositoryFactory, que provee instancias de TransactionRepository, SubscriptionRepository, InvoiceRepository, etc. Las conexiones a la base de datos y al cache Redis se manejan utilizando el patr√≥n Singleton.

![image](img/ClasesMarketplace2.png)


**marketplace-access-service**
Este microservicio se encarga de gestionar los permisos de acceso a los datasets y la generaci√≥n de tokens de acceso para los usuarios.

**Patrones de Dise√±o Utilizados:**
-	Morado: Facade
-	Amarillo: Observer
-	Naranja: Dependency Injection
-	Celeste: Factory
-	Caf√©: Singleton

**Organizaci√≥n de Clases:**
El punto de entrada es el AccessController, que act√∫a como Facade para las APIs de acceso (/api/v1/access/grant, /api/v1/access/token). Este controlador delega las llamadas a un Observer principal, el AccessEventManager, encargado de notificar a la l√≥gica de negocio seg√∫n el tipo de solicitud.
Dentro de la l√≥gica de negocio, se encuentran:

-	**PermissionHandler:** Otorga y revoca permisos a los datasets basados en eventos de compra o suscripci√≥n.
-	**TokenManager:** Genera, valida y revoca tokens de acceso JWT.
-	**UsageTracker:** Escucha eventos de acceso (dataset.accessed) y registra el uso de los datasets.
Estos m√≥dulos de l√≥gica de negocio reciben como dependencias los servicios de la segunda capa de Facade:
-	**AuthServiceRequester:** Se comunica con el security-service para validar tokens y autenticar usuarios.
-	**DataLakeAccessManager:** Se comunica con el Datalake (La B√≥veda) para provisionar o revocar el acceso real a los datos en S3.
-	**NotificationSender:** Se comunica con el notification-service para enviar notificaciones de concesi√≥n de acceso o revocaci√≥n.
-	**AuditLogger:** Se comunica con el audit-logger-service (si es un microservicio separado) para registrar eventos de auditor√≠a de acceso.

Finalmente, existe una capa de repositorios para la persistencia de datos (PostgreSQL, Redis). Los repositorios son gestionados mediante un patr√≥n Factory, como RepositoryFactory, que provee instancias de PermissionRepository. Las conexiones a la base de datos y al cache Redis se manejan utilizando el patr√≥n Singleton.

![image](img/ClasesMarketplace3.png)

# Servicios AWS 

## Amazon EKS (Elastic Kubernetes Service)
**Prop√≥sito**: Orquestaci√≥n de microservicios del marketplace

**Configuraci√≥n**:
- **Versi√≥n**: Kubernetes 1.29
- **Nodos**: 3-15 nodos t3.large (2 vCPU, 8 GB RAM)
- **Auto-scaling**: CPU/memoria >70% por 5 minutos
- **Almacenamiento**: EBS gp3 50GB por nodo
- **Red**: VPC privada

**Microservicios desplegados**:
- `marketplace-catalog-service`: Gesti√≥n del cat√°logo y b√∫squedas
- `marketplace-payment-service`: Procesamiento de pagos y suscripciones
- `marketplace-access-service`: Control de acceso a datasets
- `marketplace-notification-service`: Notificaciones del marketplace

## Amazon RDS PostgreSQL
**Prop√≥sito**: Almacenamiento transaccional

**Configuraci√≥n**:
- **Motor**: PostgreSQL
- **Tipo de instancia**: db.t3.medium o superior
- **Almacenamiento**: EBS gp3 escalable
- **Multi-AZ**: Habilitado para alta disponibilidad

**Tablas del marketplace**:
- `MarketplaceOrder`: √ìrdenes de compra
- `Subscription`: Suscripciones activas
- `PaymentTransaction`: Historial de transacciones
- `DatasetRating`: Calificaciones y rese√±as

## Amazon DynamoDB
**Prop√≥sito**: Almacenamiento NoSQL para datos temporales

**Configuraci√≥n**:
- **Modo**: On-Demand
- **TTL**: Habilitado para limpieza autom√°tica

**Tablas**:
- `UserBehaviorMarketplace`: Tracking de navegaci√≥n (TTL: 90 d√≠as)
- `MarketplaceSessionData`: Sesiones de usuario (TTL: 8 horas)
- `DatasetRecommendationCache`: Cache de recomendaciones (TTL: 4 horas)

## Amazon OpenSearch
**Prop√≥sito**: Motor de b√∫squeda de datasets

**Configuraci√≥n**:
- **Versi√≥n**: OpenSearch 2.3
- **Cluster**: 2 nodos t3.small.search
- **Almacenamiento**: 50GB EBS por nodo
- **Seguridad**: VPC privada, HTTPS obligatorio

**√çndices**:
- `datasets-marketplace-catalog`: Metadatos con b√∫squeda full-text
- `user-marketplace-searches`: Historial de b√∫squedas

## Amazon S3
**Prop√≥sito**: Almacenamiento de objetos

**Buckets**:
- **`dpv-marketplace-assets`**: Thumbnails y previews de datasets
- **`dpv-marketplace-invoices`**: Facturas PDF

**Configuraci√≥n**:
- Cifrado SSE-S3 autom√°tico
- Lifecycle policies a Glacier despu√©s de 90 d√≠as
- Versionado habilitado

## AWS Lambda
**Prop√≥sito**: Funciones serverless

**Funciones**:
- **`marketplace-webhook-processor`**: 
  - Procesa webhooks de Stripe
  - Memoria: 512MB, Timeout: 30s
- **`marketplace-invoice-generator`**: 
  - Genera PDFs de facturas
  - Memoria: 1GB, Timeout: 5min
- **`marketplace-search-indexer`**: 
  - Actualiza √≠ndices de OpenSearch
  - Memoria: 256MB, Timeout: 1min

## Amazon API Gateway
**Prop√≥sito**: Gateway de APIs REST

**Configuraci√≥n**:
- Rate limiting: 1000 requests/minuto por usuario
- Autenticaci√≥n v√≠a Cognito
- Caching: 5 minutos para b√∫squedas

## Amazon Cognito
**Prop√≥sito**: Autenticaci√≥n y autorizaci√≥n

**Configuraci√≥n**:
- User Pools para autenticaci√≥n
- MFA obligatorio
- JWT tokens

**Roles del marketplace**:
- `marketplace:buyer`: Usuarios que pueden comprar
- `marketplace:seller`: Organizaciones que venden datasets

## AWS Secrets Manager
**Prop√≥sito**: Gesti√≥n segura de credenciales

**Secrets almacenados**:
- `dpv/marketplace/stripe-keys`: Claves de Stripe
- `dpv/marketplace/db-credentials`: Credenciales de RDS

**Configuraci√≥n**:
- Cifrado con AWS KMS
- Rotaci√≥n autom√°tica configurada

## Amazon SES
**Prop√≥sito**: Env√≠o de emails

**Configuraci√≥n**:
- Dominio verificado: `marketplace@datapuravida.cr`
- Templates para confirmaciones, facturas y notificaciones
- Bounce handling autom√°tico
- Pol√≠ticas anti-spam

## AWS KMS
**Prop√≥sito**: Gesti√≥n de claves de cifrado

**Claves especializadas**:
- `dpv-marketplace-payments`: Datos de transacciones
- `dpv-marketplace-assets`: Assets y documentos
- `dpv-marketplace-analytics`: Datos de comportamiento

**Configuraci√≥n**:
- Rotaci√≥n autom√°tica anual
- Pol√≠ticas de acceso por servicio
###### Sistema de Monitoreo
El monitoreo del componente Marketplace de Datos de Data Pura Vida ser√° utilizado para lograr que todo funcione bien, sea seguro y est√© siempre disponible.

**M√©tricas y Rendimiento**
Utilizaremos distintas herramientas para recopilar m√©tricas. Estas m√©tricas se implementar√°n en puntos clave dentro de los microservicios, con el fin de tener una visi√≥n del comportamiento del sistema.

**M√©tricas de Negocio:**
Las m√©tricas de negocio nos dar√°n una visi√≥n de c√≥mo el marketplace est√° funcionando desde una perspectiva de usuario y valor. Por ejemplo, es crucial saber cu√°ntas veces los usuarios buscan datasets o si los pagos se est√°n procesando correctamente.

-	**N√∫mero de b√∫squedas realizadas:** Esta m√©trica es fundamental para entender la actividad del cat√°logo. Esta ser√° recopilada dentro del catalog-search-engine-service, ya que este microservicio gestiona las b√∫squedas avanzadas en Elasticsearch. Cada vez que el endpoint /api/v1/catalog/search es invocado a trav√©s del API Gateway, el catalog-search-engine-service incrementar√° un contador que reflejar√° la cantidad de b√∫squedas.

-	**Transacciones de pago iniciadas y completadas/fallidas:** El seguimiento de las financias es crucial para el componente de marketplace. Esta m√©trica se rastrear√° directamente en el payment-processor-service. Este servicio es el encargado de manejar el procesamiento de pagos √∫nicos y emitir√° eventos como payment.initiated, payment.completed y payment.failed, que ser√°n contabilizados para obtener esta m√©trica.

-	**Volumen de datos consultados:** Permite entender el consumo real de los datasets. Esta m√©trica se capturar√° en el usage-tracking-service. Este consume eventos dataset.accessed generados por el Datalake o La B√≥veda cada vez que un usuario accede a un dataset.



**M√©tricas de Infraestructura:**
Las m√©tricas de infraestructura nos ayudad a verificar nuestra plataforma, asegurando que los recursos est√©n disponibles y funcionando de manera eficiente.
-	**Latencia de consultas a bases de datos:** Esta m√©trica se medir√° en cada microservicio que interact√∫a con una base de datos. Por ejemplo, el user-profile-manager-service (que usa PostgreSQL) y el catalog-metadata-sync-service (que usa PostgreSQL). Estos servicios expondr√°n un contador o histograma de latencia para las operaciones de base de datos que realizan, como lectura y escritura.

-	**Tama√±o de las colas y lag de consumidores:** Estas m√©tricas son importantes en el monitoreo del sistema de mensajer√≠a as√≠ncronos. Se obtendr√°n directamente de los brokers de mensajes y los consumidores. Por ejemplo, el notification-dispatcher-service que consume eventos de RabbitMQ. Los exporters de Prometheus para RabbitMQ se encargar√°n de recolectar esta informaci√≥n de las colas y los grupos de consumidores.

**Herramientas de Monitoreo**
Estas m√©tricas se utilizar√°n en las siguientes herramientas:
-	**Prometheus:** Recopilar√° m√©tricas directamente desde los endpoints /metrics expuestos por cada microservicio. Los exporters de Prometheus para bases de datos (PostgreSQL), Redis y RabbitMQ se usar√°n para m√©tricas de infraestructura.

-	**AWS CloudWatch:** Para m√©tricas a nivel de infraestructura de AWS (EKS, RDS, S3, KMS) y para m√©tricas de logs.

-	**Grafana:** Ser√° la plataforma de visualizaci√≥n principal, integrando datos de Prometheus y CloudWatch para crear dashboards interactivos y personalizados.


**Logs y Trazabilidad**
Un sistema centralizado de logs y trazabilidad es crucial para diagnosticar problemas en un entorno de microservicios.

-	Centralizaci√≥n de Logs: 
    - 	Todos los microservicios configurar√°n sus aplicaciones para emitir logs estructurados (JSON) a stdout.
    -	Tambi√©n se pueden enviar logs a CloudWatch Logs para integrarse con otras herramientas de AWS y facilitar la consulta con CloudWatch Logs Insights.

-	**Trazabilidad Distribuida:**
    -	Todos los microservicios (ej., marketplace-catalog-service, marketplace-user-service, marketplace-payment-service, marketplace-access-service, marketplace-recommendation-service, marketplace-notification-service, marketplace-analytics-service, y sus microservicios internos) ser√°n instrumentados con OpenTelemetry para generar trazas.
    -	Un OpenTelemetry Collector se desplegar√° en el cluster para recolectar las trazas y exportarlas a un backend como Jaeger (para visualizaci√≥n y an√°lisis de trazas).
    -	Esto permitir√° seguir una solicitud a trav√©s de m√∫ltiples microservicios (incluyendo llamadas gRPC y HTTP entre ellos) y ver la latencia de cada salto.

-	**Auditor√≠a y Diagn√≥stico:** 
    -	**Elasticsearch:** Proporcionar√° una interfaz potente para buscar, filtrar y analizar logs estructurados de todos los microservicios, permitiendo una r√°pida identificaci√≥n de la causa ra√≠z de problemas.
    -	AWS CloudTrail: Registra todas las llamadas a la API de AWS realizadas por los roles IAM de los microservicios del marketplace, crucial para auditor√≠a de seguridad y cumplimiento.

**Sistema de Alertas y Notificaciones**

**Monitoreo de Cumplimiento y Seguridad**
Dado el manejo de datos sensibles y transacciones financieras, el monitoreo de seguridad es una prioridad.

-	**Auditor√≠a de Accesos:**
    -	**CloudTrail:** Monitorizar√° todas las llamadas a la API de AWS relacionadas con los recursos utilizados por los microservicios del marketplace (ej., acceso a S3 buckets con datos de logs/analytics, KMS, RDS, EKS).
    -	El audit-logger-service registrar√° cada acceso a los datasets y cada acci√≥n relevante (ej., pagos completados) que ocurran a trav√©s de los microservicios de acceso y pago. Estos logs ser√°n inmutables y almacenados en Elasticsearch para auditor√≠as.
    -	**Alertas de Acceso Inusual:** Se configurar√°n alertas sobre patrones de acceso an√≥malos a datos sensibles o intentos de acceso no autorizado (401/403 respuestas del API Gateway que enruta a los microservicios del marketplace).

-	**Monitoreo de Cifrado:**
    -	Se verificar√° que los datos en reposo en el datalake y en bases de datos est√©n cifrados en KMS. Esto implica monitorear las interacciones de los microservicios que escriben o leen datos sensibles (ej., access-provisioning-service, payment-processor-service, catalog-metadata-sync-service).
    -	Se monitorear√° la tasa de errores de las operaciones de cifrado/descifrado en KMS.
    -	Se asegurar√° que los datos en tr√°nsito est√©n cifrados (TLS/SSL) entre todos los microservicios del marketplace y con el API Gateway.

-	**Monitoreo de Identidad y Acceso:**
    -	Se auditar√°n los logs de autenticaci√≥n del Bioregistro para detectar patrones de ataque de credenciales.
    -	Se monitorear√° el uso de tokens JWT y la gesti√≥n de estos por el token-management-service dentro del marketplace-access-service.
    -	Se activar√°n alertas sobre cambios en pol√≠ticas de IAM o roles asociados a los microservicios del marketplace.

**Health Checks y Disponibilidad**
Cada microservicio implementar√° liveness y readiness probes de EKS, adem√°s de deep health checks.

-	**Liveness Probe:** (ej., HTTP GET a /health) Verifica que el proceso de cada microservicio est√° corriendo y no est√° en un estado de deadlock. Si falla, EKS reiniciar√° el pod.
-	**Readiness Probe:** (ej., HTTP GET a /ready) Verifica que cada microservicio est√° listo para recibir tr√°fico, incluyendo la conectividad con sus dependencias cr√≠ticas (DB, cache, message brokers, APIs externas). Si falla, EKS no enrutar√° tr√°fico al pod hasta que est√© listo.
-	**Deep Health Checks:** Endpoints m√°s exhaustivos que simulan flujos de negocio cr√≠ticos (ej., una simulaci√≥n de compra que involucra marketplace-user-service, marketplace-payment-service, marketplace-access-service; una b√∫squeda de cat√°logo que involucra marketplace-catalog-service) para validar la funcionalidad end-to-end y la conectividad a todas las dependencias.

**An√°lisis y Mejora Continua**
El sistema de monitoreo no solo detectar√° problemas, sino que tambi√©n proporcionar√° inteligencia para la optimizaci√≥n continua.

-	**An√°lisis de Tendencias:** Identificaci√≥n de patrones en el tr√°fico del marketplace, volumen de transacciones, comportamiento del usuario y rendimiento de los datasets para optimizar la asignaci√≥n de recursos y planificar la capacidad, utilizando datos de todos los microservicios del marketplace recolectados por marketplace-analytics-service.
-	**Detecci√≥n de Anomal√≠as:** Uso de capacidades de ML en Grafana o CloudWatch para detectar comportamientos inusuales en las m√©tricas (ej., ca√≠da repentina en b√∫squedas, aumento inusual de pagos fallidos) que pueden indicar problemas subyacentes en cualquier microservicio.
-	**Reportes de Capacidad:** Proyecciones de crecimiento basadas en el historial de uso de recursos para planificar el escalado de EKS clusters, bases de datos y sistemas de mensajer√≠a para todos los microservicios del marketplace.
-	**Optimizaci√≥n de Costos:** An√°lisis del uso de recursos de AWS (EKS, RDS, S3, etc.) por cada microservicio para identificar oportunidades de reducci√≥n de costos.
-	**An√°lisis de Embudos de Conversi√≥n:** Usar los datos de marketplace-analytics-service (generados a partir de eventos de user-behavior-tracker-service y payment-processor-service) para identificar d√≥nde los usuarios abandonan el flujo de compra o b√∫squeda, permitiendo mejoras en la UX del portal.
-	**Evaluaci√≥n de Modelos de ML:** Monitorear el rendimiento de los modelos de recomendaci√≥n (behavioral-ml-service, content-similarity-service, recommendation-engine-service) y detecci√≥n de fraude (fraud-detection-service) y la efectividad de las recomendaciones servidas.

### Modelo de Seguridad Detallado - Marketplace

El m√≥dulo de Marketplace maneja transacciones financieras, datos de comportamiento de usuarios y acceso a datasets premium. Su backend implementa un modelo de seguridad robusto que previene accesos no autorizados, garantiza integridad financiera y mantiene confidencialidad de transacciones.

#### 1. Control de Acceso Granular

##### RBAC para Marketplace

El sistema valida roles espec√≠ficos en tiempo real durante cada interacci√≥n mediante middleware FastAPI que intercepta requests antes de llegar a endpoints de negocio. Los roles se almacenan cifrados en DynamoDB y se cachean en Redis durante sesiones activas.

| Rol | Descripci√≥n | Permisos |
|-----|-------------|----------|
| `marketplace:viewer` | Acceso de solo lectura | B√∫squeda, visualizaci√≥n de precios, recomendaciones |
| `marketplace:buyer` | Usuario autorizado para compras | Todo lo anterior + pagos, suscripciones, datasets comprados |
| `marketplace:seller` | Representante de colectivo vendedor | Configurar precios, gestionar ventas, analytics |
| `marketplace:admin` | Administrador completo | Acceso total: pagos, reportes financieros, gesti√≥n de fraude |

##### Flujo de Autorizaci√≥n de Transacciones

La autorizaci√≥n se ejecuta s√≠ncronamente al iniciar pagos, evaluando eligibilidad de acceso, l√≠mites de gasto y scoring de fraude antes del procesamiento con providers externos. El sistema valida:

- **Elegibilidad del dataset:** Verificaci√≥n de que el usuario puede acceder al dataset solicitado
- **L√≠mites de gasto:** Validaci√≥n contra l√≠mites configurados por usuario y tipo de cuenta
- **Scoring de fraude:** Evaluaci√≥n en tiempo real usando modelos ML desplegados en SageMaker
- **Logging de seguridad:** Registro as√≠ncrono de eventos sospechosos sin impactar rendimiento

##### Pol√≠ticas IAM para Recursos AWS

Las pol√≠ticas se configuran con principio de menor privilegio, otorgando a cada microservicio √∫nicamente permisos espec√≠ficos necesarios. Las condiciones IAM se eval√∫an din√°micamente durante operaciones de base de datos, restringiendo acceso a registros del usuario autenticado mediante `LeadingKeys` correspondientes al identificador √∫nico obtenido desde tokens Cognito.

### 2. Cifrado de Datos

#### Cifrado en Tr√°nsito
TLS 1.3 obligatorio para todas las comunicaciones, con certificate pinning para payment providers validado antes de establecer conexiones SSL:

- **Frontend ‚Üî API Gateway:** HTTPS con certificados AWS Certificate Manager auto-renovables
- **Payment providers:** Certificate pinning que bloquea certificados fraudulentos autom√°ticamente
- **Microservicios internos:** mTLS en EKS service mesh autenticando ambos extremos

#### Cifrado en Reposo
Estrategia diferenciada por sensibilidad de datos con claves KMS espec√≠ficas y rotaci√≥n autom√°tica:

| Tipo de Dato | Ubicaci√≥n | Cifrado | Clave KMS | Rotaci√≥n |
|--------------|-----------|---------|-----------|----------|
| Transacciones | DynamoDB | Nativo KMS | `dpv-marketplace-payments` | 12 meses |
| Suscripciones | PostgreSQL | TDE | `dpv-marketplace-subscriptions` | 12 meses |
| Facturas | S3 | SSE-KMS | `dpv-marketplace-documents` | 12 meses |
| Cache | Redis | Aplicaci√≥n | `dpv-marketplace-cache` | 30 d√≠as |

#### Cifrado de Campo PII
Para datos personalmente identificables, se implementa cifrado a nivel de campo que se ejecuta antes del almacenamiento utilizando contexto espec√≠fico de transacci√≥n. La clave de cifrado se genera din√°micamente para cada operaci√≥n utilizando AWS KMS con contexto de encriptaci√≥n espec√≠fico, nunca almacen√°ndose en texto plano.

### 3. Auditor√≠a y Logging

#### Eventos Auditados
Sistema de captura en tiempo real con doble escritura en OpenSearch (b√∫squedas inmediatas) y DynamoDB (almacenamiento largo plazo):

- **Transacciones:** Pagos iniciados/completados/fallidos, reembolsos
- **Acceso:** Datasets comprados/acceso otorgado/revocado
- **Seguridad:** Fraude detectado, comportamiento an√≥malo, l√≠mites excedidos
- **Administrativos:** Suscripciones creadas/canceladas, precios actualizados

#### Implementaci√≥n de Auditor√≠a
Middleware autom√°tico captura eventos mediante decoradores aplicados a funciones cr√≠ticas, registrando:

- **Par√°metros de entrada:** Sanitizados para eliminar informaci√≥n sensible
- **Resultado de operaci√≥n:** Estado final y datos relevantes
- **Tiempo de ejecuci√≥n:** Para an√°lisis de rendimiento y detecci√≥n de anomal√≠as
- **Errores:** Captura completa de excepciones para diagn√≥stico
- **Contexto de sesi√≥n:** IP, user agent, trace ID para trazabilidad completa

Los eventos de alto riesgo disparan alertas inmediatas al equipo de seguridad mediante SNS y SES.

#### Retenci√≥n Automatizada
Pol√≠ticas ejecutadas durante ventanas nocturnas moviendo datos hist√≥ricos a S3 Glacier seg√∫n regulaciones:

- **Pagos:** 7 a√±os (regulaciones financieras)
- **Comportamiento:** 2 a√±os (protecci√≥n datos personales)
- **Fraude:** 5 a√±os (investigaciones seguridad)
- **Accesos:** 3 a√±os (auditor√≠as)

### 4. Protecci√≥n contra Fraude

#### Detecci√≥n en Tiempo Real
Motor h√≠brido ML + reglas de negocio ejecut√°ndose en <200ms durante cada transacci√≥n:

**Modelo de Machine Learning:**
- Entrenamiento semanal con datos hist√≥ricos etiquetados
- Deployment en SageMaker endpoints para inference en tiempo real
- Features incluyen: patrones de gasto, velocidad transaccional, geolocalizaci√≥n, comportamiento hist√≥rico

**Reglas de Negocio:**
- **Velocidad transaccional:** >5 transacciones en 1 hora aumenta score en 0.3
- **Montos inusuales:** Transacciones >10x promedio del usuario aumenta score en 0.4
- **Anomal√≠as de comportamiento:** Cambios s√∫bitos en patrones de compra
- **Geolocalizaci√≥n:** Detecci√≥n de ubicaciones inusuales basada en historial

**Scoring Combinado:** 70% ML + 30% reglas de negocio, con umbral configurable para bloqueo autom√°tico.

#### Rate Limiting
Protecci√≥n granular por usuario e IP utilizando ventanas deslizantes Redis:

| Operaci√≥n | L√≠mite Usuario | L√≠mite IP | Ventana |
|-----------|----------------|-----------|---------|
| B√∫squeda datasets | 100/minuto | 300/minuto | 60 segundos |
| Iniciar pago | 5/minuto | 15/minuto | 60 segundos |
| Ver dataset | 50/minuto | 150/minuto | 60 segundos |
| Generar recomendaci√≥n | 20/minuto | 60/minuto | 60 segundos |

El sistema aplica el l√≠mite m√°s restrictivo entre usuario e IP, registrando violaciones para an√°lisis de patrones de ataque.

### 5. Gesti√≥n de Secretos

#### Credenciales de Payment Providers
AWS Secrets Manager con validaci√≥n de integridad autom√°tica detectando compromisos:

**Funcionalidades:**
- **Validaci√≥n de integridad:** Verificaci√≥n de checksums y patrones esperados antes de cada uso
- **Auditor√≠a de acceso:** Registro de qu√© servicio accedi√≥ a qu√© credencial en qu√© momento
- **Detecci√≥n de compromisos:** Alertas autom√°ticas ante modificaciones no autorizadas
- **Acceso controlado:** Solo microservicios autorizados pueden acceder a credenciales espec√≠ficas

#### Rotaci√≥n Autom√°tica
Calendario Terraform con funciones Lambda especializadas coordinando con providers durante ventanas de mantenimiento:

- **Stripe:** Rotaci√≥n cada 30 d√≠as con coordinaci√≥n autom√°tica para generar nuevas claves antes de invalidar anteriores
- **BAC Credomatic:** Rotaci√≥n cada 60 d√≠as con notificaci√≥n previa al provider
- **Claves internas:** Rotaci√≥n cada 90 d√≠as para claves de sesi√≥n y cache

### 6. Monitoreo de Seguridad

### Detecci√≥n de Anomal√≠as
Queries predefinidas ejecut√°ndose contra OpenSearch con frecuencias diferenciadas:

**Alta Criticidad (cada 30 segundos):**
- M√∫ltiples pagos fallidos (>10 en 5 minutos)
- Intentos de fraude detectados (score >0.8)
- Violaciones masivas de rate limiting (>50 en 1 minuto)

**Criticidad Media (cada 5 minutos):**
- Patrones de gasto inusuales (>$1000 en transacciones individuales)
- Accesos an√≥malos a datasets premium
- Cambios en configuraciones de precios

Los umbrales se ajustan autom√°ticamente bas√°ndose en patrones hist√≥ricos para reducir falsos positivos.

### Alertas Cr√≠ticas
Escalaci√≥n autom√°tica con notificaciones inmediatas:

| Alerta | Umbral | Duraci√≥n | Acci√≥n |
|--------|--------|----------|--------|
| Pico de fraude | >10 scores altos | 5 minutos | Notificar equipo seguridad |
| Sistema de pagos ca√≠do | >50 fallos | 2 minutos | Llamar ingeniero de guardia |
| Acceso masivo no autorizado | >100 intentos | 1 minuto | Activar contenci√≥n autom√°tica |

### 7. Compliance PCI DSS

#### Validaci√≥n Autom√°tica
Verificaci√≥n antes de procesar operaciones de pago, bloqueando autom√°ticamente operaciones no conformes:

**Controles Verificados:**
- **Cifrado de datos:** Validaci√≥n de que campos sensibles est√©n cifrados
- **Control de acceso:** Verificaci√≥n de permisos para la operaci√≥n solicitada
- **Seguridad de red:** Confirmaci√≥n de conexiones TLS v√°lidas
- **Monitoreo:** Validaci√≥n de que logs de auditor√≠a se est√©n generando correctamente

#### Reportes Automatizados
Generaci√≥n mensual con firmas digitales y distribuci√≥n autom√°tica a stakeholders:

**Contenido de Reportes:**
- Volumen total de transacciones procesadas
- Intentos de fraude bloqueados exitosamente
- Score de cumplimiento PCI DSS
- Incidentes de seguridad y su resoluci√≥n
- M√©tricas de disponibilidad del sistema de pagos

Los reportes se firman digitalmente para garantizar integridad y se cifran antes del almacenamiento y distribuci√≥n.

### 8. Respuesta a Incidentes

#### Clasificaci√≥n y Escalaci√≥n Autom√°tica
Algoritmos ML determinando severidad y disparando respuestas seg√∫n tipo de incidente:

**Niveles de Severidad:**
- **CR√çTICO:** Brecha de datos, fraude masivo, sistema de pagos comprometido
- **ALTO:** M√∫ltiples intentos de fraude, acceso no autorizado detectado
- **MEDIO:** Anomal√≠as de comportamiento, violaciones de l√≠mites
- **BAJO:** Eventos informativos, mantenimiento programado

#### Contenci√≥n Automatizada
Medidas autom√°ticas reversibles activ√°ndose seg√∫n tipo de amenaza:

**Fraude de Pagos:**
- Suspensi√≥n temporal de procesamiento para usuarios afectados
- Incremento autom√°tico de umbrales de detecci√≥n de fraude
- Notificaci√≥n inmediata a procesadores de pago

**Ataques Brute Force:**
- Rate limiting agresivo para IPs sospechosas
- Bloqueo temporal autom√°tico de direcciones IP atacantes
- Escalaci√≥n a sistemas de protecci√≥n DDoS

**Anomal√≠as de Acceso:**
- Re-autenticaci√≥n forzada para usuarios afectados
- Invalidaci√≥n de tokens de sesi√≥n sospechosos
- Auditor√≠a intensiva de accesos recientes

### 9. Testing de Seguridad

#### Pruebas Automatizadas
Suite de pruebas ejecut√°ndose en pipeline CI/CD verificando controles de seguridad antes de despliegues:

- **Tests de cifrado:** Validaci√≥n de que datos sensibles est√©n protegidos
- **Tests de autenticaci√≥n:** Verificaci√≥n de controles de acceso
- **Tests de detecci√≥n de fraude:** Simulaci√≥n de transacciones fraudulentas conocidas
- **Tests de rate limiting:** Validaci√≥n de l√≠mites configurados
- **Tests de compliance PCI:** Verificaci√≥n de cumplimiento de controles

#### Penetration Testing
Pruebas semanales automatizadas simulando vectores de ataque reales:

- **Inyecci√≥n SQL:** Resistencia a ataques de base de datos
- **Bypass de autenticaci√≥n:** Intentos de evasi√≥n de controles de acceso
- **Manipulaci√≥n de pagos:** Pruebas de integridad en transacciones
- **Escalaci√≥n de privilegios:** Validaci√≥n de controles RBAC
- **Exposici√≥n de datos:** Verificaci√≥n de que informaci√≥n sensible no sea accesible

## Elementos de Alta Disponibilidad 

###### 1. Replicaci√≥n de Base de Datos

#### PostgreSQL Multi-AZ en marketplace-payment-service y marketplace-catalog-service
- **Ubicaci√≥n**: Instancia principal en us-east-1a, r√©plica en us-east-1b
- **Aplicaci√≥n**: Replicaci√≥n s√≠ncrona de transacciones cr√≠ticas (`MarketplaceOrder`, `PaymentTransaction`, `Subscription`)
- **Activaci√≥n**: Failover autom√°tico en <30 segundos durante fallas del payment-processor-service

#### DynamoDB en marketplace-user-service y marketplace-analytics-service
- **Ubicaci√≥n**: Replicaci√≥n autom√°tica entre 3 AZs
- **Aplicaci√≥n**: `UserBehaviorMarketplace`, `MarketplaceSessionData`, `DatasetRecommendationCache` gestionadas por user-behavior-tracker-service
- **Activaci√≥n**: Sincronizaci√≥n en milisegundos, Point-in-Time Recovery (35 d√≠as) en event-ingestion-service

#### 2. Balanceador de Carga

#### Application Load Balancer delante del cluster EKS
- **Ubicaci√≥n**: Entrada al cluster EKS del marketplace
- **Aplicaci√≥n**: 
  - Weighted round-robin distribuye tr√°fico entre pods de microservicios
  - Least connections durante picos en marketplace-catalog-service (b√∫squedas matutinas)
  - Sticky sessions para marketplace-payment-service durante checkout
- **Activaci√≥n**: Health checks cada 10 segundos en endpoints `/health` de cada microservicio

#### 3. Auto-Scaling

#### EKS Horizontal Pod Autoscaler aplicado a todos los microservicios
- **Ubicaci√≥n**: Microservicios desplegados en cluster EKS (nodos t3.large: 2 vCPU, 8 GB RAM)
- **Aplicaci√≥n**: 
  - Monitoreo en marketplace-catalog-service, marketplace-payment-service, marketplace-access-service
  - Rango: 3-15 nodos que se expanden autom√°ticamente
  - M√©tricas: CPU >70%, memoria >80% por 5 minutos
- **Activaci√≥n**: 
  - Scale-up durante picos de b√∫squedas en catalog-search-engine-service
  - Capacity reservada para marketplace-payment-service durante finales de mes

#### 4. Almacenamiento Resiliente

#### Amazon S3 utilizado por marketplace-analytics-service
- **Ubicaci√≥n**: Buckets `dpv-marketplace-assets` y `dpv-marketplace-invoices` replicados cross-region a us-west-1
- **Aplicaci√≥n**:
  - Thumbnails y previews gestionados por catalog-metadata-sync-service
  - Facturas PDF generadas por invoice-generator-service
- **Activaci√≥n**: Versionado autom√°tico, lifecycle policies a Glacier despu√©s de 90 d√≠as

#### Backups de microservicios cr√≠ticos
- **marketplace-payment-service**: cada 6 horas (horarios laborales)
- **marketplace-access-service**: cada 24 horas (fines de semana)

#### 5. Motor de B√∫squeda

#### OpenSearch Multi-Nodo para catalog-search-engine-service
- **Ubicaci√≥n**: 2 nodos t3.small.search distribuidos entre AZs con 50GB EBS por nodo
- **Aplicaci√≥n**: 
  - √çndices `datasets-marketplace-catalog` y `user-marketplace-searches`
  - Dual-write pattern con √≠ndice shadow
- **Activaci√≥n**: Circuit breaker en marketplace-catalog-service desv√≠a a Redis cuando latencia >500ms

#### 6. Cache Distribuido

#### Redis Cluster compartido entre microservicios
- **Ubicaci√≥n**: Amazon ElastiCache para Redis en modo cluster distribuido entre AZs  
- **Aplicaci√≥n**:
  - **marketplace-recommendation-service**: cache de recomendaciones personalizadas (TTL: 4h)
  - **marketplace-user-service**: datos de sesi√≥n en user-session-manager-service (TTL: 8h)
  - **marketplace-catalog-service**: resultados de b√∫squedas frecuentes (TTL: 5 min)
- **Activaci√≥n**: Failover autom√°tico en 30 segundos, consistent hashing para redistribuci√≥n

#### 7. Monitoreo y Auto-Remediaci√≥n

#### CloudWatch Alarms monitoreando endpoints de microservicios
- **Ubicaci√≥n**: Endpoints cr√≠ticos monitoreados cada 30 segundos
- **Aplicaci√≥n**:
  - `/api/v1/catalog/search` (marketplace-catalog-service)
  - `/api/v1/payments/initiate` (marketplace-payment-service)  
  - `/api/v1/access/my-datasets` (marketplace-access-service)
- **Activaci√≥n**:
  - Error rate >5%: escalado inmediato
  - Latencia >1s: cache warming
  - Auto-restart de pods en fraud-detection-service

#### 8. Recuperaci√≥n de Desastres

#### Estrategia aplicada por criticidad de microservicio

| Microservicio | RTO | RPO | Aplicaci√≥n |
|---------------|-----|-----|------------|
| **marketplace-payment-service** | 15 min | 5 min | payment-processor-service con replicaci√≥n s√≠ncrona cross-region |
| **marketplace-user-service** | 2 horas | 4 horas | user-behavior-tracker-service con backup as√≠ncrono |
| **marketplace-analytics-service** | 24 horas | 24 horas | business-metrics-calculator-service con backup diario |

#### 9. Conectividad Redundante

##### Network Architecture en cluster EKS
- **Ubicaci√≥n**: Pods distribuidos en 3 AZs con route tables independientes
- **Aplicaci√≥n**:
  - marketplace-payment-service con m√∫ltiples rutas a Stripe API
  - API Gateway balanceando entre instancias de marketplace-catalog-service
  - M√∫ltiples NAT Gateways distribuidos geogr√°ficamente
- **Activaci√≥n**: Certificate rotation autom√°tica en webhook-handler-service, DNS-based load balancing

#### 10. Kubernetes Self-Healing

##### EKS Configuration aplicada a todos los microservicios del marketplace
- **Ubicaci√≥n**: Cluster EKS con distribuci√≥n anti-affinity entre nodos y zones
- **Aplicaci√≥n**:
  - Resource quotas garantizadas en catalog-search-engine-service y payment-processor-service
  - PodDisruptionBudgets: m√≠nimo 60% pods operacionales durante updates
  - Anti-affinity rules previenen concentraci√≥n de marketplace-payment-service en un solo nodo
- **Activaci√≥n**:
  - **Liveness probes**: detectan pods hung en fraud-detection-service
  - **Readiness probes**: validan conectividad a Stripe en subscription-billing-service  
  - **Startup probes**: optimizan carga de modelos ML en behavioral-ml-service
  

### Diagrama del Backend 

A continuaci√≥n, se presenta el diagrama del backend del Marketplace de Datos de Data Pura Vida. En √©l se evidencia c√≥mo todo el ecosistema de AWS interact√∫a con los distintos microservicios desplegados en el cl√∫ster de Kubernetes provisto por EKS. Se muestra la contenerizaci√≥n de cada microservicio utilizando Docker y c√≥mo el monitoreo interno es gestionado por Prometheus. Tambi√©n se destacan las interacciones con sistemas de terceros como SumSub y Stripe.

![image](img/DiagramaBackendMarketplace.svg)


## Dise√±o de los datos

### Topolog√≠a de Datos

- **Tipo:** OLTP + OLAP + NoSQL + Motor de b√∫squeda

- Para el componente Marketplace se va a utilizar un arquitectura h√≠brida para la separaci√≥n de responsabilidades entre transacciones, anal√≠tica y b√∫squeda. Las operaciones de compra, gestiones de permisos y accesos se maneja con una base de datos `OLTP` en RDS con PostgreSQl. Las consultas de usuario y logs se maneja en `OLAP` para realizar an√°lisis. Para explorar el cat√°logo de datasets se usa un motor de b√∫squeda especializado. 

- Para `OLTP`se usa la misma instancia de RDS que se utiliza en el componente Bioregistro, extendida con nuevas tablas para:
  - Transacciones de compra de acceso.
  - Historial de accesos por usuario.
  - Registro de renovaciones, paquetes y m√©todos de pago.
  - Vinculaci√≥n entre usuarios, organizaciones y datasets adquiridos.

- Para `OLAP`, se usa Amazon Redshift en Serverless, configurado con escalado  autom√°tico. Redshift se alimenta por cargas en batch diarias desde Amazon S3 y OpenSearch incluyendo.
  - logs de acceso
  - consultas de usuarios
  - de navegaci√≥n. 
  - Redshift tambi√©n consulta directamente algunas tablas de PostgreSQL mediante Federated Queries.

- Como secci√≥n `NoSQL`, Amazon DynamoDB se usa como backend para estado temporal y comportamiento de usuarios:

  - **SessionData:** sesiones activas por usuario.
  - **UserBehavior:** m√©tricas de navegaci√≥n en vivo.
  - **RecommendationCache:** resultados de sistemas de recomendaci√≥n.
  - **NotificationQueue:** notificaciones pendientes y estado de lectura.

Estas tablas incluyen pol√≠ticas de TTL y activan Streams que alimentan pipelines de entrenamiento en SageMaker o acciones via Lambda.

- Para `tareas asincronicas` se utiliza AWS Lambda para tareas como:
  - Procesamiento de pagos y validaci√≥n antifraude.
  - Generaci√≥n de facturas PDF y almacenamiento en S3.
  - Activaci√≥n de renovaciones autom√°ticas o cancelaciones.
  - Limpieza de sesiones y sincronizaci√≥n de estados en DynamoDB.

- La arquitectura `Event-Driven` se aplica enAmazon EventBridge:
  - `payment.completed`, `dataset.viewed`, `session.expired`, etc.
  - Estos eventos disparan Lambdas, env√≠an notificaciones v√≠a SNS/SES o actualizan los √≠ndices en OpenSearch.

- Para `mensajer√≠a interna` se utiliza RabbitMQ, en donde se coordinan los microservicios desplegados en EKS:
  - Control de flujo de compra.
  - Validaci√≥n cruzada de permisos.
  - Disparadores para entrenamientos en SageMaker.

- Como `motor de busqueda` se usa OpenSearch que es el motor principal para la exploraci√≥n de datasets:

  - Indexaci√≥n de metadatos enriquecidos.
  - B√∫squeda facetada por categor√≠a, colectivo, a√±o, palabras clave.
  - Exploraci√≥n sem√°ntica usando embeddings y puntuaci√≥n por relevancia.
  - Tambi√©n almacena logs de b√∫squeda (`user-searches`) y m√©tricas de uso (`marketplace-analytics`).



- **Tecnolog√≠a Cloud**:

  - Amazon RDS (PostgreSQL)
  - Amazon Redshift Serverless
  - Amazon DynamoDB
  - Amazon OpenSearch
  - Amazon S3
  - AWS Lambda
  - AWS EventBridge
  - AWS SNS, SES
  - RabbitMQ (en EKS)

- **Pol√≠tcias y Reglas**:

- **Single-region:** Toda la infraestructura estar√° localizada en `us-east-1`
- **Backups autom√°ticos:** 
  - RDS y Redshift con respaldo diario a la 1 a.m. en S3.
  - DynamoDB habilitado con backups autom√°ticos y TTL por tabla.
  - S3 tiene versionado y reglas de ciclo de vida para archivar logs.
- **Backups cruzados:** Replicaci√≥n semanal a us-west-1 (viernes, 3 a.m.) usando S3 IA.
- **Failover autom√°tico:**
  - RDS con Multi-AZ.
  - Redshift con snapshots autom√°ticos.
  - OpenSearch con replicaci√≥n de shards entre zonas de disponibilidad.
  - DynamoDB es multi-AZ por dise√±o y no requiere configuraci√≥n adicional.



- **Beneficios**:
  - Separaci√≥n clara entre operaciones transaccionales, anal√≠ticas, temporales y de b√∫squeda.
  - Uso de m√∫ltiples motores optimizados por tipo de dato: PostgreSQL (consistencia), Redshift (consulta masiva), DynamoDB (estado r√°pido), OpenSearch (b√∫squeda).
  - Arquitectura event-driven permite desacoplar procesos complejos como pagos, notificaciones, y ML.
  - OpenSearch puede integrarse con SageMaker para enriquecer b√∫squedas con modelos IA.
  - Redshift permite consultar tablas de RDS directamente:
```sql
CREATE EXTERNAL SCHEMA marketplace_schema
FROM POSTGRES
DATABASE 'admin_db'
URI 'dpv-rds-postgres.c8xyzxyz.us-east-1.rds.amazonaws.com'
PORT 5432
IAM_ROLE 'arn:aws:iam::123456789012:role/marketplace-query'
SECRET_ARN 'arn:aws:secretsmanager:us-east-1:123456789012:secret:MarketplaceRDSSecret'
```
- Redshift maneja archivos en formato Parquet desde cargas diarias de logs almacenados en S3:
```sql
COPY marketplace.analytics_logs
FROM 's3://dpv-marketplace-logs/diario/'
IAM_ROLE 'arn:aws:iam::123456789012:role/marketplace-etl'
FORMAT AS PARQUET;
```

### RLS

No se hace uso de RLS al igual que en la b√≥veda, por las mismas razones.

### Tenency, Seguridad y Privacidad

- **Modelo**: Single-Access-Point, RBAC, Multi-Tenant 

  - Todo acceso a datos se hace a trav√©s del Single Access Point. Solo las clases autorizadas como `MarketplaceRDSRepository`, `MarketplaceSearchRepository`, `MarketplaceAnalyticsRepository`, `MarketplaceDynamoRepository` y `MarketplaceEventBridgeHandler` est√°n habilitadas para interactuar con las fuentes de datos. Esto incluye RDS, Redshift, DynamoDB y OpenSearch. Toda consulta o acci√≥n desde APIs, Lambda o dashboards debe pasar por estas clases.

  - Se usar√° multi-tenant, ya que m√∫ltiples colectivos y organizaciones pueden publicar y consumir datasets dentro del Marketplace. El aislamiento se garantiza de dos formas:

    - **Aislamieno f√≠sico:** Cada dataset publicado por un colectivo se almacena en su propia tabla en Redshift o RDS. En DynamoDB, todos los √≠tems llevan un `tenant_id` obligatorio.

    - **Aislaiento l√≥gico:** El acceso a cada dataset se controla por medio de roles IAM asignados din√°micamente tras la compra del recurso, usando LakeFormation para enlazar los permisos a recursos etiquetados.

  - Para hacer el manejo de control de acceso y RBAC se hara lo siguiente:
    - **LakeFormation + IAM:**
      - **Rol IAM de Colectivo:** Cada colectivo tiene un rol IAM vinculado a sus datasets. Al publicar un nuevo dataset, se genera un tag LakeFormation `dataset=xyz`, el cual se asigna a la tabla correspondiente. Ese tag se asocia al rol IAM del colectivo.

      - **Rol IAM por Dataset Adquirido:** Cuando un usuario compra un dataset, se le asigna un rol IAM con permisos limitados (`SELECT`, `DESCRIBE`) sobre las tablas asociadas. Esto ocurre mediante backend y EventBridge.

      - **Rol P√∫blico por Defecto:** Datasets p√∫blicos son accesibles mediante el rol IAM asociado al tag `dataset=public-free`, asignado autom√°ticamente a usuarios autenticados.

    - **OpenSearch:**
      - El acceso a √≠ndices est√° filtrado por tenant_id y validado desde backend antes de enviar la consulta.
      - La b√∫squeda sem√°ntica tambi√©n aplica dataset_access para evitar exposici√≥n de recursos no adquiridos.

    - **DynamoDB:**
      - Cada √≠tem incluye tenant_id y user_id, lo que permite el uso de condiciones en IAM Policies para evitar lectura cruzada de tenants.

    - **Ejemplo de implementaci√≥n con LakeFormation**
    
      ```py
      import boto3
      client = boto3.client('lakeformation')
      # Creaci√≥n del tag de acceso a dataset:
      client.create_lf_tag(
          TagKey='dataset',
          TagValues=['marketplace_inclusion_2025']
      )
      ```

      ```py
      # Asignaci√≥n del tag a la tabla en Redshift:
      client.assign_lf_tags_to_resource(
      Resource={
          'Table': {
              'CatalogId': 'AWS_ACCOUNT_ID',
              'DatabaseName': 'marketplace',
              'Name': 'dataset_inclusion_table'
          }
      },
      LFTags=[
          {
              'TagKey': 'dataset',
              'TagValues': ['marketplace_inclusion_2025']
          }
        ]
      )
      ```

      ```py
      # Asignaci√≥n del tag a un rol IAM de usuario comprador:
      client.grant_permissions(
      Principal={
          'DataLakePrincipalIdentifier': 'arn:aws:iam::ACCOUNT_ID:role/Buyer_Dataset_123'
      },
      Resource={
          'LFTagPolicy': {
              'ResourceType': 'TABLE',
              'Expression': [
                  {
                      'TagKey': 'dataset',
                      'TagValues': ['marketplace_inclusion_2025']
                  }
              ]
          }
        },
        Permissions=['SELECT', 'DESCRIBE']
      )
      ```

- **Cloud**: 

  - AWS RDS para PostgreSQL, esquema por colectivo.
  - AWS Redshift Serverless, segmentado por tags.
  - AWS DynamoDB, por tabla con tenant_id y TTL.
  - AWS LakeFormation, control de acceso a tablas.
  - AWS IAM, para permisos a roles por dataset o colectivo.
  - AWS KMS, cifrado de datos sensibles.
  - Amazon OpenSearch Service, con acceso filtrado por tenant.
  - AWS Lambda y EventBridge, para eventos y automatizaci√≥n.
  - AWS SNS/SES, para notificaciones de seguridad y actividad.

- **Beneficios**:

  - Gracias a Single-Access-Point, los accesos a datos del Marketplace (compras, validaci√≥n de permisos, consultas de visualizaci√≥n) pasan por validadores como `TenantManager` y `MarketplaceRepository`. Esto minimiza el riesgo de acceso directo a las bases de datos sin control l√≥gico o sin trazabilidad.

  - Como cada colectivo tiene su propio esquema en PostgreSQL, y los datasets de pago se asocian a tablas individuales, se elimina el riesgo de filtraci√≥n de datos entre organizaciones. 

  - Se pueden diferenciar los datasets p√∫blicos, privados y pagos, y aplicar diferentes niveles de acceso y visibilidad sin necesidad de duplicar datos usando tags como `dataset=public-free`.

### Conexi√≥n a Base de datos

- **Modelo**: Transaccional v√≠a Statements / ORM / Funciones asincronicas

El componente Marketplace maneja su acceso a datos utilizando una arquitectura h√≠brida:

  - SQLAlchemy ser√° el ORM principal para la interacci√≥n con RDS y Amazon Redshift.
  - Para NoSQL como DynamoDB y OpenSearch, se usaran SDKs nativos en clases de repositorio independientes (`MarketplaceDynamoRepository`, `MarketplaceSearchRepository`).
  - Algunas operaciones asincr√≥nicas (actualizaciones post-compra o notificaciones) se manejan mediante AWS Lambda, que consulta directamente las fuentes de datos o lanza eventos de actualizaci√≥n.

- **Patrones de POO**:

Factory: Se aplica el patr√≥n Factory para crear instancias de conexi√≥n y repositorios espec√≠ficos para cada motor de base de datos:       

  - `MarketplaceRDSFactory`, `MarketplaceRDSRepository`
  - `MarketplaceRedshiftFactory`, `MarketplaceRedshiftRepository`   
  - `MarketplaceSearchFactory`, `MarketplaceSearchRepository`
  - `MarketplaceDynamoFactory`, `MarketplaceDynamoRepository`


- **Beneficios**:

  - SQLAlchemy permite trabajar con objetos Python sin renunciar a la flexibilidad del SQL cuando es necesario.
  - Se protege contra vulnerabilidades como SQL Injection.
  - Se puede garantizar el cumplimiento de las propiedades ACID.
  - Permite combinar declaraciones ORM con consultas SQL puras dentro del mismo flujo transaccional.
  - Las funciones Lambda pueden ser probadas y versionadas de forma independiente, ayudando a mantener un sistema robusto.


- **Pool de Conexiones:** Usaremos el pool integrado en SQLAlchemy (QueuePool), el cual es din√°mico. El tama√±o base del pool ser√° de 10 conexiones, y podr√° escalar hasta 15 conexiones simult√°neas. 

  - Tama√±o base del pool: 10 conexiones
  - Tama√±o m√°ximo: 15 conexiones
  - Tiempo de espera: 30 segundos
  - Tiempo de vida de conexi√≥n inactiva: 60 segundos
  - **Beneficios**:
    - La escalabilidad se ajusta bajo demanda.
    - Proporciona mayor estabilidad en ambientes productivos.
    - Para DynamoDB y OpenSearch no se usan pools persistentes, ya que los SDKs est√°n optimizados para conexiones breves y asincr√≥nicas (HTTP bajo demanda).
  

- **Drivers y SDKs:** 

  - **PostgreSQL / Redshift:**

    - Driver nativo `psycopg2` + SQLAlchemy
    - Soporte para queries directas y federadas desde Redshift hacia RDS

  - **DynamoDB:**
    - SDK oficial de AWS para Python (`boto3`)
    - Conexi√≥n segura bajo IAM, acceso controlado por pol√≠ticas y validaciones de `tenant_id`

  - **OpenSearch:**
    - Cliente oficial de AWS (`opensearch-py`)
    - Firma de solicitudes con AWS Signature v4
    - Todas las consultas pasan por `MarketplaceSearchRepository`, que incluye validadores de permisos y filtrado por tenant

  - **AWS Lambda:**
    - Las Lambdas usan el runtime `python3.11` y acceden mediante SDKs (`boto3`, `sqlalchemy`, `opensearch-py`)
    - Est√°n conectadas v√≠a EventBridge a eventos como:
      - payment.completed
      - dataset.access.revoked
      - search.query.malicious

### Dise√±o para IA

**Implementaciones comunes a todas las tablas**

Con el objetivo de habilitar al componente Marketplace para interoperar con agentes de IA, se implementan las siguientes medidas en los procesos de publicaci√≥n, consulta y an√°lisis de datasets:

  - Todas las tablas publicadas en Redshift incluir√°n las siguientes columnas adicionales generadas autom√°ticamente por el sistema de transformaci√≥n:
    - `CategoriaSemantica`: Asignada por clasificaci√≥n autom√°tica o proporcionada por el colectivo.
    - `DescripcionFila`: Texto breve generado autom√°ticamente por modelo ML para describir el contenido de cada fila con lenguaje natural.

- Los documentos indexados en OpenSearchincluir√°n:
  - Embeddings sem√°nticos del t√≠tulo, descripci√≥n y contenido estructurado, generados por SageMaker.

- Todas las b√∫squedas y visualizaciones realizadas por los usuarios en el frontend ser√°n:
  - Registradas en OpenSearch bajo el √≠ndice `marketplace-analytics`.
  - Enviadas a DynamoDB y procesadas v√≠a Streams para alimentar modelos de recomendaci√≥n en SageMaker.

- Se construye una base de consultas hist√≥ricas de usuarios en formato vectorial, almacenada en S3 y DynamoDB, utilizada para entrenar modelos de:
  - Recomendaci√≥n personalizada.
  - Generaci√≥n autom√°tica de res√∫menes.

- Los modelos de generaci√≥n de texto y recomendaci√≥n se entrenan y ejecutan mediante AWS SageMaker en procesos peri√≥dicos y orquestados por EventBridge + Lambda.


**Justificaci√≥n**

- Los usuarios podr√°n explorar el cat√°logo mediante lenguaje natural. Gracias a los embeddings generados y al uso de metadatos sem√°nticos, los agentes de IA podr√°n transformar preguntas o intenciones en consultas de b√∫squeda relevantes y explicables.

- Mediante el an√°lisis de comportamiento hist√≥rico (clics, compras, visualizaciones), el sistema puede generar recomendaciones autom√°ticas ajustadas al perfil del usuario, su historial y sus intereses recientes.

- Las descripciones autom√°ticas por fila y por dataset permiten a los agentes generar documentaci√≥n y contenido explicativo sin intervenci√≥n humana, incluso para datasets nuevos.

- Cuando un dataset se actualiza o cambia su estructura, los agentes de IA utilizan las columnas sem√°nticas y los hist√≥ricos de b√∫squeda para adaptar autom√°ticamente visualizaciones, reportes y modelos entrenados.

### Diagrama de Base de Datos

El componente Marketplace reutiliza varias tablas del diagrama de La B√≥veda, ya que ambos trabajan con usuarios, colectivos y datasets. Esto evita duplicar estructuras y mantiene consistencia entre m√≥dulos.

Las tablas que se usan directamente en el Marketplace son:

- **PersonaFisica:** para los usuarios que compran y acceden a datasets.
- **Dataset:** representa los datasets disponibles para consulta o compra.
- **Colectivo y TipoDeColectivo:** identifican qui√©n publica cada dataset.
- **AccesoDataset:** registra qu√© usuario tiene permiso de acceso a cada dataset.
- **DatasetDePago y TipoDePago:** definen si el acceso es por suscripci√≥n, cuota, etc.
- **Cuotas:** controla cu√°ntas consultas le quedan a un usuario.
- Las tablas Representantes y BankAccount no se usan directamente en el Marketplace.

![alt text](img/DiagramaBDBoveda.png)

# 4.6 Centro de Visualizaci√≥n y Consumo 

## Dise√±o del Frontend 

### Construcci√≥n Arquitect√≥nica

El Generador de Dashboards es el subcomponente principal encargado de permitir la creaci√≥n, visualizaci√≥n y personalizaci√≥n de gr√°ficos de an√°lisis sobre los datasets cargados y procesados previamente en el sistema.

Su arquitectura t√©cnica sigue las siguientes capas:

- **Frontend:** Construido en React.js con Vite, estilizado en Tailwind CSS, empleando Plotly.js como librer√≠a principal de gr√°ficos.
- **Backend:** Implementado sobre la API REST general del backend centralizada en FastAPI desplegada en EKS.
- **Persistencia de datos:** Los dashboards generados se almacenan en PostgreSQL bajo el dominio de usuarios, configuraciones y plantillas personalizadas.


### Flujo Completo de Funcionamiento

1. **Selecci√≥n y configuraci√≥n inicial:**
   - El usuario accede a la interfaz gr√°fica desde el portal web.
   - Selecciona los datasets disponibles a los que tiene acceso seg√∫n los permisos RBAC y RLS ya aplicados por la b√≥veda de datos.

2. **Definici√≥n del gr√°fico:**
   - El usuario selecciona el tipo de visualizaci√≥n: barras, l√≠neas, series temporales, pie chart o scatter plot.
   - La interfaz presenta un formulario din√°mico (construido con Formik + Yup) para que el usuario configure los ejes, medidas, filtros y par√°metros adicionales de cada gr√°fico.

3. **Interacci√≥n con IA (opcional):**
   - El usuario puede emplear prompts naturales que son procesados por el backend v√≠a LangChain y OpenAI/SageMaker para autogenerar gr√°ficos sugeridos.

4. **Procesamiento Backend:**
   - El backend valida los permisos del usuario, ejecuta la consulta al datalake y transforma los datos al formato requerido por Plotly.
   - El backend responde al frontend con el JSON espec√≠fico requerido por Plotly.js.

5. **Renderizaci√≥n de gr√°ficos:**
   - Plotly.js renderiza los gr√°ficos directamente en el navegador en base al dataset recibido.

6. **Persistencia:**
   - Los dashboards completos (estructura, consultas, configuraciones) se almacenan en PostgreSQL y DynamoDB para permitir recuperaci√≥n, edici√≥n y compartici√≥n futura.

7. **Control de consumo:**
   - Se aplica control de l√≠mites en tiempo real (volumen de datos consultados, frecuencia de uso, n√∫mero de dashboards activos).



### Principios de Dise√±o Aplicados

- **MVVM:**
  - `Model:` Las estructuras de dashboards, gr√°ficos y datasets.
  - `ViewModel:` Custom Hooks como `useDatasetSearch()` o `usePromptVisualization()` gestionan la l√≥gica de negocio desacoplada de la interfaz.
  - `View:` Componentes React bajo Atomic Design (atoms, molecules, organisms, templates).

- **Atomic Design:**
  - √Åtomos: Botones, inputs, selects.
  - Mol√©culas: Formularios de configuraci√≥n de gr√°ficos.
  - Organismos: Contenedores de dashboards.
  - Templates: Editor completo de dashboards.

- **SOLID:**
  - SRP: Cada Hook maneja una responsabilidad √∫nica.
  - OCP: Nuevos tipos de gr√°ficos pueden a√±adirse sin modificar c√≥digo existente.
  - LSP: Cada gr√°fico implementa la misma interfaz de renderizado.
  - ISP: Los hooks y APIs exponen solo los par√°metros estrictamente necesarios.
  - DIP: Backend completamente desacoplado de la l√≥gica frontend, interact√∫an mediante APIs REST y contratos JSON bien definidos.

- **Clean Code & DRY:**
  - Reutilizaci√≥n m√°xima de componentes.
  - Custom Hooks independientes y altamente testeables.
  - Estricta separaci√≥n de capas de presentaci√≥n, l√≥gica y acceso a datos.

- **Separation of Concerns:**
  - Clar√≠sima divisi√≥n entre vistas (React Components), l√≥gica de negocio (Hooks) y acceso a datos (API Connector).



### Herramientas y Librer√≠as utilizadas

| Capa       | Herramienta |
|------------|-------------|
| Frontend   | React.js, Vite, Tailwind CSS, Formik, Yup, React Router, Plotly.js |
| Backend    | FastAPI, LangChain, OpenAI/SageMaker, PostgreSQL, DynamoDB |
| Infraestructura | AWS S3, CloudFront, EKS, Cognito, Lambda@Edge, Redis, RabbitMQ |
| Seguridad  | OAuth2, JWT, MFA, RBAC, RLS, SecretsManager |
| DevOps     | GitHub Actions, Terraform, Prometheus, Grafana, CloudWatch |
| Testing    | Jest (Frontend), Pytest (Backend), Postman, Gatling |


### Consideraciones de Seguridad

- Todos los accesos a dashboards pasan por validaci√≥n OAuth2 + JWT emitidos por Cognito.
- El acceso a datasets sigue las reglas RBAC y RLS definidas en la b√≥veda.
- Los dashboards nunca exportan datos en crudo, s√≥lo visualizaci√≥n interna.
- Se aplica protecci√≥n contra abusos de consumo v√≠a throttling, rate-limiting y monitoreo con CloudWatch.


### Observabilidad Espec√≠fica

- Dashboards de monitoreo propios en Grafana:
  - Volumen de dashboards generados por usuario
  - Tiempo promedio de renderizaci√≥n
  - Fallos en consultas al datalake
  - Consumo acumulado de datasets por dashboard
  - Tasa de uso de IA para generaci√≥n autom√°tica


### Esquema Simplificado de Componentes Frontend

```plaintext
frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dashboardApiConnector.js
‚îÇ   ‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DashboardModel.js
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ atoms/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ molecules/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ organisms/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useDatasetSearch.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ usePromptVisualization.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useChartConfigurator.js
‚îÇ   ‚îî‚îÄ‚îÄ pages/
‚îÇ       ‚îî‚îÄ‚îÄ DashboardBuilderPage.jsx
```

## Dise√±o del Backend

### Servicios de AWS

**Amazon EKS**

La API REST principal del Centro de Visualizaci√≥n est√° desarrollada con FastAPI y desplegada en un cl√∫ster de EKS, el cual administra din√°micamente los microservicios responsables del manejo de usuarios, dashboards, procesamiento de solicitudes y exportaci√≥n de reportes. Este cl√∫ster permite escalar horizontalmente ante aumentos de tr√°fico, especialmente durante jornadas de alta consulta o actualizaci√≥n masiva de visualizaciones.

**AWS S3**

Todos los reportes generados por el sistema, incluyendo gr√°ficos exportados en formatos como PNG, PDF y JSON, son almacenados en buckets de S3 organizados por usuario. Tambi√©n se utiliza como almacenamiento de respaldos programados de dashboards activos y configuraciones personalizadas, los cuales se ejecutan mediante tareas automatizadas desde EKS o AWS Lambda.

**AWS EventBridge**

EventBridge act√∫a como canal de eventos para integrar al Centro de Visualizaci√≥n con otros m√≥dulos del sistema mediante un enfoque event-driven. Funciona en conjunto con RabbitMQ, que opera como cola interna de eventos r√°pidos, mientras que EventBridge facilita la interoperabilidad con servicios externos o componentes asincr√≥nicos como generaci√≥n diferida de visualizaciones o triggers de actualizaci√≥n de datos.

**AWS RDS**

La base de datos PostgreSQL desplegada en Amazon RDS contiene la informaci√≥n estructural del sistema de visualizaci√≥n. Aqu√≠ se almacenan dashboards personalizados por usuario, configuraciones de filtros, historiales de visualizaci√≥n y metadata asociada a plantillas reutilizables. Esta base est√° replicada con configuraci√≥n Multi-AZ para asegurar disponibilidad constante.

**AWS SageMaker**

La plataforma permite a los usuarios generar visualizaciones sugeridas a partir de descripciones en lenguaje natural. Cuando se escribe un prompt, este se procesa mediante LangChain y modelos alojados en SageMaker o OpenAI, dependiendo del contexto. SageMaker genera propuestas de dashboards o configuraciones visuales, las cuales son evaluadas y renderizadas directamente en el frontend seg√∫n las preferencias del usuario.


# 4.7 Backoffice Administrativo

## Dise√±o del Frontend 

### Arquitectura de Construcci√≥n del Backoffice Administrativo

El m√≥dulo de Backoffice Administrativo permite a los operadores internos gestionar todos los aspectos cr√≠ticos de la operaci√≥n, seguridad, auditor√≠a y configuraci√≥n del ecosistema de Data Pura Vida. La arquitectura est√° dise√±ada bajo los mismos principios de escalabilidad, modularidad, seguridad avanzada y desacoplamiento que los dem√°s m√≥dulos.

### Flujo funcional principal:

1. El usuario (operador administrativo) accede mediante login protegido por MFA en Cognito.
2. El frontend permite administrar usuarios, llaves, flujos de trabajo y auditor√≠a mediante distintos paneles desacoplados.
3. Cada acci√≥n del backoffice es enviada al backend mediante API REST protegida.
4. El backend valida roles RBAC, ejecuta l√≥gica de negocio, actualiza bases de datos (PostgreSQL, DynamoDB, S3) y dispara eventos a EventBridge y RabbitMQ seg√∫n corresponda.
5. Se registran logs completos de auditor√≠a y trazabilidad para cada operaci√≥n sensible.
6. El frontend permite consultar en tiempo real el estado de las operaciones y extraer reportes auditables.


### Dise√±o de la arquitectura

- **Frontend**  
  - Construido en React con Tailwind, siguiendo patr√≥n MVVM.
  - Atomic Design para la composici√≥n de pantallas administrativas.
  - Integraci√≥n con React Query para sincronizaci√≥n eficiente con el backend.
  - Alta separaci√≥n de l√≥gica de negocio en hooks: `useUserManagement()`, `useAuditLogs()`, `useKeyManagement()`, `usePipelineManager()`.

- **Backend**
  - Microservicio independiente sobre FastAPI desplegado en EKS.
  - Capa de seguridad API Gateway ‚Üí Cognito ‚Üí RBAC interno.
  - Persistencia h√≠brida:
    - PostgreSQL (metadata administrativa y control de usuarios)
    - DynamoDB (logs y eventos)
    - S3 (reportes y backups)
  - Event-Driven para integraciones: RabbitMQ y EventBridge.
  - Coordinaci√≥n con el Bioregistro, La B√≥veda y el Motor de Transformaci√≥n mediante gRPC.

- **Seguridad avanzada**
  - Todos los accesos requieren autenticaci√≥n multifactor con Cognito.
  - Cada acci√≥n administrativa produce un evento de auditor√≠a.
  - Toda interacci√≥n sensible es auditada y registrada en OpenSearch.


### Construcci√≥n de objetos de negocio

**Tablas principales gestionadas:**

| Tabla | Descripci√≥n |
|-------|--------------|
| Users | Administraci√≥n de operadores internos |
| UserRoles | Roles y permisos RBAC |
| PipelinesConfig | Gesti√≥n de pipelines activos |
| SecurityKeys | Llaves de cifrado activas, revocadas y expiradas |
| AuditLogs | Trazabilidad completa de cada operaci√≥n |
| Custodians | Custodios de llaves con validaci√≥n mancomunada |
| APIIntegrations | Conexiones externas habilitadas |

**Eventos generados en el backend:**

- `user.updated`
- `pipeline.config.changed`
- `key.revoked`
- `audit.logged`
- `permission.assigned`
- `external.integration.modified`


### Principios de dise√±o aplicados

- **MVVM**  
  El frontend sigue estrictamente MVVM con separaci√≥n en `models`, `hooks` (ViewModel), `components` (View).

- **SOLID**
  - **Single Responsibility:** Cada hook gestiona un solo dominio (usuarios, llaves, pipelines, auditor√≠a).
  - **Open/Closed:** Es sencillo extender nuevos formularios de administraci√≥n sin romper flujos actuales.
  - **Liskov Substitution:** Interfaz √∫nica para CRUD administrativo de cualquier objeto gestionable.
  - **Interface Segregation:** Los hooks solo exponen las props m√≠nimas requeridas.
  - **Dependency Inversion:** El backend est√° completamente desacoplado de la UI, expone solo APIs REST bien definidas.

- **Separation of Concerns:**  
  Roles claramente aislados entre visualizaci√≥n, l√≥gica de negocio, persistencia y auditor√≠a.

- **DRY:**  
  Formularios, validadores y modales reutilizados por cada panel de administraci√≥n.


### Herramientas utilizadas

| Herramienta | Funci√≥n |
|--------------|---------|
| React + Tailwind | Frontend de la UI administrativa |
| Plotly.js | Visualizaci√≥n de reportes de uso |
| React Hook Form | Formularios administrativos |
| FastAPI | Backend de servicios administrativos |
| PostgreSQL | Metadata administrativa transaccional |
| DynamoDB | Logs de auditor√≠a y seguridad |
| EventBridge + RabbitMQ | Eventos de orquestaci√≥n |
| Cognito + MFA | Control de acceso y autenticaci√≥n |
| OpenSearch | Auditor√≠a de logs en tiempo real |
| AWS KMS | Gesti√≥n de llaves de cifrado |
| AWS SES | Notificaciones administrativas |
| AWS Secrets Manager | Manejo seguro de credenciales internas |


### Estructura de carpetas Frontend

```plaintext
frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ backofficeApi.ts
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ User.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Key.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Pipeline.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Custodian.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AuditLog.ts
‚îÇ   ‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useUserManagement.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useKeyManagement.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ usePipelineManager.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useAuditLogs.ts
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ atoms/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ molecules/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ organisms/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AdminDashboardPage.tsx
‚îÇ   ‚îî‚îÄ‚îÄ App.tsx
```
## Dise√±o del Backend

### Microservicios del Backoffice Administrativo

#### 1. admin-user-management-service

Controla el acceso al ecosistema gestionando usuarios desde registro hasta permisos operacionales.

##### Componentes Clave
- **AdminUserController**: APIs REST para gesti√≥n de usuarios y organizaciones
- **UserValidator**: Validaci√≥n autom√°tica con IA y integraci√≥n SumSub para KYC
- **RoleManager**: Sistema RBAC con permisos granulares por tipo de entidad
- **PermissionEngine**: Evaluaci√≥n en tiempo real con cache Redis
- **ProfileManager**: Sincronizaci√≥n de perfiles entre Bio Registro y ecosistema

##### Operaci√≥n
Los administradores validan registros pendientes usando scoring autom√°tico de IA y SumSub. Las aprobaciones activan creaci√≥n autom√°tica de cuentas Cognito y notificaciones por email. Los cambios de permisos se propagan inmediatamente via eventos.

**Tecnolog√≠as**: FastAPI, PostgreSQL, Redis, SumSub API, AWS SES

**Eventos**:
- *Consume*: `user.registration.completed`, `user.kyc.verified`
- *Produce*: `user.approved`, `role.assigned`, `permission.updated`


#### 2. data-pipeline-manager-service

Centro de control para pipelines de transformaci√≥n con supervisi√≥n y gesti√≥n operacional completa.

##### Componentes Clave
- **PipelineController**: Dashboard y controles de pipelines en tiempo real
- **AirflowManager**: Gesti√≥n directa de DAGs con logs e intervenci√≥n manual
- **SparkMonitor**: M√©tricas de rendimiento y detecci√≥n de cuellos de botella
- **QualityAssurance**: Validaci√≥n autom√°tica antes de carga a Redshift
- **ResourceManager**: Optimizaci√≥n de clusters Spark basada en patrones hist√≥ricos

##### Operaci√≥n
Monitoreo continuo durante ventanas nocturnas de procesamiento batch. Los operadores intervienen en fallos, analizan logs de Spark y deciden reintentos o escalaci√≥n. Las intervenciones manuales quedan auditadas para an√°lisis de patrones.

**Tecnolog√≠as**: Apache Airflow API, Spark History Server, OpenSearch, PostgreSQL, Grafana API

**Eventos**:
- *Consume*: `pipeline.execution.failed`, `data.quality.issue.detected`
- *Produce*: `pipeline.manually.paused`, `data.processing.intervention.required`


#### 3. security-key-manager-service

Gestiona llaves tripartitas que protegen datasets sensibles con custodios distribuidos.

##### Componentes Clave
- **KeyManagementController**: Administraci√≥n completa de llaves criptogr√°ficas
- **CustodianManager**: Red de custodios con validaci√≥n mancomunada
- **KeyRotator**: Rotaci√≥n autom√°tica y de emergencia seg√∫n pol√≠ticas
- **CryptoValidator**: Verificaci√≥n continua de integridad matem√°tica
- **EmergencyKeyManager**: Protocolos de acceso cr√≠tico con trazabilidad

##### Operaci√≥n
Generaci√≥n autom√°tica de llaves para organizaciones aprobadas con distribuci√≥n cifrada a custodios. Rotaciones programadas transparentes y protocolos de emergencia para revocaci√≥n inmediata ante compromisos de seguridad.

**Tecnolog√≠as**: AWS KMS, AWS Secrets Manager, PostgreSQL, AWS Lambda, AWS SES

**Eventos**:
- *Consume*: `organization.approved`, `security.threat.detected`
- *Produce*: `keys.generated`, `key.revoked`, `emergency.access.granted`

---

#### 4. audit-monitoring-service

Sistema nervioso de cumplimiento y seguridad con an√°lisis ML y evidencia forense.

##### Componentes Clave
- **AuditController**: B√∫squeda forense y reportes regulatorios autom√°ticos
- **SecurityAnalyzer**: Detecci√≥n ML de anomal√≠as con SageMaker
- **ComplianceReporter**: Reportes autom√°ticos Ley 8968, GDPR, ISO 27001
- **ForensicsExtractor**: Preservaci√≥n de evidencia con cadena de custodia legal
- **ThreatDetector**: An√°lisis en tiempo real con respuestas autom√°ticas

##### Operaci√≥n
Recopilaci√≥n continua de eventos con an√°lisis ML para patrones sospechosos. Alertas clasificadas por severidad y investigaciones formales con preservaci√≥n de evidencia. Reportes de cumplimiento autom√°ticos seg√∫n calendarios regulatorios.

**Tecnolog√≠as**: OpenSearch, AWS CloudTrail, Amazon SageMaker, PostgreSQL, AWS Lambda

**Eventos**:
- *Consume*: Todos los eventos del ecosistema
- *Produce*: `security.threat.critical`, `compliance.violation.detected`


#### 5. system-configuration-service

Centro de control operacional para configuraciones globales e integraciones externas.

##### Componentes Clave
- **ConfigController**: Configuraci√≥n global y gesti√≥n de integraciones
- **IntegrationManager**: Conexiones seguras con APIs externas y fallbacks
- **HealthMonitor**: Supervisi√≥n proactiva de infraestructura completa
- **BackupManager**: Respaldos autom√°ticos con pruebas de recuperaci√≥n
- **FeatureToggleManager**: Despliegue gradual con feature flags

##### Operaci√≥n
Dashboards centralizados del estado completo de infraestructura. Configuraciones versionadas con rollback inmediato. Monitoreo continuo de integraciones con activaci√≥n autom√°tica de contingencias y ventanas de mantenimiento coordinadas.

**Tecnolog√≠as**: AWS Systems Manager, Terraform, CloudWatch, AWS Backup, AWS Lambda

**Eventos**:
- *Consume*: `service.health.degraded`, `integration.external.failed`
- *Produce*: `config.updated`, `backup.completed`, `feature.toggle.changed`

### Servicios AWS para Backoffice Administrativo

#### Compute Services

##### Amazon EKS
**Prop√≥sito**: Orquestaci√≥n de los 5 microservicios del backoffice con escalabilidad autom√°tica
- **Configuraci√≥n**: Cluster multi-AZ con node groups optimizados para cargas administrativas
- **Auto Scaling**: HPA basado en CPU/memoria para manejar picos durante validaciones masivas
- **Networking**: VPC privada con subnets aisladas para mayor seguridad
- **Uso**: Despliega admin-user-management, data-pipeline-manager, security-key-manager, audit-monitoring y system-configuration services

##### AWS Lambda
**Prop√≥sito**: Funciones serverless para operaciones cr√≠ticas y automatizaci√≥n
- **Key Rotation Functions**: Rotaci√≥n autom√°tica de llaves tripartitas programada
- **Audit Processing**: Procesamiento en tiempo real de eventos de auditor√≠a
- **Health Checks**: Verificaciones automatizadas de salud de integraciones externas
- **Backup Validation**: Verificaci√≥n de integridad de respaldos autom√°ticos

#### Storage & Database

##### Amazon RDS PostgreSQL
**Prop√≥sito**: Base de datos principal para cada microservicio siguiendo patr√≥n Database per Service
- **Configuraci√≥n**: Multi-AZ con encrypted storage, automated backups
- **Databases**:
  - `admin_users_db`: Usuarios, roles, permisos, historial de validaciones
  - `pipelines_db`: Configuraciones de pipelines, logs de intervenciones
  - `security_keys_db`: Metadata de llaves, custodios (datos cifrados)
  - `audit_db`: Casos de investigaci√≥n, reportes de cumplimiento
  - `system_config_db`: Configuraciones globales, estado de integraciones

##### Amazon ElastiCache Redis
**Prop√≥sito**: Cache distribuido para optimizaci√≥n de rendimiento
- **Session Cache**: Sesiones activas de administradores
- **Permission Cache**: Permisos RBAC para validaci√≥n r√°pida
- **Pipeline Status**: Estado en tiempo real de pipelines de transformaci√≥n
- **Integration Health**: Estado actual de APIs externas (Stripe, SumSub)

##### Amazon S3
**Prop√≥sito**: Almacenamiento de documentos y respaldos
- **Buckets**:
  - `backoffice-audit-logs`: Logs de auditor√≠a con retention autom√°tico
  - `compliance-reports`: Reportes regulatorios con cifrado
  - `user-documents`: Documentos de validaci√≥n con acceso controlado
  - `system-backups`: Respaldos autom√°ticos de configuraciones

#### Security & Identity

##### AWS Cognito
**Prop√≥sito**: Autenticaci√≥n y autorizaci√≥n de administradores del backoffice
- **User Pool**: Gesti√≥n de identidades de operadores administrativos
- **Identity Pool**: Control de acceso granular por rol administrativo
- **MFA**: Autenticaci√≥n multifactor obligatoria para operaciones cr√≠ticas
- **Integration**: SSO con Active Directory corporativo si aplica

##### AWS KMS
**Prop√≥sito**: Gesti√≥n de llaves maestras para cifrado de datos sensibles
- **Master Keys**: Protecci√≥n de llaves tripartitas del security-key-manager
- **Database Encryption**: Cifrado de bases de datos RDS
- **S3 Encryption**: Cifrado de documentos y reportes almacenados
- **Secrets Encryption**: Protecci√≥n adicional de credenciales en Secrets Manager

##### AWS Secrets Manager
**Prop√≥sito**: Almacenamiento seguro de credenciales y llaves distribuidas
- **API Credentials**: Credenciales para SumSub, Stripe, servicios externos
- **Database Connections**: Strings de conexi√≥n a bases de datos
- **Tripartite Key Portions**: Porciones de llaves distribuidas entre custodios
- **Auto Rotation**: Rotaci√≥n autom√°tica de credenciales seg√∫n pol√≠ticas


#### Integration & Communication

##### Amazon EventBridge
**Prop√≥sito**: Bus de eventos central para comunicaci√≥n as√≠ncrona entre microservicios
- **Custom Event Bus**: Eventos espec√≠ficos del backoffice separados del bus principal
- **Event Rules**: Routing autom√°tico de eventos entre microservicios
- **Dead Letter Queues**: Manejo de eventos fallidos con reintentos
- **Event Replay**: Capacidad de replay para debugging y recovery

##### Amazon API Gateway
**Prop√≥sito**: Gateway unificado para APIs del backoffice con seguridad integrada
- **Rate Limiting**: Control de carga por usuario y endpoint
- **Authentication**: Integraci√≥n con Cognito para validaci√≥n de tokens
- **Request Validation**: Validaci√≥n autom√°tica de schemas de entrada
- **Usage Plans**: Planes diferenciados por tipo de administrador

##### Amazon SES
**Prop√≥sito**: Servicio de email para notificaciones cr√≠ticas
- **Transactional Emails**: Notificaciones de aprobaci√≥n/rechazo de usuarios
- **Security Alerts**: Alertas inmediatas de incidentes de seguridad
- **Compliance Notifications**: Notificaciones regulatorias autom√°ticas
- **Custodian Communications**: Emails cifrados para custodios de llaves

#### AI & Analytics

##### Amazon SageMaker
**Prop√≥sito**: Modelos de machine learning para an√°lisis de seguridad y detecci√≥n de anomal√≠as
- **Anomaly Detection**: Modelos para detectar patrones sospechosos en audit logs
- **Risk Scoring**: Scoring autom√°tico de usuarios durante validaciones
- **Threat Classification**: Clasificaci√≥n autom√°tica de amenazas de seguridad
- **Model Endpoints**: APIs en tiempo real para an√°lisis durante operaciones

##### AWS Comprehend
**Prop√≥sito**: An√°lisis de texto para documentos de validaci√≥n
- **Document Analysis**: An√°lisis autom√°tico de documentos subidos por usuarios
- **Sentiment Analysis**: An√°lisis de comunicaciones en casos de investigaci√≥n
- **Entity Recognition**: Extracci√≥n autom√°tica de entidades relevantes
- **Custom Models**: Modelos espec√≠ficos para documentos costarricenses

#### Backup & Disaster Recovery

##### AWS Backup
**Prop√≥sito**: Respaldos centralizados y automatizados
- **RDS Backups**: Respaldos autom√°ticos de todas las bases de datos
- **S3 Cross-Region**: Replicaci√≥n de documentos cr√≠ticos entre regiones
- **Point-in-Time Recovery**: Capacidad de restauraci√≥n granular
- **Compliance Retention**: Retenci√≥n seg√∫n requerimientos regulatorios

##### AWS CloudFormation
**Prop√≥sito**: Infraestructura como c√≥digo para disaster recovery
- **Template Versioning**: Versionado de infraestructura para rollbacks r√°pidos
- **Multi-Region Deployment**: Capacidad de despliegue en regi√≥n secundaria
- **Automated Recovery**: Scripts de recuperaci√≥n autom√°tica ante desastres
- **Configuration Drift**: Detecci√≥n de cambios no autorizados en infraestructura

#### Configuration & Compliance

##### AWS Systems Manager
**Prop√≥sito**: Gesti√≥n centralizada de configuraciones y patches
- **Parameter Store**: Configuraciones encriptadas versionadas por microservicio
- **Patch Manager**: Actualizaciones autom√°ticas de seguridad en EKS nodes
- **Session Manager**: Acceso seguro a instancias sin SSH keys
- **Compliance Scanning**: Verificaci√≥n autom√°tica de compliance de infraestructura

##### AWS Config
**Prop√≥sito**: Monitoreo de compliance y cambios de configuraci√≥n
- **Resource Compliance**: Verificaci√≥n continua de configuraciones seg√∫n pol√≠ticas
- **Change Tracking**: Historial completo de cambios en recursos AWS
- **Compliance Rules**: Reglas autom√°ticas para Ley 8968 y GDPR
- **Remediation**: Correcci√≥n autom√°tica de configuraciones no conformes


## Dise√±o de los datos

### Topolog√≠a de Datos

- **Tipo:** OLTP + OLAP + NoSQL + B√∫squeda + Almacenamiento de Objetos

- Para el Backoffice, se emplear√° una arquitectura de datos h√≠brida que combine `OLTP` para transacciones y mantenimiento de registros principales, `NoSQL` para metadatos din√°micos y de alto rendimiento, `OLAP` para auditor√≠as y reportes, b√∫squeda para la supervisi√≥n y extracci√≥n de informaci√≥n, y almacenamiento de objetos para grandes vol√∫menes de datos no estructurados como las reglas de carga o las evidencias legales.

- Para `OLTP` utilizaremos `RDS` como la base de datos principal para la gesti√≥n. Esta base de datos es ideal para operaciones transaccionales. Se usar√°n tablas para:

   - **Usuarios:** Mantenimiento de usuarios, roles, perfiles.
   - **RolEntidad:** Definici√≥n de roles y su asignaci√≥n a usuarios a trav√©s de la tabla `UsuarioEntidad`. 
   - **Entidad:** Representa las organizaciones que se registran en la plataforma.
   - **CargaDatos:** Registro de los procesos de carga de datos, incluyendo su estado y origen.
   - **Dataset:** Mantenimiento de los datasets publicados, incluyendo si son p√∫blicos/privados o pagados y sus permisos de acceso.
   - **LlaveSeguridad y LlaveTripartita :** Almacenamiento y gesti√≥n de llaves criptogr√°ficas.
   - **CustodioLlave:** Gesti√≥n de los custodios de llaves.
   - **Sesion:** Gesti√≥n de sesiones activas de los usuarios del Backoffice.
   - **DatasetPermisos:** Define los permisos espec√≠ficos que una entidad o usuario sobre datos.
   - **UsuarioEntidad:** Tabla intermedia que vincula a los usuarios con una entidad.
   - **Auditoria:** Registra todas las acciones relevantes realizadas.

- Para el `NoSQL` utilizaremos `DynamoDB` ya que este almacena metadatos din√°micos y de alto rendimiento. Ejemplos de uso:
   - Estado operativo.
   - Historial de cambios de `Dataset`.
   - Logs de ejecuci√≥n.
   - Configuraciones de conectividad.

- `OLAP` utilizando `OpenSearch` ser√° √∫til para almacenar datos para auditor√≠as de todas las operaciones del sistema. Tambi√©n ser√° la base para la generaci√≥n de reportes anal√≠ticos. Se almacenar√°:
   - Registro detallado de usuarios.
   - Estado operativo de servicios.
   - Trazabilidad del consumo de `datasets`.


`Almacenamiento de Objetos` atreves de `AWS S3` ser√° utilizado para almacenar grandes vol√∫menes de datos no estructurados y semiestructurados. Esto incluye:

   - Archivos complejos.
   - Informaci√≥n legal o regulatoria.
   - Respaldo de las bases de datos `OLTP` y `NoSQL`.
   - Datos sin procesar.


- Para el `Caching` utilizaremos `Redis` para almacenar informaci√≥n con TTL (Time To Live), ideal para:
   -  Sesiones activas del backoffice.
   -  Permisos de usuarios para acceso r√°pido.
   -  M√©tricas en tiempo real para monitoreo.

- **Tecnolog√≠a Cloud:**
   - **Amazon RDS**
   - **Amazon DynamoDB**
   - **Amazon OpenSearch**
   - **Amazon S3**
   - **Amazon Redis**
   - **AWS Glue**
   - **Apache Airflow**

- **Pol√≠ticas y Reglas:**
- **Single-region:** Toda la infraestructura estar√° localizada en `us-east-1` para simplificar la gesti√≥n y reducir la latencia.

- **Backups autom√°ticos:**
   - **RDS:** respaldo diario autom√°tico en S3.
   - **DynamoDB:** habilitado con Point-in-Time Recovery
   - **OpenSearch:** snapshots automatizados en S3.
   - **S3:** versionado activado y ciclo de vida configurado.

- **Failover autom√°tico:**
   - **RDS:** habilitado con Multi-AZ.
   - **OpenSearch:** replicaci√≥n entre zonas de disponibilidad.
   - **DynamoDB y S3**: dise√±ados como servicios multi-AZ por defecto.

### Tenencia, Seguridad y Privacidad

- **Modelo:** Single-Access-Point, RBAC (Role-Based Access Control), Single-Tenant

- Todo acceso a datos en el Backoffice se hace a trav√©s del Single-Access-Point. Solo las clases autorizadas como `UserRepository`, `SecurityKeyRepository`, `ConfigurationRepository`, y `AuditRepository` est√°n habilitadas para interactuar con las fuentes de datos.

- Esto incluye `AWS RDS`, `DynamoDB` y `OpenSearch`. Toda consulta o acci√≥n desde APIs, Lambda o dashboards debe pasar por estas clases.

- Se usar√° `Single-Tenant`, ya que el Backoffice es una aplicaci√≥n interna para la gesti√≥n y administraci√≥n de la plataforma. 

- Para hacer el manejo de control de acceso y RBAC se har√° lo siguiente:

- **AWS Cognito + IAM:**
   - AWS Cognito para autenticaci√≥n de usuarios.
   - IAM Roles con pol√≠ticas de m√≠nimos privilegios para servicios AWS.
   - Capa adicional de autorizaci√≥n implementada en FastAPI, que valida permisos a partir de los roles entregados por Cognito.

   - Ejemplo conceptual de control RBAC en FastAPI:
    ```py
    # Ejemplo conceptual: Decorador de FastAPI para RBAC
    from fastapi import Depends, HTTPException, status
    from app.core.security import get_current_user # Obtiene el usuario autenticado y sus roles

    def requires_role(role: str): # 'admin', 'auditor', 'configurator'
        def role_checker(user: dict = Depends(get_current_user)):
            if role not in user.get("roles", []): # Asumiendo que los roles vienen del token de Cognito
                raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Access denied")
            return user
        return role_checker

    # Uso en un endpoint del Backoffice:
    # @router.post("/config/rules", dependencies=[Depends(requires_role("configurator"))])
    # async def update_rule(...):
    #     pass
    ```


- **Cloud:**
   - **AWS Cognito** para autenticaci√≥n de usuarios
   - **AWS IAM** para la gesti√≥n de identidades y permisos
   - Cifrado de datos con **AWS KMS**
   - Gesti√≥n segura de credenciales con **AWS Secrets Manager**
   - **Amazon OpenSearch Service**
   - **AWS S3** almacena logs
   - Para el env√≠o de notificaciones **AWS SNS/SES**

- **Beneficios:**
- Con Single-Access-Point los accesos a datos del Backoffice pasan por validadores en las clases de repositorio del backend minimizando riesgo de acceso directo.

- El `RBAC` y `AWS IAM` asegura que solo los usuarios y servicios autorizados puedan realizar operaciones espec√≠ficas.

### Conexi√≥n a Base de Datos

- **Modelo:** Transaccional v√≠a Mapeo de Objetos / SDKs Nativos / Funciones Asincr√≥nicas

- El Backoffice adopta un modelo de conexi√≥n h√≠brido. Usaremos un sistema de Mapeo de Objetos a Relaciones (ORM) para interactuar con PostgreSQL (RDS). Para bases de datos NoSQL como DynamoDB y OpenSearch, usaremos sus SDKs nativos directamente en nuestras clases de repositorio. Algunas operaciones que necesitan ejecutarse de forma independiente o escalar bajo demanda se manejar√°n con AWS Lambda.

- **Patrones de POO:**
Aplicamos el Patr√≥n `Repositorio` para manejar el acceso a cada tipo de dato, lo que nos permite cambiar de base de datos o hacer pruebas m√°s f√°cilmente.

   -	**UserRepository:** Para usuarios en PostgreSQL.
   -	**SecurityKeyRepository:** Para llaves de seguridad en PostgreSQL.
   -	**ConfigurationRepository:** Para reglas y configuraciones en PostgreSQL.
   -	**AuditRepository:** Para registros de auditor√≠a en OpenSearch y S3.
   -	**DynamicMetadataRepository:** Para metadatos en DynamoDB.
   
- **Beneficios:** 
   - Protecci√≥n contra inyecciones SQL.
   -Abstracci√≥n para pruebas automatizadas.
   -Flexibilidad para escalar vertical u horizontalmente.

- **Pool de Conexiones:**
Un sistema de pool de conexiones para gestionar de forma eficiente las conexiones a PostgreSQL desde el backend.
   -	Tama√±o base del pool: 10 conexiones
   -	Tama√±o m√°ximo: 20 conexiones
   -	Tiempo de espera: 30 segundos
   -	Tiempo de vida inactiva: 60 segundos

- **Beneficios:** Reutiliza las conexiones reduciendo la carga. Para DynamoDB y OpenSearch, no necesitamos pools, ya que sus SDKs est√°n dise√±ados para conexiones r√°pidas y a demanda.


- **Drivers y SDKs:**
  -	**PostgreSQL:**
      -	Se usa `psycopg2` para manejar modelos y transacciones.
      -	Gestiona los modelos de datos y las sesiones para las transacciones.

  -	**DynamoDB, S3, KMS, Cognito:**
      -	Usaremos el SDK oficial de AWS para Python (boto3).
      -	DynamoDB guarda metadatos din√°micos.
      -	S3 almacena archivos (logs, modelos IA, datasets).
      -	KMS maneja claves y cifrado.
      -	Cognito valida usuarios y tokens.

  -	OpenSearch:
      -	Se usa `opensearch-py` para b√∫squedas y auditor√≠a.
      -	Se usa para buscar, indexar y analizar los logs de auditor√≠a.
      -	Protegido con `AWS Signature v4`.

  -	AWS Lambda:
      -	Las Lambdas usar√°n el entorno de ejecuci√≥n python3.11.
      -	Acceder√°n a los servicios de AWS (bases de datos, S3) usando los SDKs (boto3, nuestro sistema de mapeo de objetos, opensearch-py).
      -	Se activar√°n por eventos de EventBridge (ej., para procesar logs o enviar notificaciones).

- **Dise√±o para IA**
El Backoffice es clave para administrar los datos y procesos que usa la IA, garantizando control y calidad.

- **Implementaciones para la Habilitaci√≥n de IA:**
Para que el Backoffice apoye los sistemas de IA, hacemos lo siguiente:

  - **Gesti√≥n de Metadatos para IA:**
      - `PostgreSQL` guarda modelos y datasets.
      - `DynamoDB` registra ejecuciones de IA en tiempo real.

  - **Almacenamiento de Componentes de IA (AWS S3):**
      - `S3` guarda modelos, datasets y embeddings.

  - **Gesti√≥n de Reglas y Datos para IA (PostgreSQL / S3):**
      - PostgreSQL y S3 almacenan reglas de carga de datos.
      - Spark o Glue las aplican; Airflow coordina los procesos.

  - **Integraci√≥n con AWS SageMaker:**
      - El Backoffice permite configurar SageMaker para el entrenamiento de modelos.
      - Aunque el entrenamiento es externo, el Backoffice puede mostrar su progreso.

  - **Monitoreo y Auditor√≠a para IA:**
      - Las interacciones con los modelos de IA se registran en OpenSearch para monitorear.
      
- **Justificaci√≥n:**
Al centralizar la gesti√≥n de los elementos de IA en el Backoffice:

  -  **Optimizamos el entrenamiento:** Aseguramos que la IA use datos consistentes y de alta calidad siempre que se prepare un dataset.
  -  **Garantizamos la Gobernanza:** Tenemos un control centralizado para supervisar y auditar los modelos de IA y el uso de sus datos en todo momento.
  -  **Facilitamos la Innovaci√≥n:** Agilizamos el desarrollo y la mejora de soluciones de IA para todo Data Pura Vida continuamente.

- **Diagrama de Base de Datos**
A continuaci√≥n, se describen las tablas principales para PostgreSQL, as√≠ como una menci√≥n de c√≥mo se relacionan con DynamoDB y OpenSearch para sus prop√≥sitos espec√≠ficos


![image](img/DiagramaBDBackoffice.png)

## 5. Validaci√≥n de los requerimientos